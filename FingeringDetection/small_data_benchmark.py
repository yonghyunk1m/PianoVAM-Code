#!/usr/bin/env python3
"""
ì‘ì€ ë°ì´í„°ì…‹ ê¸°ë°˜ Floating Hand Detection ë¹ ë¥¸ ë²¤ì¹˜ë§ˆí‚¹
- 2024-09-04_22-06-40 ë°ì´í„°ë§Œ ì‚¬ìš©
- SciPy vs PyTorch ì„±ëŠ¥ ë¹„êµ  
- ì •ì„±í‰ê°€ë¥¼ ìœ„í•œ ìƒì„¸ ë¹„êµ ë¹„ë””ì˜¤ ìƒì„±
"""

import os
import pickle
import time
import numpy as np
import cv2
import json
import sys
from pathlib import Path
from typing import List, Dict, Tuple, Any
import matplotlib.pyplot as plt
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
from PIL import Image
import psutil
import threading
from datetime import datetime, timedelta

# GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ import
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# ë‘ ë²„ì „ì˜ floating hands ëª¨ë“ˆ import
# Golden Standard: ì›ë³¸ SciPy ë²„ì „
try:
    import floatinghands_original as scipy_version
    print("âœ… SciPy ì›ë³¸ ë²„ì „ ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ SciPy ì›ë³¸ ë²„ì „ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    exit(1)

# ìƒˆ êµ¬í˜„: ìˆœìˆ˜ PyTorch ë²„ì „
try:
    import floatinghands_torch_pure as pytorch_version
    print("âœ… PyTorch ìˆœìˆ˜ ë²„ì „ ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ PyTorch ìˆœìˆ˜ ë²„ì „ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    exit(1)

# MIDI ë°ì´í„° ì²˜ë¦¬ìš© ëª¨ë“ˆ import
try:
    import midicomparison
    print("âœ… MIDI ë¹„êµ ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸  MIDI ë¹„êµ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("   MIDI ê¸°ë°˜ ì„ê³„ê°’ ê³„ì‚°ì€ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    midicomparison = None

# main_loop importëŠ” ë‚˜ì¤‘ì— í•„ìš”í•  ë•Œë§Œ í•˜ë„ë¡ ì§€ì—°
main_loop_module = None

def import_main_loop():
    """main_loop ëª¨ë“ˆì„ ì§€ì—° importí•©ë‹ˆë‹¤"""
    global main_loop_module
    if main_loop_module is None:
        try:
            import main_loop as main_loop_module_temp
            main_loop_module = main_loop_module_temp
            pass  # ì¶œë ¥ ê°„ì†Œí™”
        except Exception as e:
            print(f"âŒ main_loop ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"   ìƒì„¸ ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
            raise
    return main_loop_module

# MediaPipe ê·¸ë¦¬ê¸° ìœ í‹¸ë¦¬í‹° ì„¤ì •
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles
mp_hands = mp.solutions.hands

def draw_enhanced_hand_landmarks(image, hand_landmarks, handedness, floating_status, depth_info, handtype_suffix="", threshold_value=0.9):
    """í–¥ìƒëœ Hand landmarksì™€ floating ìƒíƒœ, ê¹Šì´ ì •ë³´ë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤
    
    Args:
        threshold_value: ë‹¨ì¼ ì„ê³„ê°’(float) ë˜ëŠ” ì¢Œìš° ì† êµ¬ë¶„ ì„ê³„ê°’(dict)
    """
    if not hand_landmarks:
        return image
    
    # ì´ë¯¸ì§€ ë³µì‚¬
    annotated_image = image.copy()
    height, width = annotated_image.shape[:2]
    
    for idx, landmarks in enumerate(hand_landmarks):
        # ì† ì¢…ë¥˜ í™•ì¸
        if idx < len(handedness):
            hand_type = handedness[idx].classification[0].category_name
            
            # ê¹Šì´ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            depth_value = depth_info.get(hand_type, {}).get('depth', 0.0)
            is_floating = floating_status.get(hand_type, False)
            
            # Floating ìƒíƒœì— ë”°ë¥¸ ìƒ‰ìƒ ì„¤ì • (ë” ëª…í™•í•œ ìƒ‰ìƒ)
            if is_floating:
                color = (0, 0, 255)  # ë¹¨ê°„ìƒ‰ - Floating
                bg_color = (0, 0, 100)  # ì–´ë‘ìš´ ë¹¨ê°„ìƒ‰ ë°°ê²½
                status_text = f"{hand_type} FLOATING"
            else:
                color = (0, 255, 0)  # ì´ˆë¡ìƒ‰ - Normal
                bg_color = (0, 100, 0)  # ì–´ë‘ìš´ ì´ˆë¡ìƒ‰ ë°°ê²½
                status_text = f"{hand_type} NORMAL"
            
            # ì†ëª© ì¢Œí‘œ ê°€ì ¸ì˜¤ê¸° (landmarksëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)
            if len(landmarks) > 0:
                wrist = landmarks[0]  # ì²« ë²ˆì§¸ëŠ” ì†ëª©
                wrist_x = int((wrist.x + 1) * width / 2)  # [-1,1] -> [0,width]
                wrist_y = int((wrist.y + 1) * height / 2)  # [-1,1] -> [0,height]
                
                # ì£¼ìš” ì† ê´€ì ˆë“¤ ê·¸ë¦¬ê¸° (ë” ëª…í™•í•œ ì—°ê²°ì„  í¬í•¨)
                for i, landmark in enumerate(landmarks):
                    x = int((landmark.x + 1) * width / 2)
                    y = int((landmark.y + 1) * height / 2)
                    
                    if 0 <= x < width and 0 <= y < height:
                        # ê´€ì ˆ ì  ê·¸ë¦¬ê¸° (í¬ê¸° ì¡°ì •)
                        cv2.circle(annotated_image, (x, y), 4, color, -1)
                        cv2.circle(annotated_image, (x, y), 5, (255, 255, 255), 1)  # í°ìƒ‰ í…Œë‘ë¦¬
                        
                        # ì¤‘ìš”í•œ ê´€ì ˆì ì€ ë” í¬ê²Œ í‘œì‹œ
                        if i in [0, 4, 8, 12, 16, 20]:  # ì†ëª©, ì—„ì§€~ì†Œì§€ ëì 
                            cv2.circle(annotated_image, (x, y), 8, color, 2)
                            cv2.circle(annotated_image, (x, y), 10, (255, 255, 255), 1)
                
                # ì •ë³´ ë°•ìŠ¤ ìœ„ì¹˜ ê³„ì‚°
                info_x = max(10, min(wrist_x - 100, width - 250))
                info_y = max(60, wrist_y - 50)
                
                # ì •ë³´ ë°°ê²½ ë°•ìŠ¤
                cv2.rectangle(annotated_image, (info_x, info_y - 45), (info_x + 240, info_y + 15), bg_color, -1)
                cv2.rectangle(annotated_image, (info_x, info_y - 45), (info_x + 240, info_y + 15), color, 2)
                
                # ìƒíƒœ ë° ê¹Šì´ ì •ë³´ í‘œì‹œ
                cv2.putText(annotated_image, status_text + handtype_suffix, (info_x + 5, info_y - 25),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                cv2.putText(annotated_image, f"Depth: {depth_value:.3f}", (info_x + 5, info_y - 5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                
                # ì„ê³„ê°’ê³¼ì˜ ë¹„êµ í‘œì‹œ (ì¢Œìš° ì† êµ¬ë¶„ ì§€ì›)
                if isinstance(threshold_value, dict):
                    # ì¢Œìš° ì† êµ¬ë¶„ ì„ê³„ê°’
                    current_threshold = threshold_value.get(hand_type, 0.9)
                    threshold_status = f"< {current_threshold:.3f} (NORMAL)" if depth_value < current_threshold else f">= {current_threshold:.3f} (FLOATING)"
                    threshold_color = (255, 255, 0) if depth_value < current_threshold else (0, 255, 255)
                else:
                    # ë‹¨ì¼ ì„ê³„ê°’ (ê¸°ì¡´ ë°©ì‹)
                    threshold_status = f"< {threshold_value:.3f} (NORMAL)" if depth_value < threshold_value else f">= {threshold_value:.3f} (FLOATING)"
                    threshold_color = (255, 255, 0) if depth_value < threshold_value else (0, 255, 255)
                cv2.putText(annotated_image, threshold_status, (info_x + 5, info_y + 12),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, threshold_color, 1)
                
                # ì†ëª©ì— ìƒíƒœ í‘œì‹œ ì›
                circle_radius = 15 if is_floating else 12
                cv2.circle(annotated_image, (wrist_x, wrist_y), circle_radius, color, 4)
                cv2.circle(annotated_image, (wrist_x, wrist_y), circle_radius + 2, (255, 255, 255), 1)
    
    return annotated_image

def draw_enhanced_comparison_header(image, frame_num, scipy_floating, pytorch_floating, scipy_depth_info, pytorch_depth_info, total_frames):
    """í–¥ìƒëœ ë¹„êµ ì •ë³´ë¥¼ ì´ë¯¸ì§€ ìƒë‹¨ì— ì˜¤ë²„ë ˆì´í•©ë‹ˆë‹¤"""
    height, width = image.shape[:2]
    
    # ìƒë‹¨ ì •ë³´ íŒ¨ë„ í¬ê¸° ê³„ì‚°
    panel_height = 120
    
    # ë°˜íˆ¬ëª… ë°°ê²½ ë°•ìŠ¤
    overlay = image.copy()
    cv2.rectangle(overlay, (0, 0), (width, panel_height), (0, 0, 0), -1)
    cv2.addWeighted(overlay, 0.8, image, 0.2, 0, image)
    
    # ì „ì²´ í…Œë‘ë¦¬
    border_color = (0, 0, 255) if scipy_floating != pytorch_floating else (0, 255, 0)
    cv2.rectangle(image, (0, 0), (width-1, panel_height-1), border_color, 3)
    
    # ì¢Œì¸¡ ì˜ì—­ (í”„ë ˆì„ ì •ë³´)
    left_x = 20
    
    # í”„ë ˆì„ ì •ë³´
    cv2.putText(image, f"Frame: {frame_num}/{total_frames}", (left_x, 30),
               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    
    progress = frame_num / total_frames * 100
    cv2.putText(image, f"Progress: {progress:.1f}%", (left_x, 55),
               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)
    
    # ì¤‘ì•™ ì˜ì—­ (ë¹„êµ ê²°ê³¼)
    center_x = width // 2 - 150
    
    # ì œëª©
    cv2.putText(image, "SciPy (Golden) vs PyTorch (New)", (center_x, 25),
               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
    
    # ê²°ê³¼ ë¹„êµ
    if scipy_floating == pytorch_floating:
        match_color = (0, 255, 0)  # ì´ˆë¡ìƒ‰ - ì¼ì¹˜
        match_text = "MATCH"
        match_icon = "MATCH"
    else:
        match_color = (0, 0, 255)  # ë¹¨ê°„ìƒ‰ - ë¶ˆì¼ì¹˜
        match_text = "MISMATCH"
        match_icon = "MISMATCH"
    
    cv2.putText(image, match_text, (center_x, 55),
               cv2.FONT_HERSHEY_SIMPLEX, 0.8, match_color, 2)
    
    # ìƒì„¸ ìƒíƒœ
    scipy_status = "FLOATING" if scipy_floating else "NORMAL"
    pytorch_status = "FLOATING" if pytorch_floating else "NORMAL"
    
    scipy_color = (100, 100, 255) if scipy_floating else (100, 255, 100)
    pytorch_color = (255, 150, 100) if pytorch_floating else (100, 255, 100)
    
    cv2.putText(image, f"SciPy: {scipy_status}", (center_x, 80),
               cv2.FONT_HERSHEY_SIMPLEX, 0.6, scipy_color, 1)
    cv2.putText(image, f"PyTorch: {pytorch_status}", (center_x, 100),
               cv2.FONT_HERSHEY_SIMPLEX, 0.6, pytorch_color, 1)
    
    # ìš°ì¸¡ ì˜ì—­ (ê¹Šì´ ì°¨ì´ ì •ë³´)
    right_x = width - 350
    
    if scipy_depth_info and pytorch_depth_info:
        cv2.putText(image, "Depth Comparison", (right_x, 25),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        
        y_pos = 45
        for handtype in scipy_depth_info.keys():
            if handtype in pytorch_depth_info:
                scipy_depth = scipy_depth_info[handtype].get('depth', 0.0)
                pytorch_depth = pytorch_depth_info[handtype].get('depth', 0.0)
                depth_diff = abs(scipy_depth - pytorch_depth)
                
                # ì°¨ì´ì— ë”°ë¥¸ ìƒ‰ìƒ
                diff_color = (0, 255, 255) if depth_diff > 0.1 else (255, 255, 255)
                
                cv2.putText(image, f"{handtype}: Î”={depth_diff:.3f}", (right_x, y_pos),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, diff_color, 1)
                y_pos += 20
    
    return image

def create_keyboard_overlay(image, keyboard_coords=None):
    """í‚¤ë³´ë“œ ì¢Œí‘œë¥¼ ì´ë¯¸ì§€ì— ì˜¤ë²„ë ˆì´í•©ë‹ˆë‹¤ (ì„ íƒì‚¬í•­)"""
    if keyboard_coords is None:
        return image
    
    # í‚¤ë³´ë“œ ì¢Œí‘œê°€ ìˆìœ¼ë©´ ê·¸ë¦¬ê¸°
    # ì´ ë¶€ë¶„ì€ keyboardcoordinateinfo.pklì˜ êµ¬ì¡°ì— ë”°ë¼ êµ¬í˜„
    # í˜„ì¬ëŠ” ê°„ë‹¨íˆ íŒ¨ìŠ¤
    return image

# ë¡œê¹… ìµœì í™” - í•µì‹¬ ì •ë³´ë§Œ ì¶œë ¥
class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤ - ê°„ì†Œí™”ëœ ë¡œê¹…"""
    
    def __init__(self):
        self.start_time = None
        self.step_start_time = None
        self.step_progress = 0
        self.step_total = 0
        self.overall_progress = 0
        self.overall_total = 0
        self.current_step = ""
        self.monitoring = False
        self.monitor_thread = None
        
    def start_monitoring(self, total_steps=0):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.start_time = time.time()
        self.overall_total = total_steps
        self.monitoring = True
        
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1)
        
        total_elapsed = time.time() - self.start_time if self.start_time else 0
        print(f"\nì‘ì—… ì™„ë£Œ ({self._format_time(total_elapsed)})")
        
    def start_step(self, step_name: str, total_items=0):
        """ë‹¨ê³„ ì‹œì‘"""
        self.current_step = step_name
        self.step_start_time = time.time()
        self.step_progress = 0
        self.step_total = total_items
        
        print(f"\n{step_name}...")
            
    def log_operation(self, operation_name: str, details: str = "", show_gpu=False):
        """ê°œë³„ ì—°ì‚° ë¡œê¹…"""
        pass  # ê°„ì†Œí™”ë¥¼ ìœ„í•´ ë¹„í™œì„±í™”
    
    def log_batch_progress(self, batch_num: int, total_batches: int, batch_size: int, 
                          items_processed: int, operation: str = ""):
        """ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ ìƒí™© ë¡œê¹…"""
        pass  # ê°„ì†Œí™”ë¥¼ ìœ„í•´ ë¹„í™œì„±í™”
    
    def log_algorithm_start(self, algorithm_name: str, method: str, precision: str, 
                           processing_type: str, optimizations: list = None):
        """ì•Œê³ ë¦¬ì¦˜ ì‹œì‘ ë¡œê¹…"""
        print(f"{algorithm_name} ì‹œì‘ ({method}, {precision})")
    
    def log_memory_usage(self, stage: str, cpu_memory: float = None, gpu_memory: float = None):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…"""
        pass  # ê°„ì†Œí™”ë¥¼ ìœ„í•´ ë¹„í™œì„±í™”
    
    def update_step_progress(self, current: int, message: str = ""):
        """ë‹¨ê³„ë³„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        self.step_progress = current
    
    def finish_step(self, message: str = ""):
        """ë‹¨ê³„ ì™„ë£Œ"""
        elapsed = time.time() - self.step_start_time if self.step_start_time else 0
        self.overall_progress += 1
        
        if message:
            print(f"{self.current_step} ì™„ë£Œ: {message}")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if TORCH_AVAILABLE and torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def _print_system_info(self):
        """ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥ - ê°„ì†Œí™”"""
        print(f"\nğŸ–¥ï¸  ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"   ğŸ’» CPU: {psutil.cpu_count(logical=False)}ì½”ì–´")
        
        memory = psutil.virtual_memory()
        print(f"   ğŸ’¾ RAM: {memory.total/1024**3:.1f}GB")
        
        if TORCH_AVAILABLE and torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"   ğŸ® GPU: {gpu_name} ({gpu_memory:.1f}GB)")
        else:
            print(f"   ğŸ® GPU: ì‚¬ìš© ë¶ˆê°€")
    
    def _format_time(self, seconds):
        """ì‹œê°„ í¬ë§·íŒ…"""
        if seconds < 60:
            return f"{seconds:.1f}ì´ˆ"
        elif seconds < 3600:
            minutes = int(seconds // 60)
            secs = int(seconds % 60)
            return f"{minutes}ë¶„ {secs}ì´ˆ"
        else:
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            return f"{hours}ì‹œê°„ {minutes}ë¶„"

# ê¸€ë¡œë²Œ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
monitor = PerformanceMonitor()

class SmallDataBenchmark:
    """ì‘ì€ ë°ì´í„°ì…‹ ì „ìš© ë¹ ë¥¸ ë²¤ì¹˜ë§ˆí‚¹"""
    
    # ë‚´ë¶€ ì„¤ì • - ê¸°ë³¸ê°’: True (ì´ˆë°˜ 1200í”„ë ˆì„ë§Œ ì²˜ë¦¬)
    # ì „ì²´ í”„ë ˆì„ ì²˜ë¦¬í•˜ë ¤ë©´ QUICK_TEST = False ë¡œ ë³€ê²½
    QUICK_TEST = True  # 1200í”„ë ˆì„ë§Œ ë²¤ì¹˜ë§ˆí‚¹
    FRAME_LIMIT = 1200
    
    # ìºì‹± ì„¤ì •
    ENABLE_CACHING = True  # ì²˜ë¦¬ ì™„ë£Œëœ ë°ì´í„° ìºì‹± í™œì„±í™”
    ENABLE_LANDMARK_CACHING = True  # MediaPipe landmark ë°ì´í„° ìºì‹± í™œì„±í™”
    
    # ë¹„ë””ì˜¤ ìƒì„± ì„¤ì •
    GENERATE_DETAILED_VIDEO = True  # ìƒì„¸ ë¹„êµ ë¹„ë””ì˜¤ ìƒì„± (Hand landmarks í¬í•¨, VSCode í˜¸í™˜)
    
    # ìë™ ì„ê³„ê°’ ì„¤ì •
    AUTO_THRESHOLD = False  # ìë™ ì„ê³„ê°’ ì‚¬ìš© ì—¬ë¶€
    THRESHOLD_METHOD = 'midi_based'  # 'statistical', 'clustering', 'valley', 'midi_based'
    FALLBACK_THRESHOLD = 0.9  # ìë™ ê³„ì‚° ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
    
    # ë¹„ë””ì˜¤ ì²˜ë¦¬ ì„¤ì •
    TARGET_VIDEO_DIR = "/home/jhbae/PianoVAM-Code/FingeringDetection/videocapture"
    MAX_VIDEOS = 5  # ì²˜ë¦¬í•  ë¹„ë””ì˜¤ ê°œìˆ˜ ì œí•œ
    FIXED_THRESHOLD = 0.9  # ê³ ì • ì„ê³„ê°’
    
    def __init__(self):
        self.script_dir = os.path.dirname(os.path.abspath(__file__))
        self.video_capture_dir = self.TARGET_VIDEO_DIR  # ìƒˆë¡œìš´ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        self.target_videos = []  # ì²˜ë¦¬í•  ë¹„ë””ì˜¤ ëª©ë¡
        self.current_video = None  # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ë¹„ë””ì˜¤
        self.quick_test = self.QUICK_TEST
        self.frame_limit = self.FRAME_LIMIT
        self.enable_caching = self.ENABLE_CACHING
        self.enable_landmark_caching = self.ENABLE_LANDMARK_CACHING
        self.generate_detailed_video = self.GENERATE_DETAILED_VIDEO
        
        # ê³ ì • ì„ê³„ê°’ ì„¤ì • (ìë™ ì„ê³„ê°’ ë¹„í™œì„±í™”)
        self.auto_threshold = False  # ìë™ ì„ê³„ê°’ ë¹„í™œì„±í™”
        self.threshold_method = self.THRESHOLD_METHOD
        self.fallback_threshold = self.FIXED_THRESHOLD  # 0.9ë¡œ ê³ ì •
        
        # ì„ê³„ê°’ ìºì‹± (í•œ ë²ˆ ê³„ì‚°ëœ ì„ê³„ê°’ ì¬ì‚¬ìš©)
        self.cached_threshold = {'Left': self.FIXED_THRESHOLD, 'Right': self.FIXED_THRESHOLD}
        
        # ì²˜ë¦¬í•  ë¹„ë””ì˜¤ ëª©ë¡ ì´ˆê¸°í™”
        self._initialize_video_list()
        
        # ìºì‹± ê²½ë¡œ ì„¤ì •
        self.cache_dir = os.path.join(self.script_dir, 'cache')
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.cache_dir, exist_ok=True)
        
    def _initialize_video_list(self):
        """ì²˜ë¦¬ ê°€ëŠ¥í•œ ë¹„ë””ì˜¤ ëª©ë¡ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤"""
        if not os.path.exists(self.video_capture_dir):
            print(f"âŒ ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.video_capture_dir}")
            return
        
        # .mp4 íŒŒì¼ê³¼ ëŒ€ì‘ë˜ëŠ” ë°ì´í„°ê°€ ìˆëŠ” ë¹„ë””ì˜¤ë“¤ì„ ì°¾ê¸°
        for item in os.listdir(self.video_capture_dir):
            if item.endswith('.mp4'):
                video_name = item[:-4]
                
                # í•´ë‹¹ pkl ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
                data_dir = os.path.join(self.video_capture_dir, f"{video_name}_858550")
                if os.path.exists(data_dir):
                    handlist_file = os.path.join(data_dir, f"handlist_{video_name}_858550.pkl")
                    floating_file = os.path.join(data_dir, f"floatingframes_{video_name}_858550.pkl")
                    
                    if os.path.exists(handlist_file) and os.path.exists(floating_file):
                        self.target_videos.append(video_name)
        
        # MAX_VIDEOS ì œí•œ ì ìš©
        if len(self.target_videos) > self.MAX_VIDEOS:
            self.target_videos = self.target_videos[:self.MAX_VIDEOS]
        
        print(f"ğŸ“Š ì²˜ë¦¬ ëŒ€ìƒ ë¹„ë””ì˜¤: {len(self.target_videos)}ê°œ")
        for i, video in enumerate(self.target_videos, 1):
            print(f"   {i}. {video}")
    
    def set_current_video(self, video_name: str):
        """í˜„ì¬ ì²˜ë¦¬í•  ë¹„ë””ì˜¤ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤"""
        self.current_video = video_name
        self.target_video = video_name  # ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±
        
        # ìºì‹± ê²½ë¡œ ì—…ë°ì´íŠ¸
        self.limited_video_path = os.path.join(self.cache_dir, f"{video_name}_limit{self.frame_limit}.mp4")
        self.cache_data_dir = os.path.join(self.cache_dir, f"{video_name}_limit{self.frame_limit}_data")
        self.landmark_cache_path = os.path.join(self.cache_dir, f"landmarks_{video_name}_limit{self.frame_limit}.pkl")
    
    def check_landmark_cache(self, video_path: str) -> bool:
        """Landmark ìºì‹œê°€ ìœ íš¨í•œì§€ í™•ì¸"""
        if not self.enable_landmark_caching:
            return False
        
        if not os.path.exists(self.landmark_cache_path):
            return False
        
        # ë¹„ë””ì˜¤ íŒŒì¼ê³¼ ìºì‹œ íŒŒì¼ì˜ ìˆ˜ì • ì‹œê°„ ë¹„êµ
        try:
            video_mtime = os.path.getmtime(video_path)
            cache_mtime = os.path.getmtime(self.landmark_cache_path)
            
            if cache_mtime > video_mtime:
                print(f"âœ… Landmark ìºì‹œ ë°œê²¬: {self.landmark_cache_path}")
                cache_size = os.path.getsize(self.landmark_cache_path) / 1024**2
                print(f"   ğŸ“ ìºì‹œ í¬ê¸°: {cache_size:.1f}MB")
                return True
            else:
                print(f"âš ï¸  Landmark ìºì‹œê°€ ì˜¤ë˜ë¨ (ë¹„ë””ì˜¤ íŒŒì¼ì´ ë” ìƒˆë¡œì›€)")
                return False
                
        except Exception as e:
            print(f"âŒ Landmark ìºì‹œ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False
    
    def load_landmark_cache(self) -> Dict[str, Any]:
        """ìºì‹œëœ landmark ë°ì´í„° ë¡œë“œ"""
        try:
            print(f"ğŸ“ ìºì‹œëœ landmark ë°ì´í„° ë¡œë”© ì¤‘...")
            with open(self.landmark_cache_path, 'rb') as f:
                cached_data = pickle.load(f)
            
            frame_count = len(cached_data.get('handlist_by_frame', {}))
            hand_count = sum(len(hands) for hands in cached_data.get('handlist_by_frame', {}).values())
            
            print(f"âœ… Landmark ìºì‹œ ë¡œë”© ì™„ë£Œ:")
            print(f"   ğŸ–¼ï¸  í”„ë ˆì„ ìˆ˜: {frame_count:,}ê°œ")
            print(f"   ğŸ‘‹ ì´ ì† ë°ì´í„°: {hand_count:,}ê°œ")
            print(f"   âš¡ MediaPipe ì¶”ì¶œ ê³¼ì • ìƒëµë¨")
            
            return cached_data
            
        except Exception as e:
            print(f"âŒ Landmark ìºì‹œ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def save_landmark_cache(self, handlist_by_frame: Dict, total_frames: int):
        """Landmark ë°ì´í„°ë¥¼ ìºì‹œë¡œ ì €ì¥"""
        if not self.enable_landmark_caching:
            return
        
        try:
            print(f"ğŸ’¾ Landmark ë°ì´í„° ìºì‹œ ì €ì¥ ì¤‘...")
            
            cache_data = {
                'handlist_by_frame': handlist_by_frame,
                'total_frames': total_frames,
                'video_name': self.target_video,
                'frame_limit': self.frame_limit,
                'created_at': datetime.now().isoformat(),
                'version': '1.0'
            }
            
            with open(self.landmark_cache_path, 'wb') as f:
                pickle.dump(cache_data, f)
            
            cache_size = os.path.getsize(self.landmark_cache_path) / 1024**2
            hand_count = sum(len(hands) for hands in handlist_by_frame.values())
            
            print(f"âœ… Landmark ìºì‹œ ì €ì¥ ì™„ë£Œ:")
            print(f"   ğŸ“ íŒŒì¼: {self.landmark_cache_path}")
            print(f"   ğŸ“Š í¬ê¸°: {cache_size:.1f}MB")
            print(f"   ğŸ‘‹ ì† ë°ì´í„°: {hand_count:,}ê°œ")
            print(f"   ğŸš€ ë‹¤ìŒ ì‹¤í–‰ ì‹œ ë¹ ë¥¸ ë¡œë”© ê°€ëŠ¥")
            
        except Exception as e:
            print(f"âŒ Landmark ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def calculate_auto_threshold(self, handlist: List) -> Dict[str, float]:
        """ìë™ ì„ê³„ê°’ ê³„ì‚° - MIDI ê¸°ë°˜ ë°©ë²•ë§Œ ì‚¬ìš© (íˆìŠ¤í† ê·¸ë¨ ê³¨ì§œê¸° ë°©ì‹ ì™„ì „ ì œê±°)"""
        
        if self.threshold_method == 'midi_based':
            return self.calculate_midi_based_threshold(handlist)
        else:
            print(f"âš ï¸  íˆìŠ¤í† ê·¸ë¨ ê³¨ì§œê¸° ë°©ì‹ì€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(f"   MIDI ê¸°ë°˜ ë°©ë²•ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ ê³„ì‚°í•©ë‹ˆë‹¤.")
            return self.calculate_midi_based_threshold(handlist)
    
    def calculate_midi_based_threshold(self, handlist: List) -> Dict[str, float]:
        """MIDI ë°ì´í„° ê¸°ë°˜ ì„ê³„ê°’ ê³„ì‚° - ì •êµí•œ í†µê³„í•™ì  ì ‘ê·¼"""
        print(f"ğŸ¹ ì •êµí•œ MIDI ê¸°ë°˜ ì¢Œìš° ì† ì„ê³„ê°’ ê³„ì‚° ì¤‘...")
        
        if midicomparison is None:
            print(f"MIDI ëª¨ë“ˆ ì—†ìŒ, fallback ì‚¬ìš©: Left={self.fallback_threshold}, Right={self.fallback_threshold}")
            return {'Left': self.fallback_threshold, 'Right': self.fallback_threshold}
        
        midi_path = os.path.join(self.script_dir, 'midiconvert', f"{self.target_video}.mid")
        
        if not os.path.exists(midi_path):
            print(f"MIDI íŒŒì¼ ì—†ìŒ, fallback ì‚¬ìš©: Left={self.fallback_threshold}, Right={self.fallback_threshold}")
            return {'Left': self.fallback_threshold, 'Right': self.fallback_threshold}
        
        try:
            video_path = os.path.join(self.video_capture_dir, f"{self.target_video}.mp4")
            cap = cv2.VideoCapture(video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            cap.release()
            
            midi_filename = f"{self.target_video}"
            tokenlist = midicomparison.miditotoken(midi_filename, fps, "simplified")
            total_frames = max([max([hand.handframe for hand in hands]) for hands in handlist if hands]) + 1
            frame_midi_info = midicomparison.tokentoframeinfo(tokenlist, total_frames)
            
            # ì¢Œìš° ì† êµ¬ë¶„í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘
            hand_data = {
                'Left': {'playing': [], 'non_playing': []},
                'Right': {'playing': [], 'non_playing': []}
            }
            
            for frame_idx, hands in enumerate(handlist):
                if self.quick_test and frame_idx >= self.frame_limit:
                    break
                    
                if not hands or frame_idx >= len(frame_midi_info):
                    continue
                
                current_midi = frame_midi_info[frame_idx]
                is_playing = len(current_midi) > 0
                
                for hand in hands:
                    if hasattr(hand, 'handdepth') and hand.handdepth > 0 and hasattr(hand, 'handtype'):
                        hand_type = hand.handtype
                        if hand_type in hand_data:
                            if is_playing:
                                hand_data[hand_type]['playing'].append(hand.handdepth)
                            else:
                                hand_data[hand_type]['non_playing'].append(hand.handdepth)
            
            # ê° ì† íƒ€ì…ë³„ë¡œ ì •êµí•œ ì„ê³„ê°’ ê³„ì‚°
            thresholds = {}
            
            for hand_type in ['Left', 'Right']:
                playing_depths = hand_data[hand_type]['playing']
                non_playing_depths = hand_data[hand_type]['non_playing']
                
                print(f"\nğŸ“Š {hand_type} ì† ë°ì´í„° ë¶„ì„:")
                print(f"   ì—°ì£¼ ì¤‘: {len(playing_depths)}ê°œ ìƒ˜í”Œ")
                print(f"   ë¹„ì—°ì£¼ ì¤‘: {len(non_playing_depths)}ê°œ ìƒ˜í”Œ")
                
                if len(playing_depths) < 20 or len(non_playing_depths) < 20:
                    print(f"   âš ï¸  ë°ì´í„° ë¶€ì¡±, fallback ì‚¬ìš©: {self.fallback_threshold}")
                    thresholds[hand_type] = self.fallback_threshold
                    continue
                
                # ì •êµí•œ í†µê³„í•™ì  ì„ê³„ê°’ ê³„ì‚°
                optimal_threshold = self._calculate_optimal_threshold_advanced(
                    playing_depths, non_playing_depths, hand_type
                )
                
                if 0.1 <= optimal_threshold <= 1.5:
                    thresholds[hand_type] = float(optimal_threshold)
                    print(f"   âœ… {hand_type} ì† ìµœì  ì„ê³„ê°’: {optimal_threshold:.4f}")
                else:
                    print(f"   âŒ ê³„ì‚°ëœ ì„ê³„ê°’ ë²”ìœ„ ì´ˆê³¼: {optimal_threshold:.4f}")
                    print(f"   ğŸ”„ Fallback ì„ê³„ê°’ ì‚¬ìš©: {self.fallback_threshold}")
                    thresholds[hand_type] = self.fallback_threshold
            
            print(f"\nğŸ¯ ì •êµí•œ ì¢Œìš° ì† ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ:")
            print(f"   Left={thresholds.get('Left', self.fallback_threshold):.4f}")
            print(f"   Right={thresholds.get('Right', self.fallback_threshold):.4f}")
            
            return thresholds
                
        except Exception as e:
            print(f"MIDI ì„ê³„ê°’ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {'Left': self.fallback_threshold, 'Right': self.fallback_threshold}
    
    def _calculate_optimal_threshold_advanced(self, playing_depths: List, non_playing_depths: List, hand_type: str) -> float:
        """ì˜¤ë¶„ë¥˜ ìµœì†Œí™”ë¡œ ìµœì  ì„ê³„ê°’ ê³„ì‚° - ë””ë²„ê¹… ê°•í™”"""
        
        print(f"   ğŸ¯ {hand_type} ì† ì˜¤ë¶„ë¥˜ ìµœì†Œí™” ë¶„ì„ ì‹œì‘...")
        
        playing_array = np.array(playing_depths)
        non_playing_array = np.array(non_playing_depths)
        
        # ê¸°ë³¸ í†µê³„
        playing_mean = np.mean(playing_array)
        playing_std = np.std(playing_array)
        non_playing_mean = np.mean(non_playing_array)
        non_playing_std = np.std(non_playing_array)
        
        print(f"   ğŸ“Š ì—°ì£¼ ì¤‘ í†µê³„: Î¼={playing_mean:.3f}, Ïƒ={playing_std:.3f} ({len(playing_array)}ê°œ)")
        print(f"   ğŸ“Š ë¹„ì—°ì£¼ ì¤‘ í†µê³„: Î¼={non_playing_mean:.3f}, Ïƒ={non_playing_std:.3f} ({len(non_playing_array)}ê°œ)")
        
        # ë°ì´í„° ë²”ìœ„ í™•ì¸
        print(f"   ğŸ“ˆ ë°ì´í„° ë²”ìœ„:")
        print(f"      ì—°ì£¼ ì¤‘: {np.min(playing_array):.3f} ~ {np.max(playing_array):.3f}")
        print(f"      ë¹„ì—°ì£¼ ì¤‘: {np.min(non_playing_array):.3f} ~ {np.max(non_playing_array):.3f}")
        
        # ëª¨ë“  ê°€ëŠ¥í•œ ì„ê³„ê°’ í›„ë³´ ìƒì„±
        all_depths = np.concatenate([playing_array, non_playing_array])
        unique_depths = np.sort(np.unique(all_depths))
        
        print(f"   ğŸ” ì„ê³„ê°’ í›„ë³´: {len(unique_depths)}ê°œ ({unique_depths.min():.3f} ~ {unique_depths.max():.3f})")
        
        # ê° ì„ê³„ê°’ì— ëŒ€í•´ ê· í˜• ì¡íŒ ì •í™•ë„ ê³„ì‚°
        best_threshold = self.fallback_threshold
        max_balanced_accuracy = -1
        best_fp = 0
        best_fn = 0
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ ì„ê³„ê°’ë³„ ê²°ê³¼ ì €ì¥
        debug_results = []
        
        for threshold in unique_depths:
            # False Positive: ì—°ì£¼ ì¤‘ì¸ë° floatingìœ¼ë¡œ ì˜ëª» ë¶„ë¥˜ (depth >= threshold)
            false_positives = np.sum(playing_array >= threshold)
            
            # False Negative: ë¹„ì—°ì£¼ ì¤‘ì¸ë° normalë¡œ ì˜ëª» ë¶„ë¥˜ (depth < threshold)  
            false_negatives = np.sum(non_playing_array < threshold)
            
            # True Positive, True Negative ê³„ì‚°
            true_positives = len(non_playing_array) - false_negatives  # ë¹„ì—°ì£¼ ì¤‘ ì˜¬ë°”ë¥´ê²Œ floating ë¶„ë¥˜
            true_negatives = len(playing_array) - false_positives      # ì—°ì£¼ ì¤‘ ì˜¬ë°”ë¥´ê²Œ normal ë¶„ë¥˜
            
            # ê· í˜• ì¡íŒ ì •í™•ë„ ê³„ì‚° (ë¯¼ê°ë„ì™€ íŠ¹ì´ë„ì˜ í‰ê· )
            sensitivity = true_positives / len(non_playing_array) if len(non_playing_array) > 0 else 0  # ë¹„ì—°ì£¼ ì¤‘ ì •í™•ë„
            specificity = true_negatives / len(playing_array) if len(playing_array) > 0 else 0         # ì—°ì£¼ ì¤‘ ì •í™•ë„
            balanced_accuracy = (sensitivity + specificity) / 2
            
            # ë””ë²„ê¹… ì •ë³´ ì €ì¥
            debug_results.append({
                'threshold': threshold,
                'fp': false_positives,
                'fn': false_negatives,
                'sensitivity': sensitivity * 100,
                'specificity': specificity * 100,
                'balanced_acc': balanced_accuracy * 100
            })
            
            # ê· í˜• ì¡íŒ ì •í™•ë„ê°€ ìµœëŒ€ì¸ ì„ê³„ê°’ ì„ íƒ
            if balanced_accuracy > max_balanced_accuracy:
                max_balanced_accuracy = balanced_accuracy
                best_threshold = threshold
                best_fp = false_positives
                best_fn = false_negatives
        
        # ë””ë²„ê¹…: ìƒìœ„ 5ê°œì™€ í•˜ìœ„ 5ê°œ ì„ê³„ê°’ ê²°ê³¼ ì¶œë ¥
        print(f"   ğŸ” ì„ê³„ê°’ í›„ë³´ ë¶„ì„ (ìƒìœ„ 5ê°œ):")
        sorted_results = sorted(debug_results, key=lambda x: x['balanced_acc'], reverse=True)
        for i, result in enumerate(sorted_results[:5]):
            print(f"      #{i+1}: threshold={result['threshold']:.3f}, FP={result['fp']}, FN={result['fn']}, ì •í™•ë„={result['balanced_acc']:.1f}%")
        
        print(f"   ğŸ” ì„ê³„ê°’ í›„ë³´ ë¶„ì„ (í•˜ìœ„ 5ê°œ):")
        for i, result in enumerate(sorted_results[-5:]):
            print(f"      #{len(sorted_results)-4+i}: threshold={result['threshold']:.3f}, FP={result['fp']}, FN={result['fn']}, ì •í™•ë„={result['balanced_acc']:.1f}%")
        
        # ì˜¤ë¶„ë¥˜ìœ¨ ê³„ì‚°
        total_samples = len(playing_array) + len(non_playing_array)
        misclassification_rate = (best_fp + best_fn) / total_samples * 100
        fp_rate = best_fp / len(playing_array) * 100
        fn_rate = best_fn / len(non_playing_array) * 100
        
        print(f"   ğŸ¯ ìµœì  ì„ê³„ê°’: {best_threshold:.4f}")
        print(f"   ğŸ“Š ì˜¤ë¶„ë¥˜ ë¶„ì„:")
        print(f"      ì´ ì˜¤ë¶„ë¥˜: {best_fp + best_fn}/{total_samples} ({misclassification_rate:.1f}%)")
        print(f"      False Positive: {best_fp}/{len(playing_array)} ({fp_rate:.1f}%) - ì—°ì£¼ë°©í•´")
        print(f"      False Negative: {best_fn}/{len(non_playing_array)} ({fn_rate:.1f}%) - ë†“ì¹œê°ì§€")
        
        # ğŸš¨ ë¬¸ì œ ì§„ë‹¨
        if fp_rate > 90:
            print(f"   ğŸš¨ ë¬¸ì œ ê°ì§€: False Positive ë¹„ìœ¨ì´ {fp_rate:.1f}%ë¡œ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤!")
            print(f"      â†’ ê±°ì˜ ëª¨ë“  ì—°ì£¼ê°€ floatingìœ¼ë¡œ ì˜ëª» ë¶„ë¥˜ë¨")
            print(f"      â†’ ì„ê³„ê°’ {best_threshold:.3f}ì´ ë„ˆë¬´ ë‚®ì„ ê°€ëŠ¥ì„±")
        
        if fn_rate > 90:
            print(f"   ğŸš¨ ë¬¸ì œ ê°ì§€: False Negative ë¹„ìœ¨ì´ {fn_rate:.1f}%ë¡œ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤!")
            print(f"      â†’ ê±°ì˜ ëª¨ë“  ë¹„ì—°ì£¼ê°€ normalë¡œ ì˜ëª» ë¶„ë¥˜ë¨")
            print(f"      â†’ ì„ê³„ê°’ {best_threshold:.3f}ì´ ë„ˆë¬´ ë†’ì„ ê°€ëŠ¥ì„±")
        
        # ë¶„ë¥˜ ì •í™•ë„ ê³„ì‚°
        correct_playing = len(playing_array) - best_fp  # ì—°ì£¼ ì¤‘ ì˜¬ë°”ë¥´ê²Œ normal ë¶„ë¥˜
        correct_non_playing = len(non_playing_array) - best_fn  # ë¹„ì—°ì£¼ ì¤‘ ì˜¬ë°”ë¥´ê²Œ floating ë¶„ë¥˜
        
        playing_accuracy = correct_playing / len(playing_array) * 100
        non_playing_accuracy = correct_non_playing / len(non_playing_array) * 100
        overall_accuracy = (correct_playing + correct_non_playing) / total_samples * 100
        
        print(f"   âœ… ë¶„ë¥˜ ì •í™•ë„:")
        print(f"      ì—°ì£¼ ì¤‘ ì •í™•ë„: {playing_accuracy:.1f}% ({correct_playing}/{len(playing_array)})")
        print(f"      ë¹„ì—°ì£¼ ì¤‘ ì •í™•ë„: {non_playing_accuracy:.1f}% ({correct_non_playing}/{len(non_playing_array)})")
        print(f"      ì „ì²´ ì •í™•ë„: {overall_accuracy:.1f}%")
        
        # ì„ê³„ê°’ í’ˆì§ˆ í‰ê°€
        if misclassification_rate <= 5:
            print(f"   ğŸŒŸ ìš°ìˆ˜í•œ ì„ê³„ê°’: ì˜¤ë¶„ë¥˜ìœ¨ {misclassification_rate:.1f}%")
        elif misclassification_rate <= 15:
            print(f"   âœ… ì–‘í˜¸í•œ ì„ê³„ê°’: ì˜¤ë¶„ë¥˜ìœ¨ {misclassification_rate:.1f}%")
        elif misclassification_rate <= 30:
            print(f"   âš ï¸  ë³´í†µ ì„ê³„ê°’: ì˜¤ë¶„ë¥˜ìœ¨ {misclassification_rate:.1f}%")
        else:
            print(f"   âŒ ë¬¸ì œ ìˆëŠ” ì„ê³„ê°’: ì˜¤ë¶„ë¥˜ìœ¨ {misclassification_rate:.1f}%")
            print(f"      â†’ ë°ì´í„° ë¶„í¬ ì¬ê²€í†  í•„ìš”")
        
        # ë¶„í¬ ê²¹ì¹¨ ì •ë„ ë¶„ì„
        overlap_analysis = self._analyze_distribution_overlap(playing_array, non_playing_array)
        print(f"   ğŸ“ˆ ë¶„í¬ ê²¹ì¹¨ ë¶„ì„:")
        print(f"      ì—°ì£¼ì¤‘ ë²”ìœ„: {overlap_analysis['playing_range']}")
        print(f"      ë¹„ì—°ì£¼ì¤‘ ë²”ìœ„: {overlap_analysis['non_playing_range']}")
        print(f"      ê²¹ì¹¨ ì •ë„: {overlap_analysis['overlap_description']}")
        
        # ğŸ” ì¶”ê°€ ì§„ë‹¨: ë°ì´í„° ë¶„í¬ í™•ì¸
        if playing_mean > non_playing_mean:
            print(f"   âš ï¸  ë°ì´í„° ë¶„í¬ ì´ìƒ: ì—°ì£¼ ì¤‘ í‰ê· ({playing_mean:.3f})ì´ ë¹„ì—°ì£¼ ì¤‘ í‰ê· ({non_playing_mean:.3f})ë³´ë‹¤ í½ë‹ˆë‹¤")
            print(f"      â†’ ì¼ë°˜ì ìœ¼ë¡œ ì—°ì£¼ ì¤‘ì´ ë” ì‘ì€ depth ê°’ì„ ê°€ì ¸ì•¼ í•¨")
        
        return best_threshold
    
    def _analyze_distribution_overlap(self, playing_array: np.ndarray, non_playing_array: np.ndarray) -> Dict[str, str]:
        """ë‘ ë¶„í¬ì˜ ê²¹ì¹¨ ì •ë„ ë¶„ì„"""
        
        playing_min = np.min(playing_array)
        playing_max = np.max(playing_array)
        non_playing_min = np.min(non_playing_array)
        non_playing_max = np.max(non_playing_array)
        
        # ê²¹ì¹¨ êµ¬ê°„ ê³„ì‚°
        overlap_start = max(playing_min, non_playing_min)
        overlap_end = min(playing_max, non_playing_max)
        
        if overlap_start >= overlap_end:
            # ê²¹ì¹˜ì§€ ì•ŠìŒ
            gap_size = overlap_start - overlap_end
            overlap_description = f"ë¶„ë¦¬ë¨ (ê°„ê²©: {gap_size:.3f})"
        else:
            # ê²¹ì¹¨ ì¡´ì¬
            overlap_size = overlap_end - overlap_start
            total_range = max(playing_max, non_playing_max) - min(playing_min, non_playing_min)
            overlap_ratio = overlap_size / total_range * 100
            overlap_description = f"ê²¹ì¹¨ {overlap_ratio:.1f}% ({overlap_start:.3f}~{overlap_end:.3f})"
        
        return {
            'playing_range': f"{playing_min:.3f}~{playing_max:.3f}",
            'non_playing_range': f"{non_playing_min:.3f}~{non_playing_max:.3f}",
            'overlap_description': overlap_description
        }
    
    def _evaluate_threshold_performance(self, threshold: float, playing_array: np.ndarray, non_playing_array: np.ndarray) -> Dict[str, float]:
        """ì„ê³„ê°’ ì„±ëŠ¥ í‰ê°€"""
        
        # ì˜ˆì¸¡ ìƒì„±
        playing_predictions = (playing_array >= threshold).astype(int)
        non_playing_predictions = (non_playing_array >= threshold).astype(int)
        
        # ì‹¤ì œ ë ˆì´ë¸” (ì—°ì£¼ ì¤‘ = 0, ë¹„ì—°ì£¼ ì¤‘ = 1)
        playing_labels = np.zeros(len(playing_array))
        non_playing_labels = np.ones(len(non_playing_array))
        
        # ëª¨ë“  ë°ì´í„° í•©ì¹˜ê¸°
        all_predictions = np.concatenate([playing_predictions, non_playing_predictions])
        all_labels = np.concatenate([playing_labels, non_playing_labels])
        
        # í˜¼ë™ í–‰ë ¬ ê³„ì‚°
        tp = np.sum((all_predictions == 1) & (all_labels == 1))
        fp = np.sum((all_predictions == 1) & (all_labels == 0))
        tn = np.sum((all_predictions == 0) & (all_labels == 0))
        fn = np.sum((all_predictions == 0) & (all_labels == 1))
        
        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        accuracy = (tp + tn) / (tp + fp + tn + fn) * 100 if (tp + fp + tn + fn) > 0 else 0
        sensitivity = tp / (tp + fn) * 100 if (tp + fn) > 0 else 0  # Recall
        specificity = tn / (tn + fp) * 100 if (tn + fp) > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        f1_score = 2 * (precision * sensitivity / 100) / (precision + sensitivity / 100) if (precision + sensitivity) > 0 else 0
        
        return {
            'accuracy': accuracy,
            'sensitivity': sensitivity,
            'specificity': specificity,
            'precision': precision * 100,
            'f1_score': f1_score
        }
    
    def calculate_valley_threshold(self, handlist: List) -> float:
        """íˆìŠ¤í† ê·¸ë¨ ê³¨ì§œê¸° ì°¾ê¸° ë°©ë²•ìœ¼ë¡œ ì„ê³„ê°’ ê³„ì‚°"""
        print(f"ğŸ”ï¸  íˆìŠ¤í† ê·¸ë¨ ê³¨ì§œê¸° ì°¾ê¸°ë¡œ ìë™ ì„ê³„ê°’ ê³„ì‚° ì¤‘...")
        
        # ëª¨ë“  ì†ì˜ ê¹Šì´ ê°’ ìˆ˜ì§‘
        all_depths = []
        for hands in handlist:
            for hand in hands:
                if hasattr(hand, 'handdepth') and hand.handdepth > 0:
                    all_depths.append(hand.handdepth)
        
        print(f"   ğŸ“Š ì´ ê¹Šì´ ë°ì´í„°: {len(all_depths):,}ê°œ")
        
        # ìƒˆë¡œìš´ ë©”ì„œë“œ ì‚¬ìš©
        threshold = self.calculate_valley_threshold_with_data(all_depths)
        
        if len(all_depths) >= 50:
            # íˆìŠ¤í† ê·¸ë¨ ì •ë³´ ì¶œë ¥
            all_depths_array = np.array(all_depths)
            floating_count = np.sum(all_depths_array >= threshold)
            floating_ratio = floating_count / len(all_depths) * 100
            print(f"âœ… íˆìŠ¤í† ê·¸ë¨ ê³¨ì§œê¸° ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ:")
            print(f"   ğŸª ì˜ˆìƒ floating ë¹„ìœ¨: {floating_ratio:.1f}% ({floating_count:,}ê°œ)")
        
        return threshold
    
    def calculate_valley_threshold_with_data(self, depths_data: List) -> float:
        """ì£¼ì–´ì§„ ê¹Šì´ ë°ì´í„°ë¡œ íˆìŠ¤í† ê·¸ë¨ ê³¨ì§œê¸° ì°¾ê¸°"""
        print(f"ğŸ”ï¸  íˆìŠ¤í† ê·¸ë¨ ê³¨ì§œê¸° ì°¾ê¸° (ë°ì´í„°: {len(depths_data):,}ê°œ)")
        
        if len(depths_data) < 50:
            print(f"âš ï¸  ê¹Šì´ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©: {self.fallback_threshold}")
            return self.fallback_threshold
        
        all_depths = np.array(depths_data)
        
        # íˆìŠ¤í† ê·¸ë¨ ìƒì„±
        optimal_bins = min(100, max(20, len(all_depths) // 20))
        hist, bins = np.histogram(all_depths, bins=optimal_bins)
        
        # íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”
        from scipy.ndimage import gaussian_filter1d
        try:
            smoothed_hist = gaussian_filter1d(hist.astype(float), sigma=1.0)
        except ImportError:
            smoothed_hist = hist.astype(float)
            for i in range(1, len(smoothed_hist)-1):
                smoothed_hist[i] = (hist[i-1] + hist[i] + hist[i+1]) / 3.0
        
        # ê³¨ì§œê¸° ì°¾ê¸°
        valley_indices = []
        for i in range(2, len(smoothed_hist)-2):
            if (smoothed_hist[i] < smoothed_hist[i-1] and 
                smoothed_hist[i] < smoothed_hist[i+1] and
                smoothed_hist[i] < smoothed_hist[i-2] and
                smoothed_hist[i] < smoothed_hist[i+2]):
                valley_indices.append(i)
        
        if valley_indices:
            # ê°€ì¥ ê¹Šì€ ê³¨ì§œê¸° ì„ íƒ
            deepest_valley_idx = min(valley_indices, key=lambda x: smoothed_hist[x])
            threshold = bins[deepest_valley_idx]
            
            print(f"   ğŸ”ï¸  ë°œê²¬ëœ ê³¨ì§œê¸°: {len(valley_indices)}ê°œ")
            print(f"   ğŸ¯ ì„ íƒëœ ì„ê³„ê°’: {threshold:.3f}")
            print(f"   ğŸ“ˆ ë°ì´í„° ë²”ìœ„: {all_depths.min():.3f} ~ {all_depths.max():.3f}")
            
            return float(threshold)
        else:
            # ê³¨ì§œê¸°ê°€ ì—†ìœ¼ë©´ 25 ë°±ë¶„ìœ„ìˆ˜ ì‚¬ìš©
            alternative_threshold = np.percentile(all_depths, 25)
            print(f"   âš ï¸  ê³¨ì§œê¸° ì—†ìŒ. 25 ë°±ë¶„ìœ„ìˆ˜ ì‚¬ìš©: {alternative_threshold:.3f}")
            
            if 0.1 <= alternative_threshold <= 1.5:
                return float(alternative_threshold)
            else:
                return self.fallback_threshold
    
    def get_dynamic_threshold(self, handlist: List) -> Dict[str, float]:
        """ë™ì  ì„ê³„ê°’ ê³„ì‚° - ìºì‹±ìœ¼ë¡œ ì¤‘ë³µ ê³„ì‚° ë°©ì§€"""
        if not self.auto_threshold:
            return {'Left': self.fallback_threshold, 'Right': self.fallback_threshold}
        
        # ì´ë¯¸ ê³„ì‚°ëœ ì„ê³„ê°’ì´ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
        if self.cached_threshold is not None:
            if isinstance(self.cached_threshold, dict):
                print(f"ğŸ¯ ìºì‹œëœ ì¢Œìš° ì† ì„ê³„ê°’ ì‚¬ìš©: Left={self.cached_threshold['Left']:.3f}, Right={self.cached_threshold['Right']:.3f}")
                return self.cached_threshold
            else:
                # ê¸°ì¡´ ë‹¨ì¼ ì„ê³„ê°’ì„ ì¢Œìš° ì† ê³µí†µìœ¼ë¡œ ì‚¬ìš©
                threshold_dict = {'Left': self.cached_threshold, 'Right': self.cached_threshold}
                print(f"ğŸ¯ ìºì‹œëœ ê³µí†µ ì„ê³„ê°’ ì‚¬ìš©: {self.cached_threshold:.3f}")
                return threshold_dict
        
        print(f"ğŸ¯ ë™ì  ì¢Œìš° ì† ì„ê³„ê°’ ê³„ì‚° ì‹œì‘...")
        print(f"   AUTO_THRESHOLD: {self.auto_threshold}")
        print(f"   THRESHOLD_METHOD: {self.threshold_method}")
        print(f"   FALLBACK_THRESHOLD: {self.fallback_threshold}")
        
        thresholds = self.calculate_auto_threshold(handlist)
        
        # ê° ì† íƒ€ì…ë³„ë¡œ ë²”ìœ„ í™•ì¸
        for hand_type in ['Left', 'Right']:
            if hand_type in thresholds:
                threshold = thresholds[hand_type]
                if threshold < 0.1 or threshold > 1.5:
                    print(f"âš ï¸  {hand_type} ì† ì„ê³„ê°’ ë²”ìœ„ ì´ˆê³¼: {threshold:.3f} â†’ fallback ì‚¬ìš©: {self.fallback_threshold}")
                    thresholds[hand_type] = self.fallback_threshold
        
        # ê³„ì‚°ëœ ì„ê³„ê°’ ìºì‹±
        self.cached_threshold = thresholds
        print(f"âœ… ì¢Œìš° ì† ì„ê³„ê°’ ìºì‹œ ì €ì¥: Left={thresholds.get('Left', self.fallback_threshold):.3f}, Right={thresholds.get('Right', self.fallback_threshold):.3f}")
        
        return thresholds
    
    def find_target_data(self) -> Dict[str, Any]:
        """íƒ€ê²Ÿ ë°ì´í„° íŒŒì¼ì„ ì°¾ê±°ë‚˜ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤"""
        print(f"ğŸ” íƒ€ê²Ÿ ë°ì´í„° ê²€ìƒ‰ ì¤‘: {self.target_video}")
        
        # 1. ìºì‹œëœ ì œí•œ ë°ì´í„° í™•ì¸ (Quick test ëª¨ë“œì¼ ë•Œ)
        if self.quick_test and self.enable_caching:
            if self.check_cached_data():
                return self.load_cached_data()
        
        # 2. ê¸°ì¡´ ì²˜ë¦¬ëœ ë°ì´í„° í™•ì¸ (ì „ì²´ ë°ì´í„°)
        if not self.quick_test:
            for item in os.listdir(self.video_capture_dir):
                item_path = os.path.join(self.video_capture_dir, item)
                
                if (os.path.isdir(item_path) and 
                    item.startswith(self.target_video) and 
                    '_' in item and item.split('_')[-1].isdigit()):
                    
                    # handlistì™€ floatingframes íŒŒì¼ í™•ì¸
                    handlist_files = [f for f in os.listdir(item_path) 
                                    if f.startswith(f"handlist_{self.target_video}_") and f.endswith('.pkl')]
                    floating_files = [f for f in os.listdir(item_path) 
                                    if f.startswith(f"floatingframes_{self.target_video}_") and f.endswith('.pkl')]
                    
                    if handlist_files and floating_files:
                        print(f"âœ… ê¸°ì¡´ ì²˜ë¦¬ëœ ë°ì´í„° ë°œê²¬: {item_path}")
                        return {
                            'video_name': self.target_video,
                            'data_dir': item_path,
                            'handlist_file': os.path.join(item_path, handlist_files[0]),
                            'floating_file': os.path.join(item_path, floating_files[0]),
                            'original_video': f"{self.target_video}.mp4"
                        }
        
        # 3. ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ìƒì„±
        print(f"âŒ ì²˜ë¦¬ëœ ë°ì´í„° ì—†ìŒ. ìë™ìœ¼ë¡œ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘...")
        
        if self.quick_test:
            print(f"ğŸš€ Quick Test ëª¨ë“œ: {self.frame_limit}í”„ë ˆì„ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤")
            return self.process_limited_video_data()
        else:
            print(f"âš ï¸  ì „ì²´ ë¹„ë””ì˜¤ ì²˜ë¦¬ ëª¨ë“œ")
            return self.process_video_data()
    
    def process_video_data(self) -> Dict[str, Any]:
        """ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤"""
        video_file = f"{self.target_video}.mp4"
        video_path = os.path.join(self.video_capture_dir, video_file)
        
        # ë¹„ë””ì˜¤ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ ì—†ìŒ: {video_path}")
        
        # keyboardcoordinateinfo.pkl íŒŒì¼ í™•ì¸
        keyboard_info_path = os.path.join(self.script_dir, "keyboardcoordinateinfo.pkl")
        if not os.path.exists(keyboard_info_path):
            raise FileNotFoundError(f"âŒ í‚¤ë³´ë“œ ì¢Œí‘œ ì •ë³´ íŒŒì¼ ì—†ìŒ: {keyboard_info_path}")
        
        # í•´ë‹¹ ë¹„ë””ì˜¤ì˜ í‚¤ë³´ë“œ ì¢Œí‘œ ì •ë³´ í™•ì¸
        try:
            with open(keyboard_info_path, 'rb') as f:
                keyboard_info = pickle.load(f)
            if self.target_video not in keyboard_info:
                raise KeyError(f"âŒ {self.target_video}ì— ëŒ€í•œ í‚¤ë³´ë“œ ì¢Œí‘œ ì •ë³´ ì—†ìŒ")
            print(f"âœ… í‚¤ë³´ë“œ ì¢Œí‘œ ì •ë³´ í™•ì¸: {self.target_video}")
        except Exception as e:
            raise RuntimeError(f"âŒ í‚¤ë³´ë“œ ì¢Œí‘œ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print(f"ğŸ¬ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘: {video_file}")
        print(f"   í¬ê¸°: {os.path.getsize(video_path) / (1024*1024):.1f}MB")
        
        try:
            # main_loop ëª¨ë“ˆì„ ì§€ì—° importí•˜ê³  datagenerate í•¨ìˆ˜ í˜¸ì¶œ
            main_loop = import_main_loop()
            
            # Quick test ëª¨ë“œì¼ ë•Œ í”„ë ˆì„ ì œí•œ ì•Œë¦¼
            if self.quick_test:
                print(f"âš ï¸  ì£¼ì˜: ì „ì²´ ë¹„ë””ì˜¤ ì²˜ë¦¬ í›„ {self.frame_limit}í”„ë ˆì„ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤")
                print(f"   ì²« ì‹¤í–‰ ì‹œì—ëŠ” ì „ì²´ ì²˜ë¦¬ê°€ í•„ìš”í•˜ë©°, ì´í›„ ì‹¤í–‰ì—ì„œëŠ” ë¹ ë¥´ê²Œ ë¡œë”©ë©ë‹ˆë‹¤")
            
            main_loop.datagenerate(video_file)
            print(f"âœ… ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ: {video_file}")
            
            # ì²˜ë¦¬ëœ ë°ì´í„° ìœ„ì¹˜ ë‹¤ì‹œ ì°¾ê¸°
            for item in os.listdir(self.video_capture_dir):
                item_path = os.path.join(self.video_capture_dir, item)
                
                if (os.path.isdir(item_path) and 
                    item.startswith(self.target_video) and 
                    '_' in item and item.split('_')[-1].isdigit()):
                    
                    # handlistì™€ floatingframes íŒŒì¼ í™•ì¸
                    handlist_files = [f for f in os.listdir(item_path) 
                                    if f.startswith(f"handlist_{self.target_video}_") and f.endswith('.pkl')]
                    floating_files = [f for f in os.listdir(item_path) 
                                    if f.startswith(f"floatingframes_{self.target_video}_") and f.endswith('.pkl')]
                    
                    if handlist_files and floating_files:
                        print(f"âœ… ìƒˆë¡œ ìƒì„±ëœ ë°ì´í„° í™•ì¸: {item_path}")
                        return {
                            'video_name': self.target_video,
                            'data_dir': item_path,
                            'handlist_file': os.path.join(item_path, handlist_files[0]),
                            'floating_file': os.path.join(item_path, floating_files[0]),
                            'original_video': f"{self.target_video}.mp4"
                        }
            
            raise RuntimeError(f"âŒ ë°ì´í„° ì²˜ë¦¬ëŠ” ì™„ë£Œë˜ì—ˆì§€ë§Œ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
        except Exception as e:
            print(f"âŒ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def check_cached_data(self) -> bool:
        """ìºì‹œëœ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸"""
        if not os.path.exists(self.cache_data_dir):
            return False
        
        # í•„ìš”í•œ íŒŒì¼ë“¤ í™•ì¸
        handlist_file = os.path.join(self.cache_data_dir, f"handlist_{self.target_video}_limit{self.frame_limit}.pkl")
        floating_file = os.path.join(self.cache_data_dir, f"floatingframes_{self.target_video}_limit{self.frame_limit}.pkl")
        
        return os.path.exists(handlist_file) and os.path.exists(floating_file)
    
    def load_cached_data(self) -> Dict[str, Any]:
        """ìºì‹œëœ ë°ì´í„° ë¡œë“œ"""
        print(f"âœ… ìºì‹œëœ {self.frame_limit}í”„ë ˆì„ ë°ì´í„° ë°œê²¬: {self.cache_data_dir}")
        
        handlist_file = os.path.join(self.cache_data_dir, f"handlist_{self.target_video}_limit{self.frame_limit}.pkl")
        floating_file = os.path.join(self.cache_data_dir, f"floatingframes_{self.target_video}_limit{self.frame_limit}.pkl")
        
        return {
            'video_name': f"{self.target_video}_limit{self.frame_limit}",
            'data_dir': self.cache_data_dir,
            'handlist_file': handlist_file,
            'floating_file': floating_file,
            'original_video': f"{self.target_video}_limit{self.frame_limit}.mp4"
        }
    
    def extract_limited_frames(self, input_path: str, output_path: str) -> bool:
        """ì œí•œëœ í”„ë ˆì„ ì¶”ì¶œ"""
        print(f"ğŸ“¹ ì²˜ìŒ {self.frame_limit}í”„ë ˆì„ ì¶”ì¶œ ì¤‘: {input_path}")
        
        cap = cv2.VideoCapture(input_path)
        if not cap.isOpened():
            print(f"âŒ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨: {input_path}")
            return False
        
        # ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì • (VSCode í˜¸í™˜ì„±ì„ ìœ„í•œ H.264 ì½”ë±)
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        frame_count = 0
        while cap.isOpened() and frame_count < self.frame_limit:
            ret, frame = cap.read()
            if not ret:
                break
            
            out.write(frame)
            frame_count += 1
            
            if frame_count % 100 == 0:
                print(f"   ğŸ“Š ì¶”ì¶œ ì¤‘: {frame_count}/{self.frame_limit} í”„ë ˆì„")
        
        cap.release()
        out.release()
        
        print(f"âœ… {self.frame_limit}í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ: {output_path}")
        return True
    
    def process_limited_video_data(self) -> Dict[str, Any]:
        """ì œí•œëœ í”„ë ˆì„ ë¹„ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬"""
        original_video_path = os.path.join(self.video_capture_dir, f"{self.target_video}.mp4")
        
        # ì›ë³¸ ë¹„ë””ì˜¤ ì¡´ì¬ í™•ì¸
        if not os.path.exists(original_video_path):
            raise FileNotFoundError(f"âŒ ì›ë³¸ ë¹„ë””ì˜¤ ì—†ìŒ: {original_video_path}")
        
        # 1. ì œí•œëœ í”„ë ˆì„ ë¹„ë””ì˜¤ ìƒì„±
        if not os.path.exists(self.limited_video_path):
            print(f"ğŸ“¹ {self.frame_limit}í”„ë ˆì„ ë¹„ë””ì˜¤ ìƒì„± ì¤‘...")
            if not self.extract_limited_frames(original_video_path, self.limited_video_path):
                raise RuntimeError(f"âŒ ì œí•œëœ í”„ë ˆì„ ë¹„ë””ì˜¤ ìƒì„± ì‹¤íŒ¨")
        else:
            print(f"âœ… {self.frame_limit}í”„ë ˆì„ ë¹„ë””ì˜¤ ë°œê²¬: {self.limited_video_path}")
        
        # 2. í‚¤ë³´ë“œ ì¢Œí‘œ ì •ë³´ í™•ì¸
        keyboard_info_path = os.path.join(self.script_dir, "keyboardcoordinateinfo.pkl")
        if not os.path.exists(keyboard_info_path):
            raise FileNotFoundError(f"âŒ í‚¤ë³´ë“œ ì¢Œí‘œ ì •ë³´ íŒŒì¼ ì—†ìŒ: {keyboard_info_path}")
        
        try:
            with open(keyboard_info_path, 'rb') as f:
                keyboard_info = pickle.load(f)
            if self.target_video not in keyboard_info:
                raise KeyError(f"âŒ {self.target_video}ì— ëŒ€í•œ í‚¤ë³´ë“œ ì¢Œí‘œ ì •ë³´ ì—†ìŒ")
            print(f"âœ… í‚¤ë³´ë“œ ì¢Œí‘œ ì •ë³´ í™•ì¸: {self.target_video}")
        except Exception as e:
            raise RuntimeError(f"âŒ í‚¤ë³´ë“œ ì¢Œí‘œ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # 3. ì œí•œëœ ë¹„ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬
        print(f"ğŸ”„ {self.frame_limit}í”„ë ˆì„ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘...")
        
        try:
            # ì„ì‹œë¡œ ì œí•œëœ ë¹„ë””ì˜¤ë¥¼ videocapture ë””ë ‰í† ë¦¬ì— ë³µì‚¬
            temp_video_name = f"{self.target_video}_temp.mp4"
            temp_video_path = os.path.join(self.video_capture_dir, temp_video_name)
            
            import shutil
            shutil.copy2(self.limited_video_path, temp_video_path)
            
            # í‚¤ë³´ë“œ ì¢Œí‘œ ì •ë³´ì— ì„ì‹œ í‚¤ ì¶”ê°€
            keyboard_info_path = os.path.join(self.script_dir, "keyboardcoordinateinfo.pkl")
            with open(keyboard_info_path, 'rb') as f:
                keyboard_info = pickle.load(f)
            
            # ì›ë³¸ í‚¤ë³´ë“œ ì •ë³´ë¥¼ ì„ì‹œ í‚¤ë¡œ ë³µì‚¬
            temp_video_key = f"{self.target_video}_temp"
            keyboard_info[temp_video_key] = keyboard_info[self.target_video]
            
            # ì„ì‹œë¡œ í‚¤ë³´ë“œ ì •ë³´ ì €ì¥
            with open(keyboard_info_path, 'wb') as f:
                pickle.dump(keyboard_info, f)
            
            print(f"âœ… ì„ì‹œ í‚¤ë³´ë“œ ì¢Œí‘œ ì •ë³´ ì¶”ê°€: {temp_video_key}")
            
            # main_loop ëª¨ë“ˆë¡œ ë°ì´í„° ì²˜ë¦¬
            main_loop = import_main_loop()
            main_loop.datagenerate(temp_video_name)
            
            # ì²˜ë¦¬ëœ ë°ì´í„° ì°¾ê¸°
            for item in os.listdir(self.video_capture_dir):
                item_path = os.path.join(self.video_capture_dir, item)
                
                if (os.path.isdir(item_path) and 
                    item.startswith(self.target_video + "_temp")):
                    
                    handlist_files = [f for f in os.listdir(item_path) 
                                    if f.startswith(f"handlist_{self.target_video}_temp_") and f.endswith('.pkl')]
                    floating_files = [f for f in os.listdir(item_path) 
                                    if f.startswith(f"floatingframes_{self.target_video}_temp_") and f.endswith('.pkl')]
                    
                    if handlist_files and floating_files:
                        # ìºì‹œ ë””ë ‰í† ë¦¬ì— ë°ì´í„° ë³µì‚¬
                        os.makedirs(self.cache_data_dir, exist_ok=True)
                        
                        src_handlist = os.path.join(item_path, handlist_files[0])
                        src_floating = os.path.join(item_path, floating_files[0])
                        
                        dst_handlist = os.path.join(self.cache_data_dir, f"handlist_{self.target_video}_limit{self.frame_limit}.pkl")
                        dst_floating = os.path.join(self.cache_data_dir, f"floatingframes_{self.target_video}_limit{self.frame_limit}.pkl")
                        
                        shutil.copy2(src_handlist, dst_handlist)
                        shutil.copy2(src_floating, dst_floating)
                        
                        # ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬
                        os.remove(temp_video_path)
                        shutil.rmtree(item_path)
                        
                        # ì„ì‹œ í‚¤ë³´ë“œ ì •ë³´ ì œê±°
                        keyboard_info_path = os.path.join(self.script_dir, "keyboardcoordinateinfo.pkl")
                        with open(keyboard_info_path, 'rb') as f:
                            keyboard_info = pickle.load(f)
                        
                        temp_video_key = f"{self.target_video}_temp"
                        if temp_video_key in keyboard_info:
                            del keyboard_info[temp_video_key]
                            with open(keyboard_info_path, 'wb') as f:
                                pickle.dump(keyboard_info, f)
                            print(f"âœ… ì„ì‹œ í‚¤ë³´ë“œ ì¢Œí‘œ ì •ë³´ ì œê±°: {temp_video_key}")
                        
                        print(f"âœ… {self.frame_limit}í”„ë ˆì„ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ ë° ìºì‹œ ì €ì¥")
                        
                        return {
                            'video_name': f"{self.target_video}_limit{self.frame_limit}",
                            'data_dir': self.cache_data_dir,
                            'handlist_file': dst_handlist,
                            'floating_file': dst_floating,
                            'original_video': f"{self.target_video}_limit{self.frame_limit}.mp4"
                        }
            
            raise RuntimeError(f"âŒ ì²˜ë¦¬ëœ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
        except Exception as e:
            print(f"âŒ ì œí•œëœ ë¹„ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def load_data(self, data_info: Dict[str, Any]) -> Tuple[List, List, float]:
        """ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤"""
        print(f"ğŸ“ ë°ì´í„° ë¡œë”©: {data_info['video_name']}")
        
        # handlist ë¡œë“œ
        with open(data_info['handlist_file'], 'rb') as f:
            handlist = pickle.load(f)
        
        # ê¸°ì¡´ floating frames ë¡œë“œ
        with open(data_info['floating_file'], 'rb') as f:
            existing_floating_frames = pickle.load(f)
        
        # ë¹„ë””ì˜¤ ì •ë³´ì—ì„œ ratio ê³„ì‚°
        video_path = os.path.join(self.video_capture_dir, data_info['original_video'])
        ratio = 1.0
        
        if os.path.exists(video_path):
            video = cv2.VideoCapture(video_path)
            width = video.get(cv2.CAP_PROP_FRAME_WIDTH)
            height = video.get(cv2.CAP_PROP_FRAME_HEIGHT)
            ratio = height / width
            video.release()
            print(f"   ğŸ“ ë¹„ë””ì˜¤ í•´ìƒë„: {width:.0f}x{height:.0f}, ratio: {ratio:.3f}")
        
        print(f"   ğŸ“Š handlist: {len(handlist)}ê°œ í”„ë ˆì„")
        print(f"   ğŸ” ê¸°ì¡´ floating: {len(existing_floating_frames)}ê°œ")
        
        # Quick test ëª¨ë“œì¼ ë•Œ í”„ë ˆì„ ì œí•œ (ìºì‹œëœ ë°ì´í„°ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ)
        is_cached_data = "limit" in data_info['video_name']
        
        if self.quick_test and not is_cached_data:
            print(f"ğŸš€ Quick Test ëª¨ë“œ: ì´ˆë°˜ {self.frame_limit}í”„ë ˆì„ë§Œ ì²˜ë¦¬")
            
            # handlist ì œí•œ
            limited_handlist = []
            for hands in handlist:
                if hands:  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°
                    limited_hands = [hand for hand in hands if hand.handframe < self.frame_limit]
                    limited_handlist.append(limited_hands)
                else:
                    limited_handlist.append(hands)
            
            # floating frames ì œí•œ
            limited_floating = [
                (frame, handtype, depth, status) 
                for frame, handtype, depth, status in existing_floating_frames 
                if frame < self.frame_limit
            ]
            
            print(f"   âš¡ ì œí•œëœ handlist: {sum(len(hands) for hands in limited_handlist if hands)}ê°œ ì†")
            print(f"   âš¡ ì œí•œëœ floating: {len(limited_floating)}ê°œ")
            
            return limited_handlist, limited_floating, ratio
        elif is_cached_data:
            print(f"âœ… ìºì‹œëœ {self.frame_limit}í”„ë ˆì„ ë°ì´í„° ë¡œë”© ì™„ë£Œ")
            print(f"   ğŸ“Š handlist: {len(handlist)}ê°œ í”„ë ˆì„ (ì´ë¯¸ ì œí•œë¨)")
            print(f"   ğŸ” floating: {len(existing_floating_frames)}ê°œ (ì´ë¯¸ ì œí•œë¨)")
        
        return handlist, existing_floating_frames, ratio
    
    def run_scipy_detection(self, handlist: List, ratio: float) -> Tuple[List, float]:
        """SciPy ì›ë³¸ ë²„ì „ ì‹¤í–‰ (Golden Standard)"""
        monitor.start_step("SciPy ì›ë³¸ ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰", 4)
        
        start_time = time.time()
        
        try:
            import copy
            handlist_copy = copy.deepcopy(handlist)
            
            total_hands = sum(len(hands) for hands in handlist_copy if hands)
            total_frames = len([hands for hands in handlist_copy if hands])
            
            print(f"SciPy ì²˜ë¦¬: {total_hands:,}ê°œ ì†, {total_frames:,}ê°œ í”„ë ˆì„")
            
            # 1ë‹¨ê³„: ëª¨ë¸ ìƒì„±
            monitor.update_step_progress(1, "ì† ê³¨ê²© ëª¨ë¸ ìƒì„± ì¤‘...")
            monitor.log_operation("ì† ê³¨ê²© ëª¨ë¸ êµ¬ì„±", 
                                "ì¢Œ/ìš° ì† ê¸°ì¤€ ëª¨ë¸ ìƒì„± (3ì°¨ì› ê´€ì ˆ êµ¬ì¡° ë¶„ì„)", False)
            
            model_start = time.time()
            monitor.log_memory_usage("ëª¨ë¸ ìƒì„± ì „")
            
            print(f"ğŸ”§ SciPy ëª¨ë¸ ìƒì„± ì‹œì‘:")
            print(f"   ğŸ“Š ì…ë ¥ ë°ì´í„°: {len(handlist_copy)} í”„ë ˆì„")
            print(f"   ğŸ–ï¸  ì¢Œ/ìš° ì† êµ¬ë¶„ ë¶„ì„ ì¤‘...")
            
            lhmodel, rhmodel = scipy_version.modelskeleton(handlist_copy)
            model_time = time.time() - model_start
            
            print(f"âœ… SciPy ëª¨ë¸ ìƒì„± ì™„ë£Œ:")
            print(f"   ğŸ–ï¸  ì¢Œì† ëª¨ë¸: {len(lhmodel) if lhmodel else 0}ê°œ ê´€ì ˆ")
            print(f"   ğŸ–ï¸  ìš°ì† ëª¨ë¸: {len(rhmodel) if rhmodel else 0}ê°œ ê´€ì ˆ")
            print(f"   â±ï¸  ì†Œìš” ì‹œê°„: {model_time:.2f}ì´ˆ")
            
            monitor.log_memory_usage("ëª¨ë¸ ìƒì„± í›„")
            
            # 2ë‹¨ê³„: ê¹Šì´ ê³„ì‚° (ê°€ì¥ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
            monitor.update_step_progress(2, f"SciPy ë°©ì •ì‹ í•´ë²•ìœ¼ë¡œ {total_hands:,}ê°œ ì† ê¹Šì´ ê³„ì‚° ì¤‘...")
            
            print(f"\nğŸ”¬ SciPy ê¹Šì´ ê³„ì‚° ì‹œì‘:")
            print(f"   ğŸ§® í•´ë²• ë°©ì‹: Powell's dog leg (Hybrid Newton-Raphson)")
            print(f"   ğŸ“ ë¹„ì„ í˜• ë°©ì •ì‹ ì‹œìŠ¤í…œ: 3ë³€ìˆ˜ 3ë°©ì •ì‹")
            print(f"   ğŸ¯ í—ˆìš© ì˜¤ì°¨: SciPy ê¸°ë³¸ê°’ (1e-6 ~ 1e-12)")
            print(f"   âš™ï¸  ì²˜ë¦¬ ë°©ì‹: ìˆœì°¨ ì²˜ë¦¬ (CPU ë‹¨ì¼ ìŠ¤ë ˆë“œ)")
            print(f"   ğŸ“Š ëŒ€ìƒ ì†: {total_hands:,}ê°œ")
            
            monitor.log_memory_usage("ê¹Šì´ ê³„ì‚° ì „")
            start_depth = time.time()
            
            # ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ì½œë°± ì„¤ì •
            hands_processed = 0
            last_update = time.time()
            
            def depth_progress_callback():
                nonlocal hands_processed, last_update
                current_time = time.time()
                if current_time - last_update > 5:  # 5ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
                    elapsed = current_time - start_depth
                    rate = hands_processed / elapsed if elapsed > 0 else 0
                    remaining = total_hands - hands_processed
                    eta = remaining / rate if rate > 0 else 0
                    
                    print(f"   â³ ì§„í–‰ ìƒí™©: {hands_processed:,}/{total_hands:,} ({hands_processed/total_hands*100:.1f}%)")
                    print(f"   âš¡ ì²˜ë¦¬ ì†ë„: {rate:.1f}ê°œ/ì´ˆ")
                    print(f"   â° ì˜ˆìƒ ì™„ë£Œ: {monitor._format_time(eta)}")
                    
                    # ê°œë³„ ì† ì²˜ë¦¬ ê³¼ì • ì„¤ëª…
                    print(f"   ğŸ§® í˜„ì¬ ì—°ì‚°: ë¹„ì„ í˜• ë°©ì •ì‹ í•´ë²• (Newton-Raphson)")
                    print(f"   ğŸ“ ê³„ì‚° ì¤‘: 3D ê³µê°„ ì¢Œí‘œ â†’ ê¹Šì´ ê°’ ë³€í™˜")
                    
                    last_update = current_time
            
            print(f"ğŸ”„ ê°œë³„ ì† ì²˜ë¦¬ ì‹œì‘ (ìˆœì°¨ ì²˜ë¦¬):")
            scipy_version.depthlist(handlist_copy, lhmodel, rhmodel, ratio)
            
            depth_time = time.time() - start_depth
            depth_rate = total_hands / depth_time if depth_time > 0 else 0
            
            print(f"âœ… SciPy ê¹Šì´ ê³„ì‚° ì™„ë£Œ:")
            print(f"   â±ï¸  ì´ ì†Œìš” ì‹œê°„: {depth_time:.2f}ì´ˆ")
            print(f"   âš¡ í‰ê·  ì²˜ë¦¬ ì†ë„: {depth_rate:.1f}ê°œ/ì´ˆ")
            print(f"   ğŸ¯ ê³„ì‚° ì •í™•ë„: ê³¼í•™ ì—°ì‚° ìˆ˜ì¤€ (reference)")
            
            monitor.log_memory_usage("ê¹Šì´ ê³„ì‚° í›„")
            
            # 3ë‹¨ê³„: ê²°í•¨ í”„ë ˆì„ ê²€ì¶œ
            monitor.update_step_progress(3, "ê²°í•¨ í”„ë ˆì„ ë¶„ì„ ì¤‘...")
            monitor.log_operation("ê²°í•¨ í”„ë ˆì„ ê²€ì¶œ", 
                                "ì† ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ ë° ì´ìƒ í”„ë ˆì„ ì‹ë³„")
            
            print(f"ğŸ” ê²°í•¨ í”„ë ˆì„ ê²€ì¶œ ì‹œì‘:")
            print(f"   ğŸ“Š ë¶„ì„ ëŒ€ìƒ: {total_frames:,}ê°œ í”„ë ˆì„")
            print(f"   ğŸ” ê²€ì‚¬ í•­ëª©: ì† ë°ì´í„° ë¬´ê²°ì„±, ì¢Œí‘œ ìœ íš¨ì„±")
            
            faulty_start = time.time()
            faultyframe = scipy_version.faultyframes(handlist_copy)
            faulty_time = time.time() - faulty_start
            
            print(f"âœ… ê²°í•¨ í”„ë ˆì„ ê²€ì¶œ ì™„ë£Œ:")
            print(f"   ğŸš« ê²°í•¨ í”„ë ˆì„: {len(faultyframe):,}ê°œ")
            print(f"   âœ… ìœ íš¨ í”„ë ˆì„: {total_frames - len(faultyframe):,}ê°œ")
            print(f"   ğŸ“Š ë°ì´í„° í’ˆì§ˆ: {(1-len(faultyframe)/total_frames)*100:.1f}%")
            print(f"   â±ï¸  ì†Œìš” ì‹œê°„: {faulty_time:.2f}ì´ˆ")
            
            # 4ë‹¨ê³„: floating í”„ë ˆì„ ê²€ì¶œ (ê¸°ì¡´ 0.9 ì„ê³„ê°’ ì‚¬ìš©)
            frame_count = max([max([hand.handframe for hand in hands]) for hands in handlist_copy if hands]) + 1
            valid_frames = frame_count - len(faultyframe)
            
            monitor.update_step_progress(4, f"Floating í”„ë ˆì„ ë¶„ì„ ({frame_count:,}í”„ë ˆì„)...")
            monitor.log_operation("Floating ê²€ì¶œ ì•Œê³ ë¦¬ì¦˜", 
                                f"ê¸°ì¡´ 0.9 ì„ê³„ê°’ ê¸°ë°˜ floating ìƒíƒœ íŒì • ({valid_frames:,}ê°œ ìœ íš¨ í”„ë ˆì„)")
            
            print(f"ğŸ¯ SciPy Floating í”„ë ˆì„ ê²€ì¶œ ì‹œì‘:")
            print(f"   ğŸ“Š ì „ì²´ í”„ë ˆì„: {frame_count:,}ê°œ")
            print(f"   ğŸš« ê²°í•¨ í”„ë ˆì„: {len(faultyframe):,}ê°œ (ì œì™¸)")
            print(f"   âœ… ë¶„ì„ ëŒ€ìƒ: {valid_frames:,}ê°œ í”„ë ˆì„")
            print(f"   ğŸ“ ê³ ì • ì„ê³„ê°’: 0.9 (ê¸°ì¡´ í‘œì¤€ ê¸°ì¤€)")
            print(f"   ğŸ§® íŒì • ë°©ì‹: ê³„ì‚°ëœ ê¹Šì´ ê°’ >= 0.9 â†’ FLOATING")
            
            start_floating = time.time()
            floating_frames = scipy_version.detectfloatingframes(
                handlist_copy, frame_count, faultyframe, lhmodel, rhmodel, ratio
            )
            floating_time = time.time() - start_floating
            floating_rate = valid_frames / floating_time if floating_time > 0 else 0
            
            print(f"âœ… SciPy Floating ê²€ì¶œ ì™„ë£Œ:")
            print(f"   ğŸ¯ Floating ê°ì§€: {len([f for f in floating_frames if f[3] == 'floating']):,}ê°œ")
            print(f"   ğŸ“Š Normal ìƒíƒœ: {len([f for f in floating_frames if f[3] == 'notfloating']):,}ê°œ")
            print(f"   ğŸ“ˆ Floating ë¹„ìœ¨: {len([f for f in floating_frames if f[3] == 'floating'])/len(floating_frames)*100:.1f}%")
            print(f"   â±ï¸  ì†Œìš” ì‹œê°„: {floating_time:.2f}ì´ˆ")
            print(f"   âš¡ ì²˜ë¦¬ ì†ë„: {floating_rate:.1f}í”„ë ˆì„/ì´ˆ")
            
            monitor.log_memory_usage("Floating ê²€ì¶œ í›„")
            
        except Exception as e:
            print(f"âŒ SciPy ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            monitor.finish_step("ì‹¤í–‰ ì‹¤íŒ¨")
            return [], 0.0
        
        scipy_time = time.time() - start_time
        
        monitor.finish_step(f"ì´ {len(floating_frames):,}ê°œ floating ê²€ì¶œ, {scipy_time:.2f}ì´ˆ ì†Œìš”")
        
        print(f"\nğŸ“ˆ SciPy ì „ì²´ ì„±ëŠ¥ ìš”ì•½:")
        print(f"   â±ï¸  ì´ ì†Œìš”ì‹œê°„: {scipy_time:.2f}ì´ˆ")
        print(f"   ğŸ—ï¸  ëª¨ë¸ ìƒì„±: {model_time:.2f}ì´ˆ ({model_time/scipy_time*100:.1f}%)")
        print(f"   ğŸ”¬ ê¹Šì´ ê³„ì‚°: {depth_time:.2f}ì´ˆ ({depth_time/scipy_time*100:.1f}%)")
        print(f"   ğŸ” ê²°í•¨ ê²€ì¶œ: {faulty_time:.2f}ì´ˆ ({faulty_time/scipy_time*100:.1f}%)")
        print(f"   ğŸ¯ Floating ê²€ì¶œ: {floating_time:.2f}ì´ˆ ({floating_time/scipy_time*100:.1f}%)")
        print(f"   ğŸ“Š ì „ì²´ ì²˜ë¦¬ëŸ‰: {total_hands/scipy_time:.1f}ê°œ/ì´ˆ")
        
        return floating_frames, scipy_time
    
    def run_pytorch_detection(self, handlist: List, ratio: float) -> Tuple[List, float]:
        """PyTorch ìˆœìˆ˜ ë²„ì „ ì‹¤í–‰ (ìƒˆ êµ¬í˜„) - ìƒì„¸ ë¡œê¹… í¬í•¨"""
        monitor.start_step("PyTorch GPU ê°€ì† ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰", 4)
        
        start_time = time.time()
        
        try:
            # handlist ë³µì‚¬
            import copy
            handlist_copy = copy.deepcopy(handlist)
            
            # ì´ ì† ìˆ˜ ê³„ì‚°
            total_hands = sum(len(hands) for hands in handlist_copy if hands)
            total_frames = len([hands for hands in handlist_copy if hands])
            
            print(f"ğŸš€ PyTorch ì²˜ë¦¬ ëŒ€ìƒ ë¶„ì„:")
            print(f"   ğŸ‘‹ ì´ ì† ë°ì´í„°: {total_hands:,}ê°œ")
            print(f"   ğŸ–¼ï¸  ìœ íš¨ í”„ë ˆì„: {total_frames:,}ê°œ")
            print(f"   ğŸ“ ë¹„ë””ì˜¤ ë¹„ìœ¨: {ratio:.3f}")
            
            # GPU ì •ë³´ ì¶œë ¥
            if TORCH_AVAILABLE and torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                gpu_props = torch.cuda.get_device_properties(0)
                print(f"ğŸ® GPU ê°€ì† í™˜ê²½:")
                print(f"   ğŸ”§ GPU: {gpu_name}")
                print(f"   ğŸ’¾ VRAM: {gpu_memory:.1f}GB")
                print(f"   âš¡ Compute: {gpu_props.major}.{gpu_props.minor}")
                print(f"   ğŸ”¥ CUDA: {torch.version.cuda}")
            else:
                print(f"âš ï¸  CPU ëª¨ë“œë¡œ ì‹¤í–‰ (GPU ê°€ì† ë¶ˆê°€)")
            
            # ì•Œê³ ë¦¬ì¦˜ ì •ë³´ ë¡œê¹…
            monitor.log_algorithm_start(
                "PyTorch GPU ê°€ì†",
                "ê³ ì •ì  ë°˜ë³µë²• (Fixed-point Iteration)",
                "32-bit ë‹¨ì •ë°€ë„ (ì„±ëŠ¥ ìµœì í™”)",
                "ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬ (GPU vectorized)",
                [
                    "GPU í…ì„œ ì—°ì‚° ìµœì í™”",
                    "ë°°ì¹˜ ì²˜ë¦¬ (2048-8192ê°œ ë™ì‹œ)",
                    "ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë¦¬ë°",
                    "ë²¡í„°í™” ì—°ì‚° (CUDA ì»¤ë„)",
                    "ìë™ ë¯¸ë¶„ ë¹„í™œì„±í™”"
                ]
            )
            
            # 1ë‹¨ê³„: ëª¨ë¸ ìƒì„±
            monitor.update_step_progress(1, "ì† ê³¨ê²© ëª¨ë¸ ìƒì„± ì¤‘...")
            monitor.log_operation("ì† ê³¨ê²© ëª¨ë¸ êµ¬ì„±", 
                                "ì¢Œ/ìš° ì† ê¸°ì¤€ ëª¨ë¸ ìƒì„± (PyTorch í…ì„œ í˜•íƒœ)", False)
            
            model_start = time.time()
            monitor.log_memory_usage("ëª¨ë¸ ìƒì„± ì „")
            
            print(f"ğŸ”§ PyTorch ëª¨ë¸ ìƒì„± ì‹œì‘:")
            print(f"   ğŸ“Š ì…ë ¥ ë°ì´í„°: {len(handlist_copy)} í”„ë ˆì„")
            print(f"   ğŸ–ï¸  ì¢Œ/ìš° ì† êµ¬ë¶„ ë¶„ì„ ì¤‘...")
            
            lhmodel, rhmodel = pytorch_version.modelskeleton(handlist_copy)
            model_time = time.time() - model_start
            
            print(f"âœ… PyTorch ëª¨ë¸ ìƒì„± ì™„ë£Œ:")
            print(f"   ğŸ–ï¸  ì¢Œì† ëª¨ë¸: {len(lhmodel) if lhmodel else 0}ê°œ ê´€ì ˆ")
            print(f"   ğŸ–ï¸  ìš°ì† ëª¨ë¸: {len(rhmodel) if rhmodel else 0}ê°œ ê´€ì ˆ")
            print(f"   â±ï¸  ì†Œìš” ì‹œê°„: {model_time:.2f}ì´ˆ")
            
            monitor.log_memory_usage("ëª¨ë¸ ìƒì„± í›„")
            
            # 2ë‹¨ê³„: ê¹Šì´ ê³„ì‚° (ê°€ì¥ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼) - ìƒì„¸ ë¡œê¹… ì¶”ê°€
            monitor.update_step_progress(2, f"PyTorch ë²¡í„°í™” ì—°ì‚°ìœ¼ë¡œ {total_hands:,}ê°œ ì† ê¹Šì´ ê³„ì‚° ì¤‘...")
            
            print(f"\nğŸš€ PyTorch ê¹Šì´ ê³„ì‚° ì‹œì‘:")
            print(f"   ğŸ§® í•´ë²• ë°©ì‹: ê³ ì •ì  ë°˜ë³µë²• (Fixed-point Iteration)")
            print(f"   ğŸ“ ë¹„ì„ í˜• ë°©ì •ì‹ ì‹œìŠ¤í…œ: 3ë³€ìˆ˜ 3ë°©ì •ì‹ (ë²¡í„°í™”)")
            print(f"   ğŸ¯ í—ˆìš© ì˜¤ì°¨: 1e-6 (ì„±ëŠ¥ ìµœì í™”)")
            print(f"   âš™ï¸  ì²˜ë¦¬ ë°©ì‹: GPU ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬")
            print(f"   ğŸ“Š ëŒ€ìƒ ì†: {total_hands:,}ê°œ")
            
            # ë°°ì¹˜ í¬ê¸° ê²°ì • ë¡œì§
            if TORCH_AVAILABLE and torch.cuda.is_available():
                available_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                batch_size = min(8192, max(1024, int(available_memory * 1000)))
                print(f"   ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {batch_size:,}ê°œ (GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ë™ì  ì¡°ì •)")
                print(f"   ğŸ’¾ ì‚¬ìš© ê°€ëŠ¥ VRAM: {available_memory:.1f}GB")
            else:
                batch_size = 1024
                print(f"   ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {batch_size:,}ê°œ (CPU ëª¨ë“œ)")
            
            monitor.log_memory_usage("ê¹Šì´ ê³„ì‚° ì „")
            start_depth = time.time()
            
            # GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ê°•í™”
            if TORCH_AVAILABLE and torch.cuda.is_available():
                initial_memory = torch.cuda.memory_allocated() / 1024**3
                print(f"   ğŸ’¾ ì´ˆê¸° GPU ë©”ëª¨ë¦¬: {initial_memory:.2f}GB")
                
                # ì‹¤ì œ GPU ì—°ì‚° ì‹œì‘ í™•ì¸
                test_tensor = torch.randn(1000, 1000, device='cuda')
                torch.cuda.synchronize()  # GPU ì—°ì‚° ì™„ë£Œ ëŒ€ê¸°
                print(f"   ğŸ”¥ GPU ì—°ì‚° ì¤€ë¹„ ì™„ë£Œ")
            
            # ë°°ì¹˜ ì²˜ë¦¬ ìƒíƒœ ëª¨ë‹ˆí„°ë§
            total_batches = (total_hands + batch_size - 1) // batch_size
            print(f"   ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ê³„íš: {total_batches:,}ê°œ ë°°ì¹˜")
            
            # ì‹¤ì œ ê¹Šì´ ê³„ì‚° ì‹œì‘ - ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ì¶”ê°€
            print(f"ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘:")
            
            # ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ì½œë°± í•¨ìˆ˜ ì •ì˜
            def create_progress_callback():
                from tqdm import tqdm
                pbar = tqdm(total=total_hands, desc="PyTorch ê¹Šì´ ê³„ì‚°", ncols=80, leave=True)
                processed_count = 0
                
                def update_progress(batch_processed):
                    nonlocal processed_count
                    processed_count += batch_processed
                    pbar.update(batch_processed)
                    
                    # GPU ìƒíƒœ ì—…ë°ì´íŠ¸
                    if TORCH_AVAILABLE and torch.cuda.is_available():
                        try:
                            current_memory = torch.cuda.memory_allocated() / 1024**3
                            pbar.set_postfix({
                                'ì§„í–‰ë¥ ': f'{processed_count}/{total_hands}',
                                'GPU_ë©”ëª¨ë¦¬': f'{current_memory:.1f}GB'
                            })
                        except:
                            pass
                
                def close_progress():
                    pbar.close()
                    
                return update_progress, close_progress
            
            # ì§„í–‰ ìƒí™© ì½œë°± ìƒì„±
            progress_callback, close_callback = create_progress_callback()
            
            # PyTorch ëª¨ë“ˆì— ì§„í–‰ ìƒí™© ì½œë°± ì „ë‹¬ (ë§Œì•½ ì§€ì›í•œë‹¤ë©´)
            # í˜„ì¬ëŠ” ê¸°ë³¸ depthlist í˜¸ì¶œ
            try:
                # PyTorch ê¹Šì´ ê³„ì‚° ì‹œì‘
                print(f"âš¡ GPU ê°€ì† ë²¡í„° ì—°ì‚° ì‹œì‘...")
                
                # ì‹¤ì œ ì²˜ë¦¬ ì‹œì‘ ì „ GPU í™œì„±í™”
                if TORCH_AVAILABLE and torch.cuda.is_available():
                    # GPU ì›Œë°ì—…
                    dummy_tensor = torch.randn(batch_size, 10, device='cuda')
                    torch.cuda.synchronize()
                    print(f"   ğŸ”¥ GPU ì›Œë°ì—… ì™„ë£Œ")
                
                # ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë‹ˆí„°ë§
                hands_processed = 0
                last_log_time = time.time()
                
                # ì‹¤ì œ depthlist í˜¸ì¶œ
                pytorch_version.depthlist(handlist_copy, lhmodel, rhmodel, ratio)
                
                # ì™„ë£Œ í›„ ì§„í–‰ë°” ì—…ë°ì´íŠ¸
                progress_callback(total_hands)
                
            except Exception as e:
                print(f"âŒ ê¹Šì´ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
                raise
            finally:
                close_callback()
            
            depth_time = time.time() - start_depth
            depth_rate = total_hands / depth_time if depth_time > 0 else 0
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            if TORCH_AVAILABLE and torch.cuda.is_available():
                peak_memory = torch.cuda.max_memory_allocated() / 1024**3
                final_memory = torch.cuda.memory_allocated() / 1024**3
                print(f"   ğŸ’¾ ìµœëŒ€ GPU ë©”ëª¨ë¦¬: {peak_memory:.2f}GB")
                print(f"   ğŸ’¾ ìµœì¢… GPU ë©”ëª¨ë¦¬: {final_memory:.2f}GB")
                print(f"   ğŸ“Š ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: {(final_memory/peak_memory)*100:.1f}%")
            
            print(f"âœ… PyTorch ê¹Šì´ ê³„ì‚° ì™„ë£Œ:")
            print(f"   â±ï¸  ì´ ì†Œìš” ì‹œê°„: {depth_time:.2f}ì´ˆ")
            print(f"   âš¡ í‰ê·  ì²˜ë¦¬ ì†ë„: {depth_rate:.1f}ê°œ/ì´ˆ")
            print(f"   ğŸ¯ ê³„ì‚° ì •í™•ë„: float64 ì •ë°€ë„ (ê³¼í•™ì  ë“±ê¸‰)")
            print(f"   ğŸ“ˆ ì •í™•ì„± ìš°ì„ : Newton-Raphson ë°©ë²• ì‚¬ìš©")
            
            monitor.log_memory_usage("ê¹Šì´ ê³„ì‚° í›„")
            
            # 3ë‹¨ê³„: ê²°í•¨ í”„ë ˆì„ ê²€ì¶œ
            monitor.update_step_progress(3, "ê²°í•¨ í”„ë ˆì„ ë¶„ì„ ì¤‘...")
            monitor.log_operation("ê²°í•¨ í”„ë ˆì„ ê²€ì¶œ", 
                                "ì† ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ ë° ì´ìƒ í”„ë ˆì„ ì‹ë³„")
            
            print(f"ğŸ” ê²°í•¨ í”„ë ˆì„ ê²€ì¶œ ì‹œì‘:")
            print(f"   ğŸ“Š ë¶„ì„ ëŒ€ìƒ: {total_frames:,}ê°œ í”„ë ˆì„")
            print(f"   ğŸ” ê²€ì‚¬ í•­ëª©: ì† ë°ì´í„° ë¬´ê²°ì„±, ì¢Œí‘œ ìœ íš¨ì„±")
            
            faulty_start = time.time()
            faultyframe = pytorch_version.faultyframes(handlist_copy)
            faulty_time = time.time() - faulty_start
            
            print(f"âœ… ê²°í•¨ í”„ë ˆì„ ê²€ì¶œ ì™„ë£Œ:")
            print(f"   ğŸš« ê²°í•¨ í”„ë ˆì„: {len(faultyframe):,}ê°œ")
            print(f"   âœ… ìœ íš¨ í”„ë ˆì„: {total_frames - len(faultyframe):,}ê°œ")
            print(f"   ğŸ“Š ë°ì´í„° í’ˆì§ˆ: {(1-len(faultyframe)/total_frames)*100:.1f}%")
            print(f"   â±ï¸  ì†Œìš” ì‹œê°„: {faulty_time:.2f}ì´ˆ")
            
            # 4ë‹¨ê³„: floating í”„ë ˆì„ ê²€ì¶œ - ìƒì„¸ ë¡œê¹… ì¶”ê°€
            frame_count = max([max([hand.handframe for hand in hands]) for hands in handlist_copy if hands]) + 1
            valid_frames = frame_count - len(faultyframe)
            
            monitor.update_step_progress(4, f"Floating í”„ë ˆì„ ë¶„ì„ ({frame_count:,}í”„ë ˆì„)...")
            monitor.log_operation("GPU ê°€ì† Floating ê²€ì¶œ", 
                                f"ë°°ì¹˜ ì²˜ë¦¬ ê¸°ë°˜ floating ìƒíƒœ íŒì • ({valid_frames:,}ê°œ ìœ íš¨ í”„ë ˆì„)")
            
            # ğŸ¯ ê³ ì • ì„ê³„ê°’ 0.9 ì‚¬ìš© (SciPyì™€ ë™ì¼)
            fixed_threshold = 0.9
            
            print(f"ğŸ¯ PyTorch Floating í”„ë ˆì„ ê²€ì¶œ ì‹œì‘:")
            print(f"   ğŸ“Š ì „ì²´ í”„ë ˆì„: {frame_count:,}ê°œ")
            print(f"   ğŸš« ê²°í•¨ í”„ë ˆì„: {len(faultyframe):,}ê°œ (ì œì™¸)")
            print(f"   âœ… ë¶„ì„ ëŒ€ìƒ: {valid_frames:,}ê°œ í”„ë ˆì„")
            print(f"   ğŸ”ï¸  ê³ ì • ì„ê³„ê°’: {fixed_threshold:.3f} (SciPyì™€ ë™ì¼)")
            print(f"   ğŸ§® íŒì • ë°©ì‹: GPU í…ì„œ ì—°ì‚° (ê³„ì‚°ëœ ê¹Šì´ ê°’ >= ì„ê³„ê°’ â†’ FLOATING)")
            print(f"   ğŸš€ ê°€ì† ë°©ì‹: ë°°ì¹˜ ì²˜ë¦¬ë¡œ ê³ ì† ë²¡í„° ì—°ì‚°")
            
            start_floating = time.time()
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
            if TORCH_AVAILABLE and torch.cuda.is_available():
                floating_start_memory = torch.cuda.memory_allocated() / 1024**3
                print(f"   ğŸ’¾ Floating ê²€ì¶œ ì‹œì‘ ë©”ëª¨ë¦¬: {floating_start_memory:.2f}GB")
            
            # Floating ê²€ì¶œ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
            print(f"ğŸ”„ Floating ê²€ì¶œ ì§„í–‰ ì¤‘...")
            
            # Floating ê²€ì¶œ tqdm ì§„í–‰ë°”
            from tqdm import tqdm
            with tqdm(total=valid_frames, desc="Floating ê²€ì¶œ", ncols=80, leave=True) as pbar:
                # ğŸ¯ ê³ ì • ì„ê³„ê°’ 0.9ë¥¼ ì‚¬ìš©í•œ floating ê²€ì¶œ
                floating_frames = pytorch_version.detectfloatingframes(
                    handlist_copy, frame_count, faultyframe, lhmodel, rhmodel, ratio, fixed_threshold
                )
                pbar.update(valid_frames)  # ì™„ë£Œ ì‹œ ì§„í–‰ë°” ì—…ë°ì´íŠ¸
            
            floating_time = time.time() - start_floating
            floating_rate = valid_frames / floating_time if floating_time > 0 else 0
            
            print(f"âœ… PyTorch Floating ê²€ì¶œ ì™„ë£Œ:")
            print(f"   ğŸ¯ Floating ê°ì§€: {len([f for f in floating_frames if f[3] == 'floating']):,}ê°œ")
            print(f"   ğŸ“Š Normal ìƒíƒœ: {len([f for f in floating_frames if f[3] == 'notfloating']):,}ê°œ")
            print(f"   ğŸ“ˆ Floating ë¹„ìœ¨: {len([f for f in floating_frames if f[3] == 'floating'])/len(floating_frames)*100:.1f}%")
            print(f"   â±ï¸  ì†Œìš” ì‹œê°„: {floating_time:.2f}ì´ˆ")
            print(f"   âš¡ ì²˜ë¦¬ ì†ë„: {floating_rate:.1f}í”„ë ˆì„/ì´ˆ")
            
            monitor.log_memory_usage("Floating ê²€ì¶œ í›„")
            
        except Exception as e:
            print(f"âŒ PyTorch ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            if TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()  # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            monitor.finish_step("ì‹¤í–‰ ì‹¤íŒ¨")
            return [], 0.0
        
        pytorch_time = time.time() - start_time
        
        monitor.finish_step(f"ì´ {len(floating_frames):,}ê°œ floating ê²€ì¶œ, {pytorch_time:.2f}ì´ˆ ì†Œìš”")
        
        print(f"\nğŸ“ˆ PyTorch ì „ì²´ ì„±ëŠ¥ ìš”ì•½:")
        print(f"   â±ï¸  ì´ ì†Œìš”ì‹œê°„: {pytorch_time:.2f}ì´ˆ")
        print(f"   ğŸ—ï¸  ëª¨ë¸ ìƒì„±: {model_time:.2f}ì´ˆ ({model_time/pytorch_time*100:.1f}%)")
        print(f"   ğŸš€ ê¹Šì´ ê³„ì‚°: {depth_time:.2f}ì´ˆ ({depth_time/pytorch_time*100:.1f}%)")
        print(f"   ğŸ” ê²°í•¨ ê²€ì¶œ: {faulty_time:.2f}ì´ˆ ({faulty_time/pytorch_time*100:.1f}%)")
        print(f"   ğŸ¯ Floating ê²€ì¶œ: {floating_time:.2f}ì´ˆ ({floating_time/pytorch_time*100:.1f}%)")
        print(f"   ğŸ“Š ì „ì²´ ì²˜ë¦¬ëŸ‰: {total_hands/pytorch_time:.1f}ê°œ/ì´ˆ")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if TORCH_AVAILABLE and torch.cuda.is_available():
            torch.cuda.empty_cache()
            final_cleanup_memory = torch.cuda.memory_allocated() / 1024**3
            print(f"   ğŸ’¾ ë©”ëª¨ë¦¬ ì •ë¦¬ í›„: {final_cleanup_memory:.2f}GB")
        
        return floating_frames, pytorch_time
    
    def analyze_results(self, scipy_results: List, pytorch_results: List, 
                       existing_results: List, scipy_time: float, pytorch_time: float) -> Dict[str, Any]:
        """ê²°ê³¼ ë¶„ì„ - í˜„ì‹¤ì ì¸ ì •ë°€ë„ ê¸°ì¤€ ì ìš©"""
        print("ğŸ“Š ê²°ê³¼ ë¶„ì„ ì¤‘...")
        
        # í˜„ì‹¤ì ì¸ ì •ë°€ë„ ê¸°ì¤€ ì„¤ì •
        MEANINGFUL_DEPTH_THRESHOLD = 1e-3  # 1mm ì´í•˜ëŠ” ë¬´ì˜ë¯¸í•œ ì°¨ì´ë¡œ ê°„ì£¼
        CLASSIFICATION_BOUNDARY = 0.9  # floating/normal ê²½ê³„ê°’
        
        # ê²°ê³¼ë¥¼ frame, handtypeë³„ë¡œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        def results_to_dict(results):
            return {(r[0], r[1]): {'depth': r[2], 'status': r[3]} for r in results}
        
        scipy_dict = results_to_dict(scipy_results)
        pytorch_dict = results_to_dict(pytorch_results)
        existing_dict = results_to_dict(existing_results)
        
        # ê³µí†µ í‚¤ì™€ ë¹„êµ
        all_keys = set(scipy_dict.keys()) | set(pytorch_dict.keys())
        
        agreements = 0
        depth_differences = []
        meaningful_differences = []
        status_mismatches = []
        near_boundary_cases = []
        
        for key in all_keys:
            scipy_data = scipy_dict.get(key)
            pytorch_data = pytorch_dict.get(key)
            
            if scipy_data and pytorch_data:
                # ê¹Šì´ ì°¨ì´ ê³„ì‚°
                depth_diff = abs(scipy_data['depth'] - pytorch_data['depth'])
                depth_differences.append(depth_diff)
                
                # ì˜ë¯¸ìˆëŠ” ì°¨ì´ì¸ì§€ íŒë‹¨
                if depth_diff > MEANINGFUL_DEPTH_THRESHOLD:
                    meaningful_differences.append(depth_diff)
                
                # ê²½ê³„ê°’ ê·¼ì²˜ ì¼€ì´ìŠ¤ ì²´í¬
                scipy_near_boundary = abs(scipy_data['depth'] - CLASSIFICATION_BOUNDARY) < MEANINGFUL_DEPTH_THRESHOLD
                pytorch_near_boundary = abs(pytorch_data['depth'] - CLASSIFICATION_BOUNDARY) < MEANINGFUL_DEPTH_THRESHOLD
                
                if scipy_near_boundary or pytorch_near_boundary:
                    near_boundary_cases.append({
                        'frame': key[0],
                        'handtype': key[1],
                        'scipy_depth': scipy_data['depth'],
                        'pytorch_depth': pytorch_data['depth'],
                        'depth_diff': depth_diff
                    })
                
                # ìƒíƒœ ë¹„êµ - ê²½ê³„ê°’ ê·¼ì²˜ëŠ” ê´€ëŒ€í•˜ê²Œ í‰ê°€
                if scipy_data['status'] == pytorch_data['status']:
                    agreements += 1
                elif scipy_near_boundary and pytorch_near_boundary and depth_diff <= MEANINGFUL_DEPTH_THRESHOLD:
                    # ë‘˜ ë‹¤ ê²½ê³„ê°’ ê·¼ì²˜ì´ê³  ì°¨ì´ê°€ ë¯¸ë¯¸í•˜ë©´ ì¼ì¹˜ë¡œ ê°„ì£¼
                    agreements += 1
                else:
                    status_mismatches.append({
                        'frame': key[0],
                        'handtype': key[1],
                        'scipy_status': scipy_data['status'],
                        'pytorch_status': pytorch_data['status'],
                        'scipy_depth': scipy_data['depth'],
                        'pytorch_depth': pytorch_data['depth'],
                        'depth_diff': depth_diff,
                        'near_boundary': scipy_near_boundary or pytorch_near_boundary
                    })
        
        agreement_rate = (agreements / len(all_keys) * 100) if all_keys else 0
        avg_depth_diff = np.mean(depth_differences) if depth_differences else 0
        max_depth_diff = np.max(depth_differences) if depth_differences else 0
        meaningful_diff_count = len(meaningful_differences)
        meaningful_diff_rate = (meaningful_diff_count / len(all_keys) * 100) if all_keys else 0
        speedup = scipy_time / pytorch_time if pytorch_time > 0 else 0
        
        return {
            'scipy_time': scipy_time,
            'pytorch_time': pytorch_time,
            'speedup': speedup,
            'agreement_rate': agreement_rate,
            'scipy_floating_count': len(scipy_results),
            'pytorch_floating_count': len(pytorch_results),
            'existing_floating_count': len(existing_results),
            'avg_depth_difference': avg_depth_diff,
            'max_depth_difference': max_depth_diff,
            'meaningful_diff_count': meaningful_diff_count,
            'meaningful_diff_rate': meaningful_diff_rate,
            'near_boundary_cases': len(near_boundary_cases),
            'status_mismatches': status_mismatches[:10],  # ìµœëŒ€ 10ê°œë§Œ
            'total_comparisons': len(all_keys),
            'precision_threshold': MEANINGFUL_DEPTH_THRESHOLD
        }
    
    def create_comparison_video(self, data_info: Dict[str, Any], 
                              scipy_results: List, pytorch_results: List, 
                              existing_results: List, handlist: List) -> str:
        """ë¹„êµ ê²°ê³¼ë¥¼ ì‹œê°í™”í•œ ë¹„ë””ì˜¤ ìƒì„± - Hand landmarksì™€ floating ìƒíƒœë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œ"""
        print("ğŸ¥ ìƒì„¸ ë¹„êµ ë¹„ë””ì˜¤ ìƒì„± ì¤‘...")
        
        # ìºì‹œëœ ë°ì´í„°ì¸ ê²½ìš° ìºì‹œ ë””ë ‰í† ë¦¬ì—ì„œ ë¹„ë””ì˜¤ ì°¾ê¸°
        if "limit" in data_info['video_name']:
            video_path = self.limited_video_path
        else:
            video_path = os.path.join(self.video_capture_dir, data_info['original_video'])
        
        if not os.path.exists(video_path):
            print(f"âŒ ì›ë³¸ ë¹„ë””ì˜¤ ì—†ìŒ: {video_path}")
            return ""
        
        # Landmark ìºì‹œ í™•ì¸
        cached_landmarks = None
        if self.check_landmark_cache(video_path):
            cached_landmarks = self.load_landmark_cache()
        
        # ê²°ê³¼ë¥¼ í”„ë ˆì„ë³„ë¡œ ì •ë¦¬
        def organize_by_frame(results):
            frame_dict = {}
            for frame, handtype, depth, status in results:
                if frame not in frame_dict:
                    frame_dict[frame] = {}
                frame_dict[frame][handtype] = {'depth': depth, 'status': status}
            return frame_dict
        
        scipy_frames = organize_by_frame(scipy_results)
        pytorch_frames = organize_by_frame(pytorch_results)
        existing_frames = organize_by_frame(existing_results)
        
        # handlistë¥¼ í”„ë ˆì„ë³„ë¡œ ì •ë¦¬ (ìºì‹œ ì‚¬ìš© ë˜ëŠ” ê¸°ì¡´ ë°ì´í„° ì¬ì‚¬ìš©)
        if cached_landmarks:
            handlist_by_frame = cached_landmarks['handlist_by_frame']
            print(f"ğŸš€ ìºì‹œëœ landmark ë°ì´í„° ì‚¬ìš©: {len(handlist_by_frame):,}ê°œ í”„ë ˆì„")
        else:
            # ê¸°ì¡´ handlistì—ì„œ í”„ë ˆì„ë³„ë¡œ ì •ë¦¬ (ì´ë¯¸ ì²˜ë¦¬ëœ ë°ì´í„° ì¬ì‚¬ìš©)
            handlist_by_frame = {}
            for hands in handlist:
                if hands:  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°
                    for hand in hands:
                        frame_num = hand.handframe
                        if frame_num not in handlist_by_frame:
                            handlist_by_frame[frame_num] = []
                        handlist_by_frame[frame_num].append(hand)
            
            print(f"âœ… ê¸°ì¡´ ì²˜ë¦¬ëœ landmark ë°ì´í„° ì¬ì‚¬ìš©: {len(handlist_by_frame):,}ê°œ í”„ë ˆì„")
            print(f"âš¡ MediaPipe ì¬ê³„ì‚° ìƒëµìœ¼ë¡œ ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•")
        
        # MediaPipe ì‹¤ì‹œê°„ ì¶”ì¶œ ìƒëµ (ì´ë¯¸ ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©)
        # handlistì— ì´ë¯¸ ëª¨ë“  landmark ë°ì´í„°ê°€ ìˆìœ¼ë¯€ë¡œ MediaPipe ì¬ê³„ì‚° ë¶ˆí•„ìš”
        detector = None
        print(f"âš¡ MediaPipe ì¬ê³„ì‚° ìƒëµ: ê¸°ì¡´ ì²˜ë¦¬ëœ ì† ëœë“œë§ˆí¬ ë°ì´í„° ì‚¬ìš©")
        
        # ì„ê³„ê°’ ë¯¸ë¦¬ ê³„ì‚°
        scipy_threshold = 0.9  # SciPyëŠ” í•­ìƒ 0.9 ê³ ì •
        pytorch_threshold = 0.9  # PyTorchë„ 0.9 ê³ ì •
        
        print(f"ğŸ¯ ë¹„ë””ì˜¤ ì‹œê°í™” ì„ê³„ê°’:")
        print(f"   SciPy: {scipy_threshold:.3f} (ê³ ì •)")
        print(f"   PyTorch: {pytorch_threshold:.3f} (ê³ ì •)")
        
        # ë¹„ë””ì˜¤ ì½ê¸° ë° ì“°ê¸° ì¤€ë¹„
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # Quick test ëª¨ë“œì¼ ë•Œ í”„ë ˆì„ ìˆ˜ ì œí•œ
        if self.quick_test:
            total_frames = min(total_frames, self.frame_limit)
        
        output_path = f"comparison_{self.target_video}_detailed_vscode_compatible.mp4"
        
        # VSCode ìµœì  í˜¸í™˜ì„±ì„ ìœ„í•œ ì½”ë± ì„¤ì • (ìš°ì„ ìˆœìœ„ ìˆœì„œ)
        codecs_to_try = [
            ('avc1', 'H.264 AVC1 (ìµœê³  í˜¸í™˜ì„±)'),
            ('H264', 'H.264 (í‘œì¤€)'),
            ('mp4v', 'MPEG-4 Part 2 (ì•ˆì „í•œ ê¸°ë³¸ê°’)'),
            ('XVID', 'XVID (ì••ì¶• íš¨ìœ¨ì„±)'),
            ('MJPG', 'Motion JPEG (ìµœì¢… fallback)')
        ]
        
        fourcc = None
        selected_codec = None
        
        for codec, description in codecs_to_try:
            try:
                test_fourcc = cv2.VideoWriter_fourcc(*codec)
                # ë” ì—„ê²©í•œ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
                test_path = f'test_compatibility_{codec}.mp4'
                test_writer = cv2.VideoWriter(test_path, test_fourcc, fps, (width, height))
                
                if test_writer.isOpened():
                    # ì‹¤ì œ í”„ë ˆì„ ì“°ê¸° í…ŒìŠ¤íŠ¸
                    test_frame = np.zeros((height, width, 3), dtype=np.uint8)
                    test_writer.write(test_frame)
                    test_writer.release()
                    
                    # íŒŒì¼ì´ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    if os.path.exists(test_path) and os.path.getsize(test_path) > 0:
                        # íŒŒì¼ ì½ê¸° í…ŒìŠ¤íŠ¸
                        test_cap = cv2.VideoCapture(test_path)
                        if test_cap.isOpened():
                            ret, _ = test_cap.read()
                            test_cap.release()
                            if ret:
                                fourcc = test_fourcc
                                selected_codec = codec
                                print(f"   âœ… ë¹„ë””ì˜¤ ì½”ë±: {codec} ({description}) - í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ í†µê³¼")
                                os.remove(test_path)
                                break
                        test_cap.release()
                    
                    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬
                    if os.path.exists(test_path):
                        os.remove(test_path)
                else:
                    test_writer.release()
                    
            except Exception as e:
                print(f"   âš ï¸  ì½”ë± {codec} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                continue
        
        if fourcc is None:
            # ìµœì¢… fallback
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            selected_codec = 'mp4v'
            print(f"   âš ï¸  ëª¨ë“  ì½”ë± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨, ê¸°ë³¸ ì½”ë± ì‚¬ìš©: {selected_codec}")
        
        print(f"   ğŸ¬ ì„ íƒëœ ì½”ë±: {selected_codec} - VSCode í˜¸í™˜ì„± ìµœì í™”")
        
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        frame_num = 0
        # ê¸°ì¡´ ì²˜ë¦¬ëœ handlist ë°ì´í„° ì¬ì‚¬ìš©ìœ¼ë¡œ ìƒˆë¡œìš´ landmark ì¶”ì¶œ ë¶ˆí•„ìš”
        
        # tqdmìœ¼ë¡œ ë¹„ë””ì˜¤ ìƒì„± ì§„í–‰ìƒí™© í‘œì‹œ
        from tqdm import tqdm
        with tqdm(total=total_frames, desc="ìƒì„¸ ë¹„ë””ì˜¤ ìƒì„±", ncols=80, leave=True) as pbar:
            while cap.isOpened() and frame_num < total_frames:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # í˜„ì¬ í”„ë ˆì„ì˜ floating ìƒíƒœ í™•ì¸
                scipy_frame_data = scipy_frames.get(frame_num, {})
                pytorch_frame_data = pytorch_frames.get(frame_num, {})
                existing_frame_data = existing_frames.get(frame_num, {})
                
                # ì „ì²´ í”„ë ˆì„ floating ìƒíƒœ
                scipy_floating = any(data.get('status') == 'floating' 
                                   for data in scipy_frame_data.values())
                pytorch_floating = any(data.get('status') == 'floating' 
                                     for data in pytorch_frame_data.values())
                existing_floating = any(data.get('status') == 'floating' 
                                      for data in existing_frame_data.values())
                
                # í˜„ì¬ í”„ë ˆì„ì˜ ì† ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©)
                current_hands = handlist_by_frame.get(frame_num, [])
                
                # MediaPipe ì‹¤ì‹œê°„ ì¶”ì¶œ ìƒëµ - ì´ë¯¸ ì²˜ë¦¬ëœ handlist ë°ì´í„° ì‚¬ìš©
                # current_handsì— ì´ë¯¸ í•´ë‹¹ í”„ë ˆì„ì˜ ì† ë°ì´í„°ê°€ ìˆìŒ
                
                # Hand landmarks ê·¸ë¦¬ê¸°
                if current_hands:
                    # ì† ë°ì´í„°ë¥¼ MediaPipe í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    hand_landmarks = []
                    handedness = []
                    
                    for hand in current_hands:
                        # handlandmarkë¥¼ MediaPipe NormalizedLandmarkListë¡œ ë³€í™˜
                        landmarks = hand.handlandmark
                        
                        # ì„ì‹œ handedness ê°ì²´ ìƒì„±
                        class TempHandedness:
                            def __init__(self, category_name):
                                self.classification = [TempClassification(category_name)]
                        
                        class TempClassification:
                            def __init__(self, category_name):
                                self.category_name = category_name
                        
                        hand_landmarks.append(landmarks)
                        handedness.append(TempHandedness(hand.handtype))
                    
                    # SciPy ê²°ê³¼ë¡œ floating ìƒíƒœ í‘œì‹œ
                    scipy_floating_status = {}
                    for handtype, data in scipy_frame_data.items():
                        scipy_floating_status[handtype] = data.get('status') == 'floating'
                    
                    # PyTorch ê²°ê³¼ë¡œ floating ìƒíƒœ í‘œì‹œ
                    pytorch_floating_status = {}
                    for handtype, data in pytorch_frame_data.items():
                        pytorch_floating_status[handtype] = data.get('status') == 'floating'
                    
                    # ë‘ ê°œì˜ ì´ë¯¸ì§€ ìƒì„± (ì¢Œ: SciPy, ìš°: PyTorch) - ë™ì¼í•œ ì„ê³„ê°’ 0.9 ì‚¬ìš©
                    scipy_image = draw_enhanced_hand_landmarks(
                        frame, hand_landmarks, handedness, scipy_floating_status, scipy_frame_data, " (SciPy)", scipy_threshold)
                    pytorch_image = draw_enhanced_hand_landmarks(
                        frame, hand_landmarks, handedness, pytorch_floating_status, pytorch_frame_data, " (PyTorch)", pytorch_threshold)
                    
                    # ë‘ ì´ë¯¸ì§€ë¥¼ ê°€ë¡œë¡œ ê²°í•©
                    combined_image = np.hstack([scipy_image, pytorch_image])
                    
                    # ì¤‘ì•™ ë¶„í• ì„  ê·¸ë¦¬ê¸°
                    mid_x = combined_image.shape[1] // 2
                    cv2.line(combined_image, (mid_x, 0), (mid_x, combined_image.shape[0]), (255, 255, 255), 3)
                    
                    # ì¢Œìš° ë ˆì´ë¸” ì¶”ê°€
                    cv2.putText(combined_image, "SciPy (Golden Standard)", (20, combined_image.shape[0] - 20),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
                    cv2.putText(combined_image, "PyTorch (New Implementation)", (mid_x + 20, combined_image.shape[0] - 20),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
                    
                    # ìƒˆë¡œìš´ í¬ê¸°ë¡œ ë¹„ë””ì˜¤ ì„¤ì • (ì²˜ìŒ í”„ë ˆì„ì¼ ë•Œë§Œ)
                    if frame_num == 0:
                        out.release()
                        new_width = combined_image.shape[1]
                        new_height = combined_image.shape[0]
                        # ì´ë¯¸ ì„ íƒëœ í˜¸í™˜ ì½”ë± ì‚¬ìš©
                        out = cv2.VideoWriter(output_path, fourcc, fps, (new_width, new_height))
                    
                    # í–¥ìƒëœ ë¹„êµ ì •ë³´ í—¤ë” ì¶”ê°€
                    annotated_frame = draw_enhanced_comparison_header(
                        combined_image, frame_num, scipy_floating, pytorch_floating, 
                        scipy_frame_data, pytorch_frame_data, total_frames)
                    
                    # ë¶ˆì¼ì¹˜ í”„ë ˆì„ì˜ ê²½ìš° íŠ¹ë³„í•œ ê°•ì¡° í‘œì‹œ
                    if scipy_floating != pytorch_floating:
                        # í™”ë©´ ì „ì²´ì— ë¹¨ê°„ í…Œë‘ë¦¬ (ë¶ˆì¼ì¹˜ ê°•ì¡°)
                        cv2.rectangle(annotated_frame, (0, 0), 
                                    (annotated_frame.shape[1]-1, annotated_frame.shape[0]-1), 
                                    (0, 0, 255), 8)
                        
                        # ê²½ê³  ì•„ì´ì½˜ ì¶”ê°€
                        warning_x = annotated_frame.shape[1] // 2 - 100
                        warning_y = 140
                        cv2.putText(annotated_frame, "DISAGREEMENT DETECTED", 
                                   (warning_x, warning_y), cv2.FONT_HERSHEY_SIMPLEX, 
                                   0.8, (0, 255, 255), 2)
                    
                    out.write(annotated_frame)
                else:
                    # ì†ì´ ì—†ëŠ” í”„ë ˆì„ì€ ì›ë³¸ í”„ë ˆì„ë§Œ í‘œì‹œ
                    no_hands_image = np.hstack([frame, frame])
                    
                    # ìƒˆë¡œìš´ í¬ê¸°ë¡œ ë¹„ë””ì˜¤ ì„¤ì • (ì²˜ìŒ í”„ë ˆì„ì¼ ë•Œë§Œ)
                    if frame_num == 0:
                        out.release()
                        new_width = no_hands_image.shape[1]
                        new_height = no_hands_image.shape[0]
                        # ì´ë¯¸ ì„ íƒëœ í˜¸í™˜ ì½”ë± ì‚¬ìš©
                        out = cv2.VideoWriter(output_path, fourcc, fps, (new_width, new_height))
                    
                    # "No Hands Detected" í‘œì‹œ
                    cv2.putText(no_hands_image, f"Frame: {frame_num} - No Hands Detected", 
                               (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)
                    
                    out.write(no_hands_image)
                
                frame_num += 1
                
                # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
                pbar.update(1)
                pbar.set_description(f"ìƒì„¸ ë¹„ë””ì˜¤ ìƒì„± ({frame_num}/{total_frames})")
        
        cap.release()
        out.release()
        
        # ê¸°ì¡´ ì²˜ë¦¬ëœ ë°ì´í„° ì¬ì‚¬ìš©ìœ¼ë¡œ ìºì‹œ ì €ì¥ ë¶ˆí•„ìš”
        # handlistì—ì„œ ì´ë¯¸ ëª¨ë“  landmark ë°ì´í„°ë¥¼ ê°€ì ¸ì™”ìœ¼ë¯€ë¡œ ì¶”ê°€ ìºì‹œ ìƒì„± ì—†ìŒ
        
        # ë¹„ë””ì˜¤ ìƒì„± ê²€ì¦ ë° ë©”íƒ€ë°ì´í„° í™•ì¸
        self.verify_video_compatibility(output_path, selected_codec)
        
        print(f"âœ… ìƒì„¸ ë¹„êµ ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ: {output_path}")
        print(f"   ğŸ“ í•´ìƒë„: {width*2}x{height} (ì¢Œ: SciPy, ìš°: PyTorch)")
        print(f"   ğŸ¬ í”„ë ˆì„ ìˆ˜: {frame_num}/{total_frames}")
        print(f"   ğŸ¥ ì‚¬ìš© ì½”ë±: {selected_codec}")
        print(f"   âš¡ MediaPipe ì¬ê³„ì‚° ìƒëµìœ¼ë¡œ ì²˜ë¦¬ ì‹œê°„ ëŒ€í­ ë‹¨ì¶•")
        
        # ì˜¤ë””ì˜¤ í•©ì„± ì‹œë„ - ì›ë³¸ ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ê°€ì ¸ì˜¤ê¸°
        original_video_path = os.path.join(self.video_capture_dir, f"{self.target_video}.mp4")
        final_output_path = self.add_audio_to_video(output_path, original_video_path)
        
        return final_output_path
    

    
    def verify_video_compatibility(self, video_path: str, codec_used: str):
        """ë¹„ë””ì˜¤ íŒŒì¼ í˜¸í™˜ì„± ê²€ì¦ ë° ë©”íƒ€ë°ì´í„° í™•ì¸"""
        try:
            # íŒŒì¼ ì¡´ì¬ ë° í¬ê¸° í™•ì¸
            if not os.path.exists(video_path):
                print(f"   âŒ ë¹„ë””ì˜¤ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {video_path}")
                return False
            
            file_size = os.path.getsize(video_path)
            if file_size == 0:
                print(f"   âŒ ë¹„ë””ì˜¤ íŒŒì¼ì´ ë¹„ì–´ìˆìŒ: {video_path}")
                return False
            
            # OpenCVë¡œ ë¹„ë””ì˜¤ ì½ê¸° í…ŒìŠ¤íŠ¸
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                print(f"   âŒ ë¹„ë””ì˜¤ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {video_path}")
                return False
            
            # ë©”íƒ€ë°ì´í„° í™•ì¸
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            fourcc = int(cap.get(cv2.CAP_PROP_FOURCC))
            
            # ì²« í”„ë ˆì„ ì½ê¸° í…ŒìŠ¤íŠ¸
            ret, frame = cap.read()
            if not ret:
                print(f"   âŒ ë¹„ë””ì˜¤ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨: {video_path}")
                cap.release()
                return False
            
            cap.release()
            
            # ë©”íƒ€ë°ì´í„° ì¶œë ¥
            print(f"   ğŸ” ë¹„ë””ì˜¤ ê²€ì¦ ê²°ê³¼:")
            print(f"      ğŸ“ íŒŒì¼ í¬ê¸°: {file_size/1024/1024:.1f}MB")
            print(f"      ğŸ“ í•´ìƒë„: {width}x{height}")
            print(f"      ğŸ¬ í”„ë ˆì„: {frame_count}ê°œ")
            print(f"      ğŸ“¹ FPS: {fps:.1f}")
            print(f"      ğŸ¥ ì½”ë±: {codec_used}")
            
            # VSCode í˜¸í™˜ì„± ì¶”ì²œ ì‚¬í•­ í™•ì¸
            if codec_used in ['avc1', 'H264']:
                print(f"   âœ… VSCode ìµœì  í˜¸í™˜ì„±: {codec_used} ì½”ë± ì‚¬ìš©")
            elif codec_used in ['mp4v']:
                print(f"   âš ï¸  VSCode ê¸°ë³¸ í˜¸í™˜ì„±: {codec_used} ì½”ë± ì‚¬ìš©")
                print(f"      ğŸ’¡ H.264 ì½”ë±ì´ ë” ë‚˜ì€ í˜¸í™˜ì„±ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            else:
                print(f"   âš ï¸  VSCode í˜¸í™˜ì„± ì£¼ì˜: {codec_used} ì½”ë±")
                print(f"      ğŸ’¡ VSCodeì—ì„œ ì¬ìƒë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            
            # ì¶”ê°€ í˜¸í™˜ì„± íŒ
            print(f"   ğŸ’¡ VSCode ì¬ìƒ íŒ:")
            print(f"      1. íŒŒì¼ íƒìƒ‰ê¸°ì—ì„œ ìš°í´ë¦­ â†’ 'ì—°ê²° í”„ë¡œê·¸ë¨' â†’ 'ë¯¸ë””ì–´ í”Œë ˆì´ì–´'")
            print(f"      2. ë¸Œë¼ìš°ì €ì—ì„œ ë“œë˜ê·¸ ì•¤ ë“œë¡­ìœ¼ë¡œ ì¬ìƒ")
            print(f"      3. VLC ë“± ì™¸ë¶€ í”Œë ˆì´ì–´ë¡œ ì¬ìƒ")
            
            return True
            
        except Exception as e:
            print(f"   âŒ ë¹„ë””ì˜¤ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def add_audio_to_video(self, video_path: str, source_video_path: str) -> str:
        """ì›ë³¸ ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ë¥¼ ì¶”ì¶œí•˜ì—¬ ìƒì„±ëœ ë¹„ë””ì˜¤ì— í•©ì„±"""
        try:
            import subprocess
            
            # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì„¤ì •
            video_name = os.path.splitext(video_path)[0]
            output_with_audio = f"{video_name}_with_audio.mp4"
            
            print(f"ğŸµ ì˜¤ë””ì˜¤ í•©ì„± ì‹œì‘:")
            print(f"   ğŸ“¹ ë¹„ë””ì˜¤: {video_path}")
            print(f"   ğŸ§ ì˜¤ë””ì˜¤ ì†ŒìŠ¤: {source_video_path}")
            print(f"   ğŸ¬ ì¶œë ¥: {output_with_audio}")
            
            # 1ë‹¨ê³„: íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(video_path):
                print(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {video_path}")
                return video_path
                
            if not os.path.exists(source_video_path):
                print(f"âŒ ì˜¤ë””ì˜¤ ì†ŒìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {source_video_path}")
                return video_path
            
            # 2ë‹¨ê³„: ffmpeg ì„¤ì¹˜ í™•ì¸
            try:
                ffmpeg_version = subprocess.run(['ffmpeg', '-version'], 
                                              capture_output=True, text=True, timeout=5)
                if ffmpeg_version.returncode != 0:
                    raise FileNotFoundError("ffmpeg ì‹¤í–‰ ì‹¤íŒ¨")
                print(f"   âœ… ffmpeg í™•ì¸ë¨")
            except (FileNotFoundError, subprocess.TimeoutExpired):
                print(f"âŒ ffmpegë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print(f"ğŸ’¡ ffmpeg ì„¤ì¹˜ ë°©ë²•:")
                print(f"   Ubuntu/Debian: sudo apt-get install ffmpeg")
                print(f"   Windows: https://ffmpeg.org/download.html")
                print(f"   macOS: brew install ffmpeg")
                return video_path
            
            # 3ë‹¨ê³„: ì›ë³¸ ë¹„ë””ì˜¤ì˜ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ í™•ì¸
            print(f"   ğŸ” ì›ë³¸ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ë¶„ì„ ì¤‘...")
            probe_cmd = [
                'ffprobe', '-v', 'quiet', '-print_format', 'json', 
                '-show_streams', source_video_path
            ]
            
            probe_result = subprocess.run(probe_cmd, capture_output=True, text=True)
            if probe_result.returncode != 0:
                print(f"âŒ ì›ë³¸ ë¹„ë””ì˜¤ ë¶„ì„ ì‹¤íŒ¨: {probe_result.stderr}")
                return video_path
            
            # JSON íŒŒì‹±í•˜ì—¬ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ í™•ì¸
            import json
            try:
                streams_info = json.loads(probe_result.stdout)
                audio_streams = [s for s in streams_info.get('streams', []) 
                               if s.get('codec_type') == 'audio']
                
                if not audio_streams:
                    print(f"âŒ ì›ë³¸ ë¹„ë””ì˜¤ì— ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                    print(f"   ğŸ“Š ë°œê²¬ëœ ìŠ¤íŠ¸ë¦¼: {len(streams_info.get('streams', []))}ê°œ")
                    for i, stream in enumerate(streams_info.get('streams', [])):
                        print(f"      ìŠ¤íŠ¸ë¦¼ {i}: {stream.get('codec_type', 'unknown')} - {stream.get('codec_name', 'unknown')}")
                    return video_path
                
                print(f"   âœ… ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ë°œê²¬: {len(audio_streams)}ê°œ")
                audio_codec = audio_streams[0].get('codec_name', 'unknown')
                print(f"      ì½”ë±: {audio_codec}")
                
            except json.JSONDecodeError:
                print(f"âŒ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì •ë³´ íŒŒì‹± ì‹¤íŒ¨")
                return video_path
            
            # 4ë‹¨ê³„: ë¹„ë””ì˜¤ ê¸¸ì´ ê³„ì‚°
            cap = cv2.VideoCapture(video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            video_duration = frame_count / fps
            cap.release()
            
            print(f"   ğŸ“Š ë¹„ë””ì˜¤ ì •ë³´:")
            print(f"      í”„ë ˆì„ ìˆ˜: {frame_count}")
            print(f"      FPS: {fps:.2f}")
            print(f"      ê¸¸ì´: {video_duration:.2f}ì´ˆ")
            
            # 5ë‹¨ê³„: ì˜¤ë””ì˜¤ í•©ì„± ì‹œë„ (ì—¬ëŸ¬ ë°©ë²•)
            success = False
            
            # ë°©ë²• 1: ê¸°ë³¸ AAC ì½”ë±
            print(f"   ğŸ¯ ë°©ë²• 1: AAC ì½”ë± ì‹œë„...")
            cmd1 = [
                'ffmpeg',
                '-i', video_path,          # ë¹„ë””ì˜¤ ì…ë ¥
                '-i', source_video_path,   # ì˜¤ë””ì˜¤ ì†ŒìŠ¤
                '-c:v', 'copy',            # ë¹„ë””ì˜¤ ì½”ë± ë³µì‚¬
                '-c:a', 'aac',             # ì˜¤ë””ì˜¤ ì½”ë± AAC
                '-map', '0:v:0',           # ì²« ë²ˆì§¸ ì…ë ¥ì˜ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼
                '-map', '1:a:0',           # ë‘ ë²ˆì§¸ ì…ë ¥ì˜ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼
                '-t', str(video_duration), # ë¹„ë””ì˜¤ ê¸¸ì´ë§Œí¼ ì˜¤ë””ì˜¤ ìë¥´ê¸°
                '-y',                      # ì¶œë ¥ íŒŒì¼ ë®ì–´ì“°ê¸°
                output_with_audio
            ]
            
            result1 = subprocess.run(cmd1, capture_output=True, text=True)
            if result1.returncode == 0 and os.path.exists(output_with_audio):
                success = True
                print(f"   âœ… ë°©ë²• 1 ì„±ê³µ!")
            else:
                print(f"   âŒ ë°©ë²• 1 ì‹¤íŒ¨: {result1.stderr[:200]}...")
                
                # ë°©ë²• 2: ì˜¤ë””ì˜¤ ì½”ë± ë³µì‚¬
                print(f"   ğŸ¯ ë°©ë²• 2: ì˜¤ë””ì˜¤ ì½”ë± ë³µì‚¬ ì‹œë„...")
                cmd2 = [
                    'ffmpeg',
                    '-i', video_path,
                    '-i', source_video_path,
                    '-c:v', 'copy',
                    '-c:a', 'copy',            # ì˜¤ë””ì˜¤ ì½”ë±ë„ ë³µì‚¬
                    '-map', '0:v:0',
                    '-map', '1:a:0',
                    '-t', str(video_duration),
                    '-y',
                    output_with_audio
                ]
                
                result2 = subprocess.run(cmd2, capture_output=True, text=True)
                if result2.returncode == 0 and os.path.exists(output_with_audio):
                    success = True
                    print(f"   âœ… ë°©ë²• 2 ì„±ê³µ!")
                else:
                    print(f"   âŒ ë°©ë²• 2 ì‹¤íŒ¨: {result2.stderr[:200]}...")
                    
                    # ë°©ë²• 3: ë‹¨ìˆœ í•©ì„± (shortest ì˜µì…˜)
                    print(f"   ğŸ¯ ë°©ë²• 3: ë‹¨ìˆœ í•©ì„± ì‹œë„...")
                    cmd3 = [
                        'ffmpeg',
                        '-i', video_path,
                        '-i', source_video_path,
                        '-c:v', 'copy',
                        '-c:a', 'aac',
                        '-shortest',               # ì§§ì€ ìŠ¤íŠ¸ë¦¼ì— ë§ì¶¤
                        '-y',
                        output_with_audio
                    ]
                    
                    result3 = subprocess.run(cmd3, capture_output=True, text=True)
                    if result3.returncode == 0 and os.path.exists(output_with_audio):
                        success = True
                        print(f"   âœ… ë°©ë²• 3 ì„±ê³µ!")
                    else:
                        print(f"   âŒ ë°©ë²• 3 ì‹¤íŒ¨: {result3.stderr[:200]}...")
            
            if success:
                # ì„±ê³µí•œ ê²½ìš° íŒŒì¼ í¬ê¸° ë¹„êµ
                original_size = os.path.getsize(video_path) / 1024 / 1024
                new_size = os.path.getsize(output_with_audio) / 1024 / 1024
                
                print(f"âœ… ì˜¤ë””ì˜¤ í•©ì„± ì™„ë£Œ!")
                print(f"   ğŸ“Š íŒŒì¼ í¬ê¸° ë¹„êµ:")
                print(f"      ì›ë³¸ (ë¬´ìŒ): {original_size:.1f}MB")
                print(f"      ì˜¤ë””ì˜¤ í¬í•¨: {new_size:.1f}MB")
                print(f"      ì¦ê°€ëŸ‰: {new_size - original_size:.1f}MB")
                
                # VSCode í˜¸í™˜ì„±ì„ ìœ„í•œ H.264 ë³€í™˜
                final_h264_path = self.convert_to_h264(output_with_audio)
                
                # ì¤‘ê°„ íŒŒì¼ë“¤ ì •ë¦¬
                try:
                    os.remove(video_path)  # ì›ë³¸ ë¬´ìŒ ë¹„ë””ì˜¤ ì‚­ì œ
                    print(f"   ğŸ—‘ï¸  ì›ë³¸ ë¬´ìŒ ë¹„ë””ì˜¤ ì‚­ì œ: {video_path}")
                except:
                    print(f"   âš ï¸  ì›ë³¸ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨")
                
                if final_h264_path != output_with_audio:
                    # H.264 ë³€í™˜ì´ ì„±ê³µí•œ ê²½ìš° ì¤‘ê°„ íŒŒì¼ ì‚­ì œ
                    try:
                        os.remove(output_with_audio)
                        print(f"   ğŸ—‘ï¸  ì¤‘ê°„ íŒŒì¼ ì‚­ì œ: {output_with_audio}")
                    except:
                        print(f"   âš ï¸  ì¤‘ê°„ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨")
                
                return final_h264_path
            else:
                print(f"âŒ ëª¨ë“  ì˜¤ë””ì˜¤ í•©ì„± ë°©ë²• ì‹¤íŒ¨")
                print(f"   ì›ë³¸ ë¹„ë””ì˜¤ ë°˜í™˜: {video_path}")
                return video_path
                
        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ í•©ì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            print(f"   ì›ë³¸ ë¹„ë””ì˜¤ ë°˜í™˜: {video_path}")
            import traceback
            traceback.print_exc()
            return video_path
    
    def convert_to_h264(self, input_video_path: str) -> str:
        """VSCode í˜¸í™˜ì„±ì„ ìœ„í•œ H.264 ë³€í™˜"""
        try:
            import subprocess
            
            # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì„¤ì •
            video_name = os.path.splitext(input_video_path)[0]
            h264_output = f"{video_name}_h264.mp4"
            
            print(f"ğŸ¬ VSCode í˜¸í™˜ì„± ìµœì í™” ì‹œì‘:")
            print(f"   ğŸ“¥ ì…ë ¥: {input_video_path}")
            print(f"   ğŸ“¤ ì¶œë ¥: {h264_output}")
            
            # H.264 ë³€í™˜ ëª…ë ¹ì–´ êµ¬ì„±
            cmd = [
                'ffmpeg',
                '-i', input_video_path,
                '-c:v', 'libx264',          # H.264 ë¹„ë””ì˜¤ ì½”ë±
                '-c:a', 'aac',              # AAC ì˜¤ë””ì˜¤ ì½”ë±
                '-preset', 'medium',        # ì¸ì½”ë”© ì†ë„ vs í’ˆì§ˆ ê· í˜•
                '-crf', '23',               # í’ˆì§ˆ ì„¤ì • (18-28 ë²”ìœ„, 23ì´ ê¸°ë³¸)
                '-pix_fmt', 'yuv420p',      # í”½ì…€ í¬ë§· (ìµœëŒ€ í˜¸í™˜ì„±)
                '-profile:v', 'main',       # H.264 í”„ë¡œíŒŒì¼ (í˜¸í™˜ì„± ìš°ì„ )
                '-level', '4.0',            # H.264 ë ˆë²¨
                '-movflags', '+faststart',  # ì›¹ ìŠ¤íŠ¸ë¦¬ë° ìµœì í™”
                '-y',                       # ì¶œë ¥ íŒŒì¼ ë®ì–´ì“°ê¸°
                h264_output
            ]
            
            print(f"   âš™ï¸  H.264 ë³€í™˜ ì¤‘... (ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0 and os.path.exists(h264_output):
                # ë³€í™˜ ì„±ê³µ
                original_size = os.path.getsize(input_video_path) / 1024 / 1024
                h264_size = os.path.getsize(h264_output) / 1024 / 1024
                
                print(f"âœ… H.264 ë³€í™˜ ì™„ë£Œ!")
                print(f"   ğŸ“Š íŒŒì¼ í¬ê¸° ë¹„êµ:")
                print(f"      ë³€í™˜ ì „: {original_size:.1f}MB")
                print(f"      H.264 í›„: {h264_size:.1f}MB")
                print(f"      ì°¨ì´: {h264_size - original_size:+.1f}MB")
                
                # ë¹„ë””ì˜¤ í’ˆì§ˆ í™•ì¸
                probe_cmd = ['ffprobe', '-v', 'quiet', '-print_format', 'json', '-show_streams', h264_output]
                probe_result = subprocess.run(probe_cmd, capture_output=True, text=True)
                
                if probe_result.returncode == 0:
                    import json
                    try:
                        streams_info = json.loads(probe_result.stdout)
                        video_streams = [s for s in streams_info.get('streams', []) if s.get('codec_type') == 'video']
                        audio_streams = [s for s in streams_info.get('streams', []) if s.get('codec_type') == 'audio']
                        
                        if video_streams:
                            video_codec = video_streams[0].get('codec_name', 'unknown')
                            profile = video_streams[0].get('profile', 'unknown')
                            print(f"   ğŸ¥ ë¹„ë””ì˜¤: {video_codec} ({profile})")
                        
                        if audio_streams:
                            audio_codec = audio_streams[0].get('codec_name', 'unknown')
                            print(f"   ğŸµ ì˜¤ë””ì˜¤: {audio_codec}")
                        
                        print(f"   ğŸ’» VSCode í˜¸í™˜ì„±: ìµœì í™” ì™„ë£Œ (H.264 Main Profile)")
                        print(f"   ğŸŒ ì›¹ ë¸Œë¼ìš°ì € í˜¸í™˜ì„±: ìš°ìˆ˜ (faststart í”Œë˜ê·¸)")
                        
                    except json.JSONDecodeError:
                        print(f"   âš ï¸  ë¹„ë””ì˜¤ ì •ë³´ í™•ì¸ ì‹¤íŒ¨ (ê¸°ëŠ¥ìƒ ë¬¸ì œì—†ìŒ)")
                
                return h264_output
            else:
                print(f"âŒ H.264 ë³€í™˜ ì‹¤íŒ¨:")
                print(f"   ì—ëŸ¬: {result.stderr[:300]}...")
                print(f"   ì›ë³¸ íŒŒì¼ ìœ ì§€: {input_video_path}")
                return input_video_path
                
        except Exception as e:
            print(f"âŒ H.264 ë³€í™˜ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            print(f"   ì›ë³¸ íŒŒì¼ ìœ ì§€: {input_video_path}")
            return input_video_path
    
    def make_json_serializable(self, obj):
        """ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
        if isinstance(obj, dict):
            return {key: self.make_json_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self.make_json_serializable(item) for item in obj]
        elif isinstance(obj, tuple):
            return [self.make_json_serializable(item) for item in obj]
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        elif obj is None:
            return None
        elif isinstance(obj, (str, int, float)):
            return obj
        else:
            # ê¸°íƒ€ ê°ì²´ëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜
            return str(obj)
    
    def run_benchmark(self) -> Dict[str, Any]:
        """ì „ì²´ ë²¤ì¹˜ë§ˆí‚¹ ì‹¤í–‰ - ìƒì„¸ ëª¨ë‹ˆí„°ë§ í¬í•¨"""
        mode_text = f"({self.frame_limit}í”„ë ˆì„ ì œí•œ)" if self.quick_test else "(ì „ì²´ í”„ë ˆì„)"
        print(f"ğŸš€ {self.target_video} ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘ {mode_text}")
        print("=" * 60)
        
        # ëª¨ë‹ˆí„°ë§ ì‹œì‘
        monitor.start_monitoring(total_steps=5)
        
        try:
            # 1. ë°ì´í„° ì°¾ê¸° ë° ë¡œë“œ
            monitor.start_step("ë°ì´í„° ë¡œë“œ ë° ì´ˆê¸°í™”", 3)
            
            print("ğŸ” ë²¤ì¹˜ë§ˆí‚¹ ëŒ€ìƒ ë°ì´í„° ê²€ìƒ‰ ì¤‘...")
            data_info = self.find_target_data()
            monitor.update_step_progress(1, f"ë°ì´í„° ìœ„ì¹˜: {data_info['data_dir']}")
            
            print("ğŸ“ ì† ë°ì´í„° ë° ë¹„ë””ì˜¤ ì •ë³´ ë¡œë”© ì¤‘...")
            handlist, existing_floating, ratio = self.load_data(data_info)
            monitor.update_step_progress(2, f"handlist: {len(handlist)}í”„ë ˆì„ ë¡œë”© ì™„ë£Œ")
            
            # ì‹¤ì œ ì²˜ë¦¬í•  í”„ë ˆì„ ìˆ˜ ê³„ì‚°
            actual_frame_count = len([hand for hands in handlist if hands for hand in hands])
            total_hands = sum(len(hands) for hands in handlist if hands)
            
            monitor.update_step_progress(3, f"ë¶„ì„ ì¤€ë¹„: {total_hands:,}ê°œ ì† ë°ì´í„°")
            monitor.finish_step(f"ì´ {total_hands:,}ê°œ ì†, {len(handlist)}í”„ë ˆì„ ë¡œë”© ì™„ë£Œ")
            
            print(f"\nğŸ“Š ë²¤ì¹˜ë§ˆí‚¹ ë°ì´í„° ìš”ì•½:")
            print(f"   ğŸ¬ ë¹„ë””ì˜¤: {data_info['video_name']}")
            print(f"   ğŸ“ í•´ìƒë„ ë¹„ìœ¨: {ratio:.3f}")
            print(f"   ğŸ–¼ï¸  ì´ í”„ë ˆì„: {len(handlist):,}ê°œ")
            print(f"   ğŸ‘‹ ì´ ì† ë°ì´í„°: {total_hands:,}ê°œ")
            print(f"   ğŸ“Š ê¸°ì¡´ floating: {len(existing_floating):,}ê°œ")
            
            # 2. SciPy ì›ë³¸ ë²„ì „ ì‹¤í–‰ (Golden Standard)
            print(f"\nâš™ï¸  Golden Standard ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘...")
            print(f"ğŸ”¬ SciPy ê¸°ë°˜ ì›ë³¸ ì•Œê³ ë¦¬ì¦˜ (ê³¼í•™ ì—°ì‚° ì •ë°€ë„)")
            scipy_results, scipy_time = self.run_scipy_detection(handlist, ratio)
            
            # 3. PyTorch ìˆœìˆ˜ ë²„ì „ ì‹¤í–‰ (ìƒˆ êµ¬í˜„)
            print(f"\nğŸš€ ì‹ ê·œ êµ¬í˜„ ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘...")
            print(f"ğŸ® PyTorch GPU ê°€ì† ì•Œê³ ë¦¬ì¦˜ (ì„±ëŠ¥ ìµœì í™”)")
            pytorch_results, pytorch_time = self.run_pytorch_detection(handlist, ratio)
            
            # 4. ê²°ê³¼ ë¶„ì„
            monitor.start_step("ê²°ê³¼ ì •í™•ë„ ë¶„ì„", 1)
            print("ğŸ“Š SciPy vs PyTorch ê²°ê³¼ ë¹„êµ ë¶„ì„ ì¤‘...")
            analysis_start = time.time()
            analysis = self.analyze_results(scipy_results, pytorch_results, existing_floating, 
                                          scipy_time, pytorch_time)
            analysis_time = time.time() - analysis_start
            monitor.update_step_progress(1, f"ì •í™•ë„ ë¶„ì„: {analysis['agreement_rate']:.1f}% ì¼ì¹˜ìœ¨")
            monitor.finish_step(f"ë¶„ì„ ì™„ë£Œ: {analysis['total_comparisons']:,}ê°œ ë¹„êµ")
            
            # 5. ë¹„êµ ë¹„ë””ì˜¤ ìƒì„±
            monitor.start_step("ë¹„êµ ë¹„ë””ì˜¤ ìƒì„±", 1)
            
            detailed_video_path = ""
            
            if self.generate_detailed_video:
                print("ğŸ¥ ìƒì„¸ ë¹„êµ ë¹„ë””ì˜¤ ìƒì„± ì¤‘... (Hand landmarks í¬í•¨)")
                video_start = time.time()
                detailed_video_path = self.create_comparison_video(data_info, scipy_results, pytorch_results, existing_floating, handlist)
                video_time = time.time() - video_start
                monitor.update_step_progress(1, f"ìƒì„¸ ë¹„ë””ì˜¤: {detailed_video_path} ({video_time:.1f}ì´ˆ)")
            else:
                print("âš ï¸  ë¹„ë””ì˜¤ ìƒì„±ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                monitor.update_step_progress(1, "ë¹„ë””ì˜¤ ìƒì„± ìƒëµ")
            
            monitor.finish_step("ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ")
            
            analysis['comparison_video_detailed'] = detailed_video_path
            
            return analysis
            
        finally:
            # ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
            monitor.stop_monitoring()

    def print_summary(self, results: Dict[str, Any]):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥ - í˜„ì‹¤ì ì¸ ì •ë°€ë„ ê¸°ì¤€ ë°˜ì˜"""
        print("\n" + "=" * 60)
        mode_text = f" (Quick Test: {self.frame_limit}í”„ë ˆì„)" if self.quick_test else ""
        cache_text = " [ìºì‹œ ì‚¬ìš©]" if self.quick_test and self.enable_caching else ""
        print(f"ğŸ“‹ {self.target_video} ë²¤ì¹˜ë§ˆí‚¹ ê²°ê³¼{mode_text}{cache_text}")
        print("=" * 60)
        print(f"ğŸ¯ ë¹„êµ ëŒ€ìƒ: SciPy ì›ë³¸ (Golden Standard) vs PyTorch ìˆœìˆ˜ êµ¬í˜„")
        
        if self.quick_test and self.enable_caching:
            print(f"ğŸ’¾ ìºì‹œ ì •ë³´: {self.cache_data_dir}")
            print(f"ğŸ“ ì œí•œ ë¹„ë””ì˜¤: {self.limited_video_path}")
        
        print(f"\nâ±ï¸  ì„±ëŠ¥:")
        print(f"   SciPy ì›ë³¸: {results['scipy_time']:.2f}ì´ˆ (CPU)")
        print(f"   PyTorch ìˆœìˆ˜: {results['pytorch_time']:.2f}ì´ˆ (GPU)")
        print(f"   ì„±ëŠ¥ í–¥ìƒ: {results['speedup']:.1f}ë°°")
        
        print(f"\nğŸ¯ ì •í™•ë„ ë¹„êµ:")
        print(f"   ì¼ì¹˜ìœ¨: {results['agreement_rate']:.1f}%")
        print(f"   ì´ ë¹„êµ: {results['total_comparisons']}ê°œ")
        print(f"   ë¶ˆì¼ì¹˜: {len(results['status_mismatches'])}ê°œ")
        
        print(f"\nğŸ” Floating ê°ì§€:")
        print(f"   SciPy ì›ë³¸: {results['scipy_floating_count']}ê°œ")
        print(f"   PyTorch ìˆœìˆ˜: {results['pytorch_floating_count']}ê°œ")
        print(f"   ê¸°ì¡´ ë°ì´í„°: {results['existing_floating_count']}ê°œ")
        
        print(f"\nğŸ“ ìˆ˜ì¹˜ì  ì°¨ì´ ë¶„ì„:")
        print(f"   í‰ê·  ê¹Šì´ ì°¨ì´: {results['avg_depth_difference']:.3f}")
        print(f"   ìµœëŒ€ ê¹Šì´ ì°¨ì´: {results['max_depth_difference']:.3f}")
        print(f"   í—ˆìš©ì˜¤ì°¨ ì´ˆê³¼: {results['meaningful_diff_count']}ê°œ ({results['meaningful_diff_rate']:.1f}%)")
        print(f"   ê²½ê³„ê°’(0.9) ê·¼ì²˜: {results['near_boundary_cases']}ê°œ")
        
        if results.get('comparison_video_detailed'):
            print(f"\nğŸ¥ ìƒì„¸ ë¹„êµ ë¹„ë””ì˜¤: {results['comparison_video_detailed']}")
            print(f"   ğŸ“ í•´ìƒë„: 2ë°° í™•ëŒ€ (ì¢Œ: SciPy, ìš°: PyTorch)")
            print(f"   ğŸ‘ï¸ Hand landmarks ë° floating ìƒíƒœ ì‹œê°í™”")
            print(f"   ğŸ’» VSCode í˜¸í™˜: ìµœì í™”ëœ ì½”ë± ì‚¬ìš©")
            if "h264" in results['comparison_video_detailed']:
                print(f"   ğŸµ ì˜¤ë””ì˜¤ í¬í•¨: ì›ë³¸ ë¹„ë””ì˜¤ ì‚¬ìš´ë“œ í•©ì„±")
                print(f"   ğŸ¬ ì½”ë±: H.264 (VSCode ìµœì  í˜¸í™˜ì„±)")
            elif "with_audio" in results['comparison_video_detailed']:
                print(f"   ğŸµ ì˜¤ë””ì˜¤ í¬í•¨: ì›ë³¸ ë¹„ë””ì˜¤ ì‚¬ìš´ë“œ í•©ì„±")
                print(f"   âš ï¸  H.264 ë³€í™˜ ì‹¤íŒ¨: ê¸°ë³¸ ì½”ë± ì‚¬ìš©")
            else:
                print(f"   ğŸ”‡ ì˜¤ë””ì˜¤ ì—†ìŒ: ffmpeg ì‚¬ìš© ë¶ˆê°€ ë˜ëŠ” ì˜¤ë””ì˜¤ í•©ì„± ì‹¤íŒ¨")
        
        if results['status_mismatches']:
            print(f"\nâš ï¸  ì£¼ìš” ë¶ˆì¼ì¹˜ ì¼€ì´ìŠ¤ (ê²½ê³„ê°’ ê·¼ì²˜ í¬í•¨):")
            for mismatch in results['status_mismatches'][:5]:
                boundary_marker = " [ê²½ê³„ê°’]" if mismatch.get('near_boundary', False) else ""
                print(f"   Frame {mismatch['frame']} {mismatch['handtype']}: "
                      f"SciPy={mismatch['scipy_status']}({mismatch['scipy_depth']:.3f}) vs "
                      f"PyTorch={mismatch['pytorch_status']}({mismatch['pytorch_depth']:.3f})"
                      f" ì°¨ì´={mismatch['depth_diff']:.3f}{boundary_marker}")


def process_multiple_videos():
    """ì—¬ëŸ¬ ë¹„ë””ì˜¤ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    benchmark = SmallDataBenchmark()
    
    if not benchmark.target_videos:
        print("âŒ ì²˜ë¦¬í•  ë¹„ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    all_results = []
    
    for i, video_name in enumerate(benchmark.target_videos, 1):
        print(f"\n{'='*80}")
        print(f"ğŸ¬ [{i}/{len(benchmark.target_videos)}] ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘: {video_name}")
        print(f"   ì„ê³„ê°’: {benchmark.FIXED_THRESHOLD}")
        print(f"{'='*80}")
        
        # í˜„ì¬ ë¹„ë””ì˜¤ ì„¤ì •
        benchmark.set_current_video(video_name)
        
        try:
            # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
            results = benchmark.run_benchmark()
            
            if results:
                all_results.append({
                    'video_name': video_name,
                    'results': results
                })
                
                # ê°œë³„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
                print(f"\nâœ… [{i}/{len(benchmark.target_videos)}] ì™„ë£Œ: {video_name}")
                benchmark.print_summary(results)
                
        except Exception as e:
            print(f"âŒ [{i}/{len(benchmark.target_videos)}] ì‹¤íŒ¨: {video_name}")
            print(f"   ì˜¤ë¥˜: {str(e)}")
            continue
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*80}")
    print(f"ğŸ¯ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"   ì„±ê³µ: {len(all_results)}/{len(benchmark.target_videos)}ê°œ")
    print(f"   ì„ê³„ê°’: {benchmark.FIXED_THRESHOLD}")
    print(f"{'='*80}")
    
    for result in all_results:
        video_name = result['video_name']
        res = result['results']
        print(f"\nğŸ“Š {video_name}:")
        print(f"   SciPy ì‹œê°„: {res.get('scipy_time', 0):.2f}ì´ˆ")
        print(f"   PyTorch ì‹œê°„: {res.get('pytorch_time', 0):.2f}ì´ˆ")
        print(f"   ì¼ì¹˜ìœ¨: {res.get('accuracy', {}).get('overall_accuracy', 0):.1f}%")
        
        if res.get('video_generated'):
            print(f"   ë¹„ë””ì˜¤: {res.get('output_video_path', 'ìƒì„±ë¨')}")

if __name__ == "__main__":
    print("ğŸš€ PianoVAM Floating Hand Detection ë²¤ì¹˜ë§ˆí¬")
    print("=" * 60)
    
    # ì—¬ëŸ¬ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹¤í–‰
    process_multiple_videos() 