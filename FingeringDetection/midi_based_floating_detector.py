#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MIDI ê¸°ë°˜ Floating Hand Detection
MIDIê°€ ì „í˜€ ì—°ì£¼ë˜ì§€ ì•Šì€ ìˆœê°„ì—ë§Œ floating handsë¡œ íŒì •
"""

import mido
import numpy as np
import json
import pickle
import cv2
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import re
from stqdm import stqdm
import time


class MIDIBasedFloatingDetector:
    """MIDI ì •ë³´ë¥¼ í™œìš©í•œ ìŠ¤ë§ˆíŠ¸ Floating Hand Detection"""
    
    def __init__(self, 
                 depth_data_dir: str = "depth_data",
                 midi_dir: str = "/home/jhbae/PianoVAM-Code/FingeringDetection/midiconvert",
                 target_fps: int = 20):
        self.depth_data_dir = Path(depth_data_dir)
        self.midi_dir = Path(midi_dir)
        self.target_fps = target_fps
        self.hand_split_note = 60  # Middle C (C4)
        
        # ìºì‹œ
        self._midi_cache = {}
        
    def detect_floating_hands_with_midi(self, handlist: List, video_name: str, ratio: float) -> List:
        """
        MIDI ì •ë³´ë¥¼ í™œìš©í•œ ìŠ¤ë§ˆíŠ¸ floating hands detection
        
        Args:
            handlist: ì† ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            video_name: ë¹„ë””ì˜¤ ì´ë¦„ (MIDI ë§¤ì¹­ìš©)
            ratio: ë¹„ë””ì˜¤ ë¹„ìœ¨
            
        Returns:
            floating_results: [frame, handtype, depth_value, floating_status] ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸµ MIDI ê¸°ë°˜ Floating Hands Detection ì‹œì‘: {video_name}")
        
        # 1. MIDI ë°ì´í„° ë¡œë“œ
        midi_data = self._load_and_match_midi(video_name)
        if not midi_data:
            print(f"   âš ï¸ MIDI ë°ì´í„° ì—†ìŒ - ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±")
            return self._fallback_depth_only_detection(handlist, ratio)
        
        # 2. MIDI íƒ€ì„ë¼ì¸ ìƒì„±
        midi_timeline = self._create_midi_timeline(midi_data)
        print(f"   ğŸµ MIDI íƒ€ì„ë¼ì¸ ìƒì„±: {len(midi_timeline)} ì‹œê°„ì ")
        
        # 3. ê°œì„ ëœ floating detection
        floating_results = []
        
        print(f"   ğŸ” í”„ë ˆì„ë³„ ë¶„ì„ ì‹œì‘...")
        for hands in stqdm(handlist, desc="MIDI ê¸°ë°˜ floating ë¶„ì„"):
            if not hands:
                continue
                
            for hand in hands:
                frame_num = hand.handframe
                hand_type = hand.handtype
                frame_time = frame_num / self.target_fps
                
                # ê¸°ë³¸ ê¹Šì´ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)
                depth_value = self._calculate_simple_depth(hand)
                
                # MIDI ê¸°ë°˜ ì—°ì£¼ ìƒíƒœ í™•ì¸
                is_playing_midi = self._is_hand_playing_at_time(midi_timeline, hand_type, frame_time)
                
                # ìŠ¤ë§ˆíŠ¸ floating íŒì •
                floating_status = self._smart_floating_decision(
                    depth_value, is_playing_midi, hand_type, frame_time
                )
                
                floating_results.append([
                    frame_num,
                    hand_type, 
                    depth_value,
                    floating_status
                ])
        
        # í†µê³„ ì¶œë ¥
        self._print_detection_stats(floating_results, midi_data)
        
        return floating_results
    
    def _load_and_match_midi(self, video_name: str) -> Optional[Dict]:
        """ë¹„ë””ì˜¤ëª…ê³¼ ë§¤ì¹­ë˜ëŠ” MIDI íŒŒì¼ ì°¾ê¸° ë° ë¡œë“œ"""
        # ìºì‹œ í™•ì¸
        if video_name in self._midi_cache:
            return self._midi_cache[video_name]
        
        # ë‚ ì§œ-ì‹œê°„ íŒ¨í„´ ì¶”ì¶œ (YYYY-MM-DD_HH-MM-SS)
        video_pattern = re.search(r'(\d{4}-\d{2}-\d{2}_\d{2}-\d{2}-\d{2})', video_name)
        if not video_pattern:
            print(f"   âš ï¸ ë‚ ì§œ-ì‹œê°„ íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {video_name}")
            return None
        
        target_pattern = video_pattern.group(1)
        
        # ë§¤ì¹­ë˜ëŠ” MIDI íŒŒì¼ ì°¾ê¸°
        midi_files = list(self.midi_dir.glob("*.mid")) + list(self.midi_dir.glob("*.midi"))
        
        for midi_file in midi_files:
            if target_pattern in midi_file.stem:
                print(f"   âœ… MIDI ë§¤ì¹­ ì„±ê³µ: {midi_file.name}")
                midi_data = self._parse_midi_file(midi_file)
                
                # ìºì‹±
                self._midi_cache[video_name] = midi_data
                return midi_data
        
        print(f"   âŒ ë§¤ì¹­ë˜ëŠ” MIDI íŒŒì¼ ì—†ìŒ: {target_pattern}")
        return None
    
    def _parse_midi_file(self, midi_file: Path) -> Dict:
        """MIDI íŒŒì¼ íŒŒì‹± - ì‹¤ì œ í…œí¬ ì‚¬ìš©"""
        try:
            mid = mido.MidiFile(midi_file)
            
            # ì‹¤ì œ í…œí¬ ì¶”ì¶œ
            current_tempo = 500000  # ê¸°ë³¸ê°’ (120 BPM)
            for track in mid.tracks:
                for msg in track:
                    if msg.type == 'set_tempo':
                        current_tempo = msg.tempo
                        break
                if current_tempo != 500000:
                    break
            
            bpm = 60000000 / current_tempo
            print(f"   ğŸµ MIDI í…œí¬: {bpm:.1f} BPM")
            
            # ìŒí‘œ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
            note_segments = {'Left': [], 'Right': []}
            active_notes = {'Left': {}, 'Right': {}}
            
            for track in mid.tracks:
                track_time = 0
                
                for msg in track:
                    track_time += msg.time
                    
                    if msg.type in ['note_on', 'note_off']:
                        time_seconds = mido.tick2second(track_time, mid.ticks_per_beat, current_tempo)
                        hand_type = 'Right' if msg.note >= self.hand_split_note else 'Left'
                        
                        if msg.type == 'note_on' and msg.velocity > 0:
                            # ìŒí‘œ ì‹œì‘
                            active_notes[hand_type][msg.note] = {
                                'start_time': time_seconds,
                                'velocity': msg.velocity
                            }
                        else:
                            # ìŒí‘œ ì¢…ë£Œ
                            if msg.note in active_notes[hand_type]:
                                start_info = active_notes[hand_type].pop(msg.note)
                                duration = time_seconds - start_info['start_time']
                                
                                if duration > 0.01:  # 10ms ì´ìƒ
                                    note_segments[hand_type].append({
                                        'start_time': start_info['start_time'],
                                        'end_time': time_seconds,
                                        'note': msg.note,
                                        'velocity': start_info['velocity'],
                                        'duration': duration
                                    })
            
            total_segments = len(note_segments['Left']) + len(note_segments['Right'])
            max_time = max([seg['end_time'] for segs in note_segments.values() for seg in segs]) if total_segments > 0 else 0
            
            print(f"   ğŸµ íŒŒì‹± ì™„ë£Œ: {total_segments}ê°œ ìŒí‘œ, {max_time:.1f}ì´ˆ")
            
            return {
                'file': midi_file.name,
                'note_segments': note_segments,
                'total_segments': total_segments,
                'duration': max_time,
                'tempo_bpm': bpm
            }
            
        except Exception as e:
            print(f"   âŒ MIDI íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def _create_midi_timeline(self, midi_data: Dict) -> Dict:
        """MIDI íƒ€ì„ë¼ì¸ ìƒì„± - ì‹œê°„ë³„ ì—°ì£¼ ìƒíƒœ"""
        duration = midi_data['duration']
        time_resolution = 0.05  # 50ms í•´ìƒë„
        time_points = int(duration / time_resolution) + 1
        
        timeline = {
            'Left': np.zeros(time_points, dtype=bool),
            'Right': np.zeros(time_points, dtype=bool),
            'time_resolution': time_resolution,
            'duration': duration
        }
        
        # ê° ì†ë³„ë¡œ íƒ€ì„ë¼ì¸ êµ¬ì¶•
        for hand_type in ['Left', 'Right']:
            segments = midi_data['note_segments'].get(hand_type, [])
            
            for segment in segments:
                start_idx = int(segment['start_time'] / time_resolution)
                end_idx = int(segment['end_time'] / time_resolution)
                
                # í•´ë‹¹ ì‹œê°„ êµ¬ê°„ì„ ì—°ì£¼ ì¤‘ìœ¼ë¡œ ë§ˆí‚¹
                if start_idx < time_points and end_idx >= 0:
                    start_idx = max(0, start_idx)
                    end_idx = min(time_points - 1, end_idx)
                    timeline[hand_type][start_idx:end_idx + 1] = True
        
        # í†µê³„
        left_playing_time = np.sum(timeline['Left']) * time_resolution
        right_playing_time = np.sum(timeline['Right']) * time_resolution
        
        print(f"   ğŸ“Š ì—°ì£¼ ì‹œê°„ ë¶„ì„:")
        print(f"     ì¢Œì†: {left_playing_time:.1f}ì´ˆ ({left_playing_time/duration*100:.1f}%)")
        print(f"     ìš°ì†: {right_playing_time:.1f}ì´ˆ ({right_playing_time/duration*100:.1f}%)")
        
        return timeline
    
    def _is_hand_playing_at_time(self, midi_timeline: Dict, hand_type: str, time_seconds: float) -> bool:
        """íŠ¹ì • ì‹œê°„ì— í•´ë‹¹ ì†ì´ ì—°ì£¼ ì¤‘ì¸ì§€ í™•ì¸"""
        time_resolution = midi_timeline['time_resolution']
        time_idx = int(time_seconds / time_resolution)
        
        if 0 <= time_idx < len(midi_timeline[hand_type]):
            return bool(midi_timeline[hand_type][time_idx])
        
        return False
    
    def _calculate_simple_depth(self, hand) -> float:
        """ê°„ë‹¨í•œ ê¹Šì´ ê³„ì‚° (ë³µì¡í•œ 3D ê³„ì‚° ëŒ€ì‹ )"""
        # ì†ëª©ê³¼ ì†ê°€ë½ ëì ì˜ Y ì¢Œí‘œ ì°¨ì´ë¡œ ê°„ë‹¨í•œ ê¹Šì´ ì¶”ì •
        try:
            wrist_y = hand.handlandmark[0].y
            fingertip_y = hand.handlandmark[8].y  # ê²€ì§€ ë
            
            # Y ì°¨ì´ê°€ í´ìˆ˜ë¡ ì†ì´ ë” ìˆ˜ì§ì— ê°€ê¹Œì›€ (floating ê°€ëŠ¥ì„± ë†’ìŒ)
            depth_approximation = abs(wrist_y - fingertip_y) + 0.5
            
            return depth_approximation
        except:
            return 1.0  # ê¸°ë³¸ê°’
    
    def _smart_floating_decision(self, depth_value: float, is_playing_midi: bool, 
                               hand_type: str, frame_time: float) -> str:
        """ìŠ¤ë§ˆíŠ¸ floating íŒì • ë¡œì§"""
        
        # í•µì‹¬ ê·œì¹™: MIDIê°€ ì—°ì£¼ë˜ê³  ìˆìœ¼ë©´ ì ˆëŒ€ floatingì´ ì•„ë‹˜
        if is_playing_midi:
            return 'notfloating'
        
        # MIDIê°€ ì—°ì£¼ë˜ì§€ ì•ŠëŠ” ìˆœê°„ì—ë§Œ ê¹Šì´ë¡œ ì¶”ê°€ íŒì •
        # ë” ì—„ê²©í•œ ì„ê³„ê°’ ì‚¬ìš© (ê¸°ì¡´ 0.9 â†’ 0.7)
        if depth_value < 0.7:
            return 'floating'
        else:
            return 'notfloating'
    
    def _fallback_depth_only_detection(self, handlist: List, ratio: float) -> List:
        """MIDI ë°ì´í„°ê°€ ì—†ì„ ë•Œ ê¸°ì¡´ ê¹Šì´ ê¸°ë°˜ ê²€ì¶œ ì‚¬ìš©"""
        print(f"   ğŸ”„ ê¹Šì´ ê¸°ë°˜ ê²€ì¶œë¡œ í´ë°±")
        
        floating_results = []
        
        for hands in handlist:
            if not hands:
                continue
                
            for hand in hands:
                depth_value = self._calculate_simple_depth(hand)
                floating_status = 'floating' if depth_value < 0.9 else 'notfloating'
                
                floating_results.append([
                    hand.handframe,
                    hand.handtype,
                    depth_value,
                    floating_status
                ])
        
        return floating_results
    
    def _print_detection_stats(self, floating_results: List, midi_data: Optional[Dict]):
        """ê²€ì¶œ ê²°ê³¼ í†µê³„ ì¶œë ¥"""
        if not floating_results:
            return
        
        total_hands = len(floating_results)
        floating_count = len([r for r in floating_results if r[3] == 'floating'])
        playing_count = total_hands - floating_count
        
        print(f"\nğŸ“Š MIDI ê¸°ë°˜ Floating Detection ê²°ê³¼:")
        print(f"   ì´ ì† ë°ì´í„°: {total_hands:,}ê°œ")
        print(f"   Floating íŒì •: {floating_count:,}ê°œ ({floating_count/total_hands*100:.1f}%)")
        print(f"   Playing íŒì •: {playing_count:,}ê°œ ({playing_count/total_hands*100:.1f}%)")
        
        if midi_data:
            print(f"   MIDI ìŒí‘œ ìˆ˜: {midi_data['total_segments']}ê°œ")
            print(f"   MIDI ê¸¸ì´: {midi_data['duration']:.1f}ì´ˆ")
    
    def save_results(self, floating_results: List, output_path: str):
        """ê²°ê³¼ ì €ì¥"""
        with open(output_path, 'wb') as f:
            pickle.dump(floating_results, f)
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")


def detect_floating_with_midi_integration(handlist: List, video_name: str, ratio: float) -> List:
    """
    ë©”ì¸ í•¨ìˆ˜: MIDI í†µí•© floating hands detection
    
    Args:
        handlist: ì† ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        video_name: ë¹„ë””ì˜¤ ì´ë¦„
        ratio: ë¹„ë””ì˜¤ ë¹„ìœ¨
    
    Returns:
        floating_results: [frame, handtype, depth_value, floating_status] ë¦¬ìŠ¤íŠ¸
    """
    detector = MIDIBasedFloatingDetector()
    return detector.detect_floating_hands_with_midi(handlist, video_name, ratio)


if __name__ == "__main__":
    print("ğŸµ MIDI ê¸°ë°˜ Floating Hand Detection")
    print("=" * 50)
    print("âœ… í•µì‹¬ ê°œì„  ì‚¬í•­:")
    print("   â€¢ MIDIê°€ ì—°ì£¼ë˜ëŠ” ìˆœê°„ â†’ ì ˆëŒ€ floating ì•„ë‹˜")
    print("   â€¢ MIDIê°€ ì¡°ìš©í•œ ìˆœê°„ â†’ ê¹Šì´ë¡œ ì¶”ê°€ íŒì •") 
    print("   â€¢ ë” ì •í™•í•˜ê³  ë…¼ë¦¬ì ì¸ floating ê°ì§€")
    print("   â€¢ ì‹¤ì œ ì—°ì£¼ ì˜ë„ë¥¼ ë°˜ì˜í•œ ìŠ¤ë§ˆíŠ¸ íŒì •") 