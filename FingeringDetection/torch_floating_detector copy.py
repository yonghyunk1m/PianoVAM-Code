#!/usr/bin/env python3
"""
PyTorch ê¸°ë°˜ Floating Hand Detection
- PyTorch GPU ê°€ì† ì•Œê³ ë¦¬ì¦˜ë§Œ ì‚¬ìš©
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- ìƒì„¸ ì‹œê°í™” ë¹„ë””ì˜¤ ìƒì„±
"""

import os
import pickle
import time
import numpy as np
import cv2
import json
import sys
from pathlib import Path
from typing import List, Dict, Tuple, Any
import matplotlib.pyplot as plt
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
from PIL import Image
import psutil
import threading
from datetime import datetime, timedelta

# ì§„í–‰ë¥  í‘œì‹œìš© (ì„ íƒì )
try:
    from stqdm import stqdm
    HAS_STQDM = True
except ImportError:
    HAS_STQDM = False



# GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ import
try:
    import torch
    TORCH_AVAILABLE = True
    print("? PyTorch ì‚¬ìš© ê°€ëŠ¥")
    if torch.cuda.is_available():
        print(f"? CUDA GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
    else:
        print("??  CPU ëª¨ë“œë¡œ ì‹¤í–‰")
except ImportError:
    print("? PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    exit(1)

# PyTorch ë²„ì „ floating hands ëª¨ë“ˆ import
try:
    import floatinghands_torch_pure as pytorch_version
    print("? PyTorch Floating Hands ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"? PyTorch Floating Hands ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    exit(1)

# MIDI ë°ì´í„° ì²˜ë¦¬ìš© ëª¨ë“ˆ import (ì„ê³„ê°’ ê³„ì‚°ìš©)
try:
    import midicomparison
    print("? MIDI ë¹„êµ ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"??  MIDI ë¹„êµ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("   MIDI ê¸°ë°˜ ì„ê³„ê°’ ê³„ì‚°ì€ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    midicomparison = None

# main_loop importëŠ” ë‚˜ì¤‘ì— í•„ìš”í•  ë•Œë§Œ í•˜ë„ë¡ ì§€ì—°
main_loop_module = None

def import_main_loop():
    """main_loop ëª¨ë“ˆì„ ì§€ì—° importí•©ë‹ˆë‹¤"""
    global main_loop_module
    if main_loop_module is None:
        try:
            import main_loop as main_loop_module_temp
            main_loop_module = main_loop_module_temp
        except Exception as e:
            print(f"? main_loop ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    return main_loop_module

# MediaPipe ê·¸ë¦¬ê¸° ìœ í‹¸ë¦¬í‹° ì„¤ì •
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles
mp_hands = mp.solutions.hands

def draw_hand_landmarks_with_floating(image, hand_landmarks, handedness, floating_status, depth_info, threshold_value=0.9):
    """Hand landmarksì™€ floating ìƒíƒœë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤"""
    if not hand_landmarks:
        return image
    
    annotated_image = image.copy()
    height, width = annotated_image.shape[:2]
    
    for idx, landmarks in enumerate(hand_landmarks):
        # ì† ì¢…ë¥˜ í™•ì¸
        if idx < len(handedness):
            hand_type = handedness[idx].classification[0].category_name
            
            # ê¹Šì´ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            depth_value = depth_info.get(f'{hand_type}_depth', 0.0)
            is_floating = floating_status.get(f'{hand_type}_floating', False)
            
            # ì„ê³„ê°’ ì„¤ì •
            if isinstance(threshold_value, dict):
                threshold = threshold_value.get(hand_type, 0.9)
            else:
                threshold = threshold_value
            
            # ìƒ‰ìƒ ì„¤ì • (floating ìƒíƒœì— ë”°ë¼)
            if is_floating:
                connection_color = (0, 255, 0)  # ì´ˆë¡ìƒ‰ (floating)
                landmark_color = (0, 255, 0)
                status_text = "FLOATING"
                status_color = (0, 255, 0)
            else:
                connection_color = (255, 0, 0)  # ë¹¨ê°„ìƒ‰ (not floating)
                landmark_color = (255, 0, 0)
                status_text = "PLAYING"
                status_color = (255, 0, 0)
            
            # ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
            mp_drawing.draw_landmarks(
                annotated_image,
                landmarks,
                mp_hands.HAND_CONNECTIONS,
                mp_drawing_styles.get_default_hand_landmarks_style(),
                mp_drawing.DrawingSpec(color=connection_color, thickness=2, circle_radius=2)
            )
            
            # í…ìŠ¤íŠ¸ ì •ë³´ í‘œì‹œ
            text_y_offset = 30 if hand_type == "Left" else 60
            
            # ì† ì¢…ë¥˜ í‘œì‹œ
            cv2.putText(annotated_image, f"{hand_type} Hand", 
                       (10, text_y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            # ìƒíƒœ í‘œì‹œ
            cv2.putText(annotated_image, f"Status: {status_text}", 
                       (10, text_y_offset + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, status_color, 2)
            
            # ê¹Šì´ ê°’ í‘œì‹œ
            cv2.putText(annotated_image, f"Depth: {depth_value:.3f}", 
                       (10, text_y_offset + 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            # ì„ê³„ê°’ í‘œì‹œ
            cv2.putText(annotated_image, f"Threshold: {threshold:.2f}", 
                       (10, text_y_offset + 75), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 128, 128), 2)
    
    return annotated_image

def draw_header_info(image, frame_num, floating_status, depth_info, total_frames, processing_time=None):
    """ë¹„ë””ì˜¤ í—¤ë”ì— ì „ì²´ ì •ë³´ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤"""
    annotated_image = image.copy()
    height, width = annotated_image.shape[:2]
    
    # í—¤ë” ë°°ê²½
    header_height = 100
    cv2.rectangle(annotated_image, (0, 0), (width, header_height), (0, 0, 0), -1)
    
    # ì œëª©
    cv2.putText(annotated_image, "PyTorch Floating Hand Detection", 
               (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    
    # í”„ë ˆì„ ì •ë³´
    cv2.putText(annotated_image, f"Frame: {frame_num}/{total_frames}", 
               (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
    
    # ì²˜ë¦¬ ì‹œê°„ (ìˆëŠ” ê²½ìš°)
    if processing_time:
        cv2.putText(annotated_image, f"Processing: {processing_time:.2f}s", 
                   (10, 75), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
    
    # ìš°ì¸¡ì— ì „ì²´ floating ìƒíƒœ ìš”ì•½
    right_x = width - 300
    cv2.putText(annotated_image, "Floating Status:", 
               (right_x, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
    
    left_floating = floating_status.get('Left_floating', False)
    right_floating = floating_status.get('Right_floating', False)
    
    left_color = (0, 255, 0) if left_floating else (255, 0, 0)
    right_color = (0, 255, 0) if right_floating else (255, 0, 0)
    
    cv2.putText(annotated_image, f"Left: {'FLOATING' if left_floating else 'PLAYING'}", 
               (right_x, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, left_color, 2)
    cv2.putText(annotated_image, f"Right: {'FLOATING' if right_floating else 'PLAYING'}", 
               (right_x, 75), cv2.FONT_HERSHEY_SIMPLEX, 0.5, right_color, 2)
    
    return annotated_image

def create_keyboard_overlay(image, keyboard_coords=None):
    """í‚¤ë³´ë“œ ì˜¤ë²„ë ˆì´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤ (ì„ íƒì‚¬í•­)"""
    if keyboard_coords is None:
        return image
    
    annotated_image = image.copy()
    # í‚¤ë³´ë“œ ì˜ì—­ í‘œì‹œ ë¡œì§ (í•„ìš”ì‹œ êµ¬í˜„)
    return annotated_image

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.start_time = None
        self.current_step = None
        self.step_start_time = None
        self.step_progress = 0
        self.step_total = 0
        self.total_steps = 0
        self.current_step_num = 0
        
    def start_monitoring(self, total_steps=0):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.start_time = time.time()
        self.total_steps = total_steps
        self.current_step_num = 0
        print(f"? ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        self._print_system_info()
        
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ"""
        if self.start_time:
            total_time = time.time() - self.start_time
            print(f"\n? ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ: {self._format_time(total_time)}")
        
    def start_step(self, step_name: str, total_items=0):
        """ìƒˆ ë‹¨ê³„ ì‹œì‘"""
        self.current_step_num += 1
        self.current_step = step_name
        self.step_start_time = time.time()
        self.step_progress = 0
        self.step_total = total_items
        
        progress_text = f"[{self.current_step_num}/{self.total_steps}]" if self.total_steps > 0 else ""
        print(f"\n? {progress_text} {step_name}")
        
    def log_operation(self, operation_name: str, details: str = "", show_gpu=False):
        """ì‘ì—… ë¡œê¹…"""
        print(f"   ??  {operation_name}: {details}")
        if show_gpu and TORCH_AVAILABLE and torch.cuda.is_available():
            gpu_memory = torch.cuda.memory_allocated() / 1024**3
            print(f"      ? GPU ë©”ëª¨ë¦¬: {gpu_memory:.2f}GB")
            
    def log_batch_progress(self, batch_num: int, total_batches: int, batch_size: int, 
                          items_processed: int, operation: str = ""):
        """ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ ìƒí™© ë¡œê¹…"""
        progress = (batch_num / total_batches) * 100
        print(f"   ? ë°°ì¹˜ {batch_num}/{total_batches} ({progress:.1f}%) - "
              f"{items_processed:,}ê°œ ì²˜ë¦¬ë¨ {operation}")
              
    def log_algorithm_start(self, algorithm_name: str, method: str, precision: str, 
                           processing_type: str, optimizations: list = None):
        """ì•Œê³ ë¦¬ì¦˜ ì‹œì‘ ë¡œê¹…"""
        print(f"? ì•Œê³ ë¦¬ì¦˜: {algorithm_name}")
        print(f"   ? í•´ë²•: {method}")
        print(f"   ? ì •ë°€ë„: {precision}")
        print(f"   ? ì²˜ë¦¬: {processing_type}")
        if optimizations:
            print(f"   ? ìµœì í™”:")
            for opt in optimizations:
                print(f"      ? {opt}")
                
    def log_memory_usage(self, stage: str, cpu_memory: float = None, gpu_memory: float = None):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…"""
        if cpu_memory is None:
            cpu_memory = psutil.virtual_memory().used / 1024**3
        
        memory_info = f"   ? {stage} - CPU: {cpu_memory:.2f}GB"
        
        if TORCH_AVAILABLE and torch.cuda.is_available():
            if gpu_memory is None:
                gpu_memory = torch.cuda.memory_allocated() / 1024**3
            memory_info += f", GPU: {gpu_memory:.2f}GB"
            
        print(memory_info)
        
    def update_step_progress(self, current: int, message: str = ""):
        """ë‹¨ê³„ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸"""
        self.step_progress = current
        if self.step_total > 0:
            progress = (current / self.step_total) * 100
            print(f"   ? ì§„í–‰: {current}/{self.step_total} ({progress:.1f}%) {message}")
        else:
            print(f"   ? {message}")
            
    def finish_step(self, message: str = ""):
        """ë‹¨ê³„ ì™„ë£Œ"""
        if self.step_start_time:
            step_time = time.time() - self.step_start_time
            print(f"   ? ì™„ë£Œ: {message} ({self._format_time(step_time)})")
        
    def _print_system_info(self):
        """ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥"""
        print(f"? ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"   ??  CPU: {psutil.cpu_count()}ì½”ì–´")
        print(f"   ? RAM: {psutil.virtual_memory().total / 1024**3:.1f}GB")
        
        if TORCH_AVAILABLE and torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"   ? GPU: {gpu_name} ({gpu_memory:.1f}GB)")
        else:
            print(f"   ? GPU: ì‚¬ìš© ë¶ˆê°€")
            
    def _format_time(self, seconds):
        """ì‹œê°„ í¬ë§·íŒ…"""
        if seconds < 60:
            return f"{seconds:.2f}ì´ˆ"
        elif seconds < 3600:
            minutes = int(seconds // 60)
            secs = seconds % 60
            return f"{minutes}ë¶„ {secs:.1f}ì´ˆ"
        else:
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            secs = seconds % 60
            return f"{hours}ì‹œê°„ {minutes}ë¶„ {secs:.1f}ì´ˆ"

class handclass:
    """Hand class - MediaPipe detection ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, handtype, handlandmark, handframe):
        self.handtype = handtype
        self.handlandmark = handlandmark
        self.handframe = handframe
        self.handdepth = 1  # default
    
    def set_handdepth(self, handdepth):
        self.handdepth = handdepth

def process_raw_video_with_mediapipe(video_path: str, video_name: str, 
                                   min_detection_confidence: float = 0.8,
                                   min_tracking_confidence: float = 0.5,
                                   frame_limit: int = None) -> Tuple[List, float]:
    """Raw ë¹„ë””ì˜¤ë¥¼ MediaPipeë¡œ ì²˜ë¦¬í•˜ì—¬ handlist ìƒì„±"""
    print(f"ğŸ¬ Raw ë¹„ë””ì˜¤ MediaPipe ì²˜ë¦¬ ì‹œì‘: {video_name}")
    
    # MediaPipe ì´ˆê¸°í™”
    with mp_hands.Hands(
        static_image_mode=False,
        max_num_hands=2,
        min_detection_confidence=min_detection_confidence,
        min_tracking_confidence=min_tracking_confidence
    ) as hands:
        
        # ë¹„ë””ì˜¤ ì—´ê¸°
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        
        # ë¹„ë””ì˜¤ ì •ë³´
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        ratio = height / width
        
        print(f"   ğŸ“Š ë¹„ë””ì˜¤ ì •ë³´: {width}x{height}, {fps:.1f}FPS, {total_frames:,}í”„ë ˆì„")
        
        # í”„ë ˆì„ ì œí•œ ì ìš©
        if frame_limit and total_frames > frame_limit:
            total_frames = frame_limit
            print(f"   â±ï¸  Quick test: {frame_limit:,}í”„ë ˆì„ìœ¼ë¡œ ì œí•œ")
        
        handlist = []
        frame = 0
        
        if HAS_STQDM:
            progress_bar = stqdm(range(total_frames), desc=f"ğŸ¬ {video_name} MediaPipe ì²˜ë¦¬")
        else:
            progress_bar = range(total_frames)
            print(f"   ğŸ”„ {total_frames:,}í”„ë ˆì„ ì²˜ë¦¬ ì¤‘... (ì§„í–‰ë¥  í‘œì‹œ: pip install stqdm)")
        
        for _ in progress_bar:
            if frame >= total_frames:
                break
                
            ret, cv_image = cap.read()
            if not ret:
                break
            
            # OpenCV ì´ë¯¸ì§€ë¥¼ RGBë¡œ ë³€í™˜
            rgb_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)
            
            # MediaPipe ì²˜ë¦¬
            results = hands.process(rgb_image)
            
            # ì†ì´ ê°ì§€ëœ ê²½ìš° handclass ê°ì²´ ìƒì„±
            handsinfo = []
            if results.multi_hand_landmarks and results.multi_handedness:
                for j in range(len(results.multi_handedness)):
                    # ì¢Œí‘œ ì •ê·œí™”: [0,1] -> [-1,1]
                    landmarks = results.multi_hand_landmarks[j]
                    for landmark in landmarks.landmark:
                        landmark.x = landmark.x * 2 - 1
                        landmark.y = landmark.y * 2 - 1
                    
                    # handclass ê°ì²´ ìƒì„±
                    try:
                        # MediaPipe handedness ì •ë³´ ì˜¬ë°”ë¥´ê²Œ ì¶”ì¶œ
                        handedness = results.multi_handedness[j]
                        if hasattr(handedness, 'classification') and len(handedness.classification) > 0:
                            hand_type = handedness.classification[0].category_name
                            if frame < 10:  # ì²« 10í”„ë ˆì„ë§Œ ë””ë²„ê¹… ì¶œë ¥
                                print(f"   ğŸ” ê°ì§€ëœ ì† íƒ€ì…: {hand_type} (í”„ë ˆì„ {frame})")
                        else:
                            hand_type = "Left" if j == 0 else "Right"  # ê¸°ë³¸ê°’
                            if frame < 10:  # ì²« 10í”„ë ˆì„ë§Œ ë””ë²„ê¹… ì¶œë ¥
                                print(f"   âš ï¸ handedness ì •ë³´ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©: {hand_type} (í”„ë ˆì„ {frame})")
                    except (AttributeError, IndexError) as e:
                        hand_type = "Left" if j == 0 else "Right"  # ê¸°ë³¸ê°’
                        if frame < 10:  # ì²« 10í”„ë ˆì„ë§Œ ë””ë²„ê¹… ì¶œë ¥
                            print(f"   âš ï¸ handedness ì¶”ì¶œ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©: {hand_type} (í”„ë ˆì„ {frame})")
                    
                    hand_obj = handclass(
                        handtype=hand_type,
                        handlandmark=landmarks.landmark,
                        handframe=frame
                    )
                    handsinfo.append(hand_obj)
            
            handlist.append(handsinfo)
            frame += 1
        
        cap.release()
        
        # í†µê³„
        total_hands = sum(len(hands) for hands in handlist)
        frames_with_hands = len([hands for hands in handlist if hands])
        
        print(f"âœ… MediaPipe ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"   ğŸ“Š ì´ ì† ê°ì§€: {total_hands:,}ê°œ")
        print(f"   ğŸ¯ ì†ì´ ìˆëŠ” í”„ë ˆì„: {frames_with_hands:,}/{len(handlist):,}ê°œ")
        print(f"   ğŸ“ ë¹„ë””ì˜¤ ë¹„ìœ¨: {ratio:.3f}")
        
        return handlist, ratio

def save_depth_data(handlist: List, video_name: str, output_dir: str = None) -> str:
    """ê¹Šì´ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤"""
    if output_dir is None:
        output_dir = os.path.join(os.path.dirname(__file__), 'depth_data')
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"ğŸ’¾ ê¹Šì´ ë°ì´í„° ì €ì¥ ì¤‘: {video_name}")
    
    # ê¹Šì´ ë°ì´í„° ìˆ˜ì§‘
    depth_data = {'Left': [], 'Right': [], 'All': []}
    frame_data = []
    
    for frame_idx, hands in enumerate(handlist):
        if not hands:
            continue
            
        frame_info = {'frame': frame_idx, 'hands': []}
        
        for hand in hands:
            if hasattr(hand, 'handdepth') and hand.handdepth is not None:
                depth = float(hand.handdepth)
                hand_type = getattr(hand, 'handtype', 'Unknown')
                
                # ì „ì²´ ë°ì´í„°ì— ì¶”ê°€
                depth_data['All'].append(depth)
                
                # ì† íƒ€ì…ë³„ ë°ì´í„°ì— ì¶”ê°€
                if hand_type in depth_data:
                    depth_data[hand_type].append(depth)
                
                # í”„ë ˆì„ë³„ ë°ì´í„°ì— ì¶”ê°€
                frame_info['hands'].append({
                    'type': hand_type,
                    'depth': depth,
                    'frame': frame_idx
                })
        
        if frame_info['hands']:
            frame_data.append(frame_info)
    
    # ë°ì´í„° ì •ë¦¬
    depth_summary = {
        'video_name': video_name,
        'timestamp': datetime.now().isoformat(),
        'total_hands': len(depth_data['All']),
        'left_hands': len(depth_data['Left']),
        'right_hands': len(depth_data['Right']),
        'total_frames': len(frame_data),
        'depth_range': (min(depth_data['All']), max(depth_data['All'])) if depth_data['All'] else (0, 0),
        'depth_data': depth_data,
        'frame_data': frame_data
    }
    
    # ê¸°ë³¸ í†µê³„ ê³„ì‚°
    if depth_data['All']:
        depths_array = np.array(depth_data['All'])
        depth_summary['statistics'] = {
            'mean': float(np.mean(depths_array)),
            'std': float(np.std(depths_array)),
            'min': float(np.min(depths_array)),
            'max': float(np.max(depths_array)),
            'median': float(np.median(depths_array)),
            'q25': float(np.percentile(depths_array, 25)),
            'q75': float(np.percentile(depths_array, 75)),
            'q90': float(np.percentile(depths_array, 90)),
            'q95': float(np.percentile(depths_array, 95))
        }
    
    # ì €ì¥
    save_path = os.path.join(output_dir, f'{video_name}_depth_data.json')
    
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(depth_summary, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"ğŸ“Š ì €ì¥ ì™„ë£Œ:")
    print(f"   íŒŒì¼: {save_path}")
    print(f"   ì´ ì†: {depth_summary['total_hands']:,}ê°œ")
    print(f"   ì¢Œì†: {depth_summary['left_hands']:,}ê°œ, ìš°ì†: {depth_summary['right_hands']:,}ê°œ")
    print(f"   ìœ íš¨ í”„ë ˆì„: {depth_summary['total_frames']:,}ê°œ")
    if depth_data['All']:
        stats = depth_summary['statistics']
        print(f"   ê¹Šì´ ë²”ìœ„: {stats['min']:.3f} ~ {stats['max']:.3f}")
        print(f"   í‰ê· : {stats['mean']:.3f} Â± {stats['std']:.3f}")
        print(f"   ì¤‘ì•™ê°’: {stats['median']:.3f}")
        print(f"   90ë¶„ìœ„ìˆ˜: {stats['q90']:.3f}")
    
    return save_path

class TorchFloatingDetector:
    """PyTorch ê¸°ë°˜ Floating Hand Detection"""
    
    # ê¸°ë³¸ ì„¤ì •
    QUICK_TEST = True  # 1200í”„ë ˆì„ë§Œ ì²˜ë¦¬
    FRAME_LIMIT = 1200
    
    # ìºì‹± ì„¤ì •
    ENABLE_CACHING = True  # ì²˜ë¦¬ ì™„ë£Œëœ ë°ì´í„° ìºì‹± í™œì„±í™”
    ENABLE_LANDMARK_CACHING = True  # MediaPipe landmark ë°ì´í„° ìºì‹± í™œì„±í™”
    
    # ë¹„ë””ì˜¤ ìƒì„± ì„¤ì •
    GENERATE_VIDEO = True  # ê²°ê³¼ ë¹„ë””ì˜¤ ìƒì„±
    
    # ì„ê³„ê°’ ì„¤ì •
    AUTO_THRESHOLD = False  # ìë™ ì„ê³„ê°’ ì‚¬ìš© ì—¬ë¶€
    THRESHOLD_METHOD = 'midi_based'  # 'statistical', 'clustering', 'valley', 'midi_based'
    FIXED_THRESHOLD = 0.9  # ê³ ì • ì„ê³„ê°’
    
    # ë¹„ë””ì˜¤ ì²˜ë¦¬ ì„¤ì •
    TARGET_VIDEO_DIR = "/home/jhbae/PianoVAM-Code/FingeringDetection/videocapture"
    MAX_VIDEOS = 10  # ì²˜ë¦¬í•  ë¹„ë””ì˜¤ ê°œìˆ˜ ì œí•œ
    
    def __init__(self):
        self.script_dir = os.path.dirname(os.path.abspath(__file__))
        self.video_capture_dir = self.TARGET_VIDEO_DIR
        self.target_videos = []
        self.current_video = None
        self.quick_test = self.QUICK_TEST
        self.frame_limit = self.FRAME_LIMIT
        self.enable_caching = self.ENABLE_CACHING
        self.enable_landmark_caching = self.ENABLE_LANDMARK_CACHING
        self.generate_video = self.GENERATE_VIDEO
        
        # ê³ ì • ì„ê³„ê°’ ì„¤ì •
        self.auto_threshold = self.AUTO_THRESHOLD
        self.threshold_method = self.THRESHOLD_METHOD
        self.fixed_threshold = self.FIXED_THRESHOLD
        
        # ì„ê³„ê°’ ìºì‹±
        self.cached_threshold = {'Left': self.FIXED_THRESHOLD, 'Right': self.FIXED_THRESHOLD}
        
        # ì²˜ë¦¬í•  ë¹„ë””ì˜¤ ëª©ë¡ ì´ˆê¸°í™”
        self._initialize_video_list()
        
        # ìºì‹± ê²½ë¡œ ì„¤ì •
        self.cache_dir = os.path.join(self.script_dir, 'cache')
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # ê¹Šì´ ë°ì´í„° ì €ì¥ ì„¤ì •
        self.save_depth_data = True  # ê¹Šì´ ë°ì´í„° ì €ì¥ ì—¬ë¶€
        self.depth_data_dir = os.path.join(self.script_dir, 'depth_data')
        
        # Raw ë¹„ë””ì˜¤ ì²˜ë¦¬ ì„¤ì •
        self.use_raw_videos = False  # Raw ë¹„ë””ì˜¤ ì§ì ‘ ì²˜ë¦¬ ì—¬ë¶€
        
        # MediaPipe ì„¤ì •
        self.min_detection_confidence = 0.8
        self.min_tracking_confidence = 0.5
        
    def _initialize_video_list(self):
        """ì²˜ë¦¬ ê°€ëŠ¥í•œ ë¹„ë””ì˜¤ ëª©ë¡ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤"""
        if not os.path.exists(self.video_capture_dir):
            print(f"? ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.video_capture_dir}")
            return
        
        # ì „ì²´ mp4 íŒŒì¼ ê°œìˆ˜ í™•ì¸
        all_mp4_files = [f for f in os.listdir(self.video_capture_dir) if f.endswith('.mp4')]
        print(f"ğŸ“ videocapture í´ë”ì—ì„œ ë°œê²¬ëœ ë¹„ë””ì˜¤: {len(all_mp4_files)}ê°œ")
        
        processed_videos = []
        raw_videos = []
        
        # .mp4 íŒŒì¼ê³¼ ëŒ€ì‘ë˜ëŠ” ë°ì´í„°ê°€ ìˆëŠ” ë¹„ë””ì˜¤ë“¤ì„ ì°¾ê¸°
        for item in all_mp4_files:
            video_name = item[:-4]
            
            # í•´ë‹¹ pkl ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            data_dir = os.path.join(self.video_capture_dir, f"{video_name}_858550")
            if os.path.exists(data_dir):
                handlist_file = os.path.join(data_dir, f"handlist_{video_name}_858550.pkl")
                floating_file = os.path.join(data_dir, f"floatingframes_{video_name}_858550.pkl")
                
                if os.path.exists(handlist_file) and os.path.exists(floating_file):
                    processed_videos.append(video_name)
                else:
                    raw_videos.append(video_name)
            else:
                raw_videos.append(video_name)
        
        print(f"ğŸ“Š ë¶„ì„ ê²°ê³¼:")
        print(f"   âœ… ì´ë¯¸ ì²˜ë¦¬ëœ ë¹„ë””ì˜¤: {len(processed_videos)}ê°œ")
        print(f"   ğŸ”„ ë¯¸ì²˜ë¦¬ ë¹„ë””ì˜¤: {len(raw_videos)}ê°œ")
        
        # ì²˜ë¦¬ëœ ë¹„ë””ì˜¤ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ìš°ì„  ì‚¬ìš©
        if len(processed_videos) > 0:
            print(f"\nğŸ¯ ì´ë¯¸ ì²˜ë¦¬ëœ ë¹„ë””ì˜¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤")
            self.target_videos = processed_videos
            self.use_raw_videos = False
        else:
            print(f"\nğŸ¯ Raw ë¹„ë””ì˜¤ë¥¼ ì§ì ‘ ì²˜ë¦¬í•©ë‹ˆë‹¤")
            self.target_videos = raw_videos
            self.use_raw_videos = True
        
        # MAX_VIDEOS ì œí•œ ì ìš©
        if len(self.target_videos) > self.MAX_VIDEOS:
            print(f"ğŸ“ {len(self.target_videos)}ê°œ ì¤‘ ìµœì‹  {self.MAX_VIDEOS}ê°œë§Œ ì„ íƒ")
            # ìµœì‹  íŒŒì¼ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ì„ íƒ
            self.target_videos = sorted(self.target_videos, reverse=True)[:self.MAX_VIDEOS]
        
        print(f"\nğŸ¯ ì²˜ë¦¬ ëŒ€ìƒ ë¹„ë””ì˜¤: {len(self.target_videos)}ê°œ")
        processing_type = "Raw ë¹„ë””ì˜¤ ì§ì ‘ ì²˜ë¦¬" if self.use_raw_videos else "ì´ë¯¸ ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©"
        print(f"   ğŸ“‹ ì²˜ë¦¬ ë°©ì‹: {processing_type}")
        for i, video in enumerate(self.target_videos, 1):
            print(f"   {i}. {video}")
    
    def set_current_video(self, video_name: str):
        """í˜„ì¬ ì²˜ë¦¬í•  ë¹„ë””ì˜¤ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤"""
        self.current_video = video_name
        self.target_video = video_name
        
    def find_target_data(self) -> Dict[str, Any]:
        """í˜„ì¬ ë¹„ë””ì˜¤ì˜ ë°ì´í„° ê²½ë¡œë¥¼ ì°¾ìŠµë‹ˆë‹¤"""
        video_name = self.current_video
        video_file = os.path.join(self.video_capture_dir, f"{video_name}.mp4")
        
        print(f"ğŸ” ë°ì´í„° ê²€ìƒ‰: {video_name}")
        
        if not os.path.exists(video_file):
            raise FileNotFoundError(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_file}")
        
        # use_raw_videos í”Œë˜ê·¸ë¥¼ ë¨¼ì € í™•ì¸
        if self.use_raw_videos:
            print(f"   ğŸ¬ Raw ë¹„ë””ì˜¤ ëª¨ë“œ ì„¤ì •ë¨ â†’ Raw ë¹„ë””ì˜¤ ì§ì ‘ ì²˜ë¦¬")
            return {
                'video_name': video_name,
                'video_file': video_file,
                'is_raw_video': True
            }
        
        # ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        data_dir = os.path.join(self.video_capture_dir, f"{video_name}_858550")
        handlist_file = os.path.join(data_dir, f"handlist_{video_name}_858550.pkl")
        floating_file = os.path.join(data_dir, f"floatingframes_{video_name}_858550.pkl")
        
        has_processed_data = (os.path.exists(data_dir) and 
                             os.path.exists(handlist_file) and 
                             os.path.exists(floating_file))
        
        # ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ Raw ë¹„ë””ì˜¤ ëª¨ë“œë¡œ ê°•ì œ ì„¤ì •
        if not has_processed_data:
            print(f"   ğŸ¬ ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŒ â†’ Raw ë¹„ë””ì˜¤ ëª¨ë“œë¡œ ì§„í–‰")
            return {
                'video_name': video_name,
                'video_file': video_file,
                'is_raw_video': True
            }
        
        # ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©
        print(f"   ğŸ“ ì²˜ë¦¬ëœ ë°ì´í„° ë°œê²¬ â†’ ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©")
        return {
            'video_name': video_name,
            'data_dir': data_dir,
            'handlist_file': handlist_file,
            'floating_file': floating_file,
            'video_file': video_file,
            'is_raw_video': False
        }
    
    def load_data(self, data_info: Dict[str, Any]) -> Tuple[List, List, float]:
        """ì† ë°ì´í„° ë° ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤"""
        
        # ë””ë²„ê¹…: data_info ë‚´ìš© í™•ì¸
        print(f"ğŸ” load_data ë””ë²„ê¹…:")
        print(f"   data_info í‚¤ë“¤: {list(data_info.keys())}")
        print(f"   is_raw_video: {data_info.get('is_raw_video', False)}")
        print(f"   self.use_raw_videos: {self.use_raw_videos}")
        
        # Raw ë¹„ë””ì˜¤ ì²˜ë¦¬ì¸ ê²½ìš°
        if data_info.get('is_raw_video', False):
            print(f"ğŸ¬ Raw ë¹„ë””ì˜¤ ì§ì ‘ ì²˜ë¦¬ ëª¨ë“œ")
            
            # MediaPipeë¡œ handlist ìƒì„±
            frame_limit = self.frame_limit if self.quick_test else None
            handlist, ratio = process_raw_video_with_mediapipe(
                video_path=data_info['video_file'],
                video_name=data_info['video_name'],
                min_detection_confidence=self.min_detection_confidence,
                min_tracking_confidence=self.min_tracking_confidence,
                frame_limit=frame_limit
            )
            
            # ê¸°ì¡´ floating ë°ì´í„° ì—†ìŒ (ë¹ˆ ë¦¬ìŠ¤íŠ¸)
            existing_floating = []
            
            print(f"ğŸ¬ Raw ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ:")
            print(f"   ğŸ“Š ìƒì„±ëœ handlist: {len(handlist)}í”„ë ˆì„")
            print(f"   ğŸ“ ë¹„ë””ì˜¤ ë¹„ìœ¨: {ratio:.3f}")
            
            return handlist, existing_floating, ratio
        
        # ê¸°ì¡´ ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©ì¸ ê²½ìš°
        print(f"ğŸ“ ê¸°ì¡´ ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”©")
        
        # handlist ë¡œë”©
        with open(data_info['handlist_file'], 'rb') as f:
            handlist = pickle.load(f)
        
        # ê¸°ì¡´ floating ê²°ê³¼ ë¡œë”© (ì°¸ê³ ìš©)
        with open(data_info['floating_file'], 'rb') as f:
            existing_floating = pickle.load(f)
        
        # ë¹„ë””ì˜¤ ì •ë³´ ë¡œë”©
        cap = cv2.VideoCapture(data_info['video_file'])
        if not cap.isOpened():
            raise ValueError(f"ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_info['video_file']}")
        
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        ratio = height / width
        cap.release()
        
        print(f"ğŸ“ ë°ì´í„° ë¡œë”© ì™„ë£Œ:")
        print(f"   ğŸ“Š handlist: {len(handlist)}í”„ë ˆì„")
        print(f"   ğŸ“‹ ê¸°ì¡´ floating: {len(existing_floating)}ê°œ")
        print(f"   ğŸ“ ë¹„ë””ì˜¤ í•´ìƒë„: {width}x{height} ({ratio:.3f})")
        print(f"   ğŸ¬ ë¹„ë””ì˜¤ FPS: {fps}")
        print(f"   ğŸ“ˆ ì´ í”„ë ˆì„: {total_frames:,}ê°œ")
        
        # Quick testì¸ ê²½ìš° í”„ë ˆì„ ì œí•œ
        if self.quick_test and len(handlist) > self.frame_limit:
            handlist = handlist[:self.frame_limit]
            print(f"   â±ï¸  Quick test: {self.frame_limit}í”„ë ˆì„ìœ¼ë¡œ ì œí•œ")
        
        return handlist, existing_floating, ratio
    
    def run_pytorch_detection(self, handlist: List, ratio: float) -> Tuple[List, float, List]:
        """PyTorch ê¸°ë°˜ floating detection ì‹¤í–‰"""
        monitor.start_step("PyTorch GPU ê°€ì† ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰", 4)
        
        start_time = time.time()
        
        try:
            import copy
            handlist_copy = copy.deepcopy(handlist)
            
            total_hands = sum(len(hands) for hands in handlist_copy if hands)
            total_frames = len([hands for hands in handlist_copy if hands])
            
            print(f"? PyTorch ì²˜ë¦¬ ëŒ€ìƒ ë¶„ì„:")
            print(f"   ? ì´ ì† ë°ì´í„°: {total_hands:,}ê°œ")
            print(f"   ??  ìœ íš¨ í”„ë ˆì„: {total_frames:,}ê°œ")
            print(f"   ? ë¹„ë””ì˜¤ ë¹„ìœ¨: {ratio:.3f}")
            
            # ë””ë²„ê¹…: handlist êµ¬ì¡° í™•ì¸
            print(f"\nğŸ” handlist êµ¬ì¡° ë””ë²„ê¹…:")
            if handlist_copy:
                print(f"   ì´ í”„ë ˆì„: {len(handlist_copy)}")
                for i, hands in enumerate(handlist_copy[:3]):  # ì²« 3í”„ë ˆì„ë§Œ í™•ì¸
                    if hands:
                        print(f"   í”„ë ˆì„ {i}: {len(hands)}ê°œ ì†")
                        for j, hand in enumerate(hands):
                            print(f"     ì† {j}: {hand.handtype}")
                            print(f"       handlandmark íƒ€ì…: {type(hand.handlandmark)}")
                            print(f"       handlandmark ê¸¸ì´: {len(hand.handlandmark) if hasattr(hand.handlandmark, '__len__') else 'N/A'}")
                            if hasattr(hand.handlandmark, '__len__') and len(hand.handlandmark) > 0:
                                print(f"       ì²« ë²ˆì§¸ landmark íƒ€ì…: {type(hand.handlandmark[0])}")
                                if hasattr(hand.handlandmark[0], 'x'):
                                    print(f"       ì²« ë²ˆì§¸ landmark ì¢Œí‘œ: x={hand.handlandmark[0].x:.3f}, y={hand.handlandmark[0].y:.3f}")
                            break  # ì²« ë²ˆì§¸ ì†ë§Œ í™•ì¸
                        break  # ìœ íš¨í•œ í”„ë ˆì„ í•˜ë‚˜ë§Œ í™•ì¸
            
            # GPU ì •ë³´ ì¶œë ¥
            if TORCH_AVAILABLE and torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                gpu_props = torch.cuda.get_device_properties(0)
                print(f"? GPU ê°€ì† í™˜ê²½:")
                print(f"   ? GPU: {gpu_name}")
                print(f"   ? VRAM: {gpu_memory:.1f}GB")
                print(f"   ? Compute: {gpu_props.major}.{gpu_props.minor}")
                print(f"   ? CUDA: {torch.version.cuda}")
            else:
                print(f"??  CPU ëª¨ë“œë¡œ ì‹¤í–‰ (GPU ê°€ì† ë¶ˆê°€)")
            
            # ì•Œê³ ë¦¬ì¦˜ ì •ë³´ ë¡œê¹…
            monitor.log_algorithm_start(
                "PyTorch GPU ê°€ì†",
                "ê³ ì •ì  ë°˜ë³µë²• (Fixed-point Iteration)",
                "32-bit ë‹¨ì •ë°€ë„ (ì„±ëŠ¥ ìµœì í™”)",
                "ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬ (GPU vectorized)",
                [
                    "GPU í…ì„œ ì—°ì‚° ìµœì í™”",
                    "ë°°ì¹˜ ì²˜ë¦¬ (2048-8192ê°œ ë™ì‹œ)",
                    "ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë¦¬ë°",
                    "ë²¡í„°í™” ì—°ì‚° (CUDA ì»¤ë„)",
                    "ìë™ ë¯¸ë¶„ ë¹„í™œì„±í™”"
                ]
            )
            
            # 1ë‹¨ê³„: ëª¨ë¸ ìƒì„±
            monitor.update_step_progress(1, "ì† ê³¨ê²© ëª¨ë¸ ìƒì„± ì¤‘...")
            model_start = time.time()
            
            print(f"? PyTorch ëª¨ë¸ ìƒì„± ì‹œì‘:")
            lhmodel, rhmodel = pytorch_version.modelskeleton(handlist_copy)
            model_time = time.time() - model_start
            
            print(f"? PyTorch ëª¨ë¸ ìƒì„± ì™„ë£Œ:")
            print(f"   ??  ì¢Œì† ëª¨ë¸: {len(lhmodel) if lhmodel else 0}ê°œ ê´€ì ˆ")
            print(f"   ??  ìš°ì† ëª¨ë¸: {len(rhmodel) if rhmodel else 0}ê°œ ê´€ì ˆ")
            print(f"   ??  ì†Œìš” ì‹œê°„: {model_time:.2f}ì´ˆ")
            
            # 2ë‹¨ê³„: ê¹Šì´ ê³„ì‚°
            monitor.update_step_progress(2, f"PyTorch ë²¡í„°í™” ì—°ì‚°ìœ¼ë¡œ {total_hands:,}ê°œ ì† ê¹Šì´ ê³„ì‚° ì¤‘...")
            
            print(f"\n? PyTorch ê¹Šì´ ê³„ì‚° ì‹œì‘:")
            print(f"   ? í•´ë²• ë°©ì‹: ê³ ì •ì  ë°˜ë³µë²• (Fixed-point Iteration)")
            print(f"   ? ë¹„ì„ í˜• ë°©ì •ì‹ ì‹œìŠ¤í…œ: 3ë³€ìˆ˜ 3ë°©ì •ì‹ (ë²¡í„°í™”)")
            print(f"   ? í—ˆìš© ì˜¤ì°¨: 1e-6 (ì„±ëŠ¥ ìµœì í™”)")
            print(f"   ??  ì²˜ë¦¬ ë°©ì‹: GPU ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬")
            
            start_depth = time.time()
            pytorch_version.depthlist(handlist_copy, lhmodel, rhmodel, ratio)
            depth_time = time.time() - start_depth
            
            print(f"? PyTorch ê¹Šì´ ê³„ì‚° ì™„ë£Œ:")
            print(f"   ??  ì´ ì†Œìš” ì‹œê°„: {depth_time:.2f}ì´ˆ")
            print(f"   ? í‰ê·  ì²˜ë¦¬ ì†ë„: {total_hands/depth_time:.1f}ê°œ/ì´ˆ")
            
            # 3ë‹¨ê³„: ê²°í•¨ í”„ë ˆì„ ê²€ì¶œ
            monitor.update_step_progress(3, "ê²°í•¨ í”„ë ˆì„ ë¶„ì„ ì¤‘...")
            
            print(f"? ê²°í•¨ í”„ë ˆì„ ê²€ì¶œ:")
            defect_start = time.time()
            defective_frames = pytorch_version.faultyframes(handlist_copy)
            defect_time = time.time() - defect_start
            
            print(f"? ê²°í•¨ í”„ë ˆì„ ê²€ì¶œ ì™„ë£Œ:")
            print(f"   ? ê²°í•¨ í”„ë ˆì„: {len(defective_frames)}ê°œ")
            print(f"   ??  ì†Œìš” ì‹œê°„: {defect_time:.2f}ì´ˆ")
            
            # 4ë‹¨ê³„: Floating íŒì •
            monitor.update_step_progress(4, "Floating ìƒíƒœ íŒì • ì¤‘...")
            
            print(f"? Floating ìƒíƒœ íŒì •:")
            floating_start = time.time()
            
            # ì„ê³„ê°’ ì„¤ì •
            if self.auto_threshold:
                threshold = self.calculate_auto_threshold(handlist_copy)
                print(f"   ? ìë™ ì„ê³„ê°’: ì¢Œì†={threshold['Left']:.3f}, ìš°ì†={threshold['Right']:.3f}")
            else:
                threshold = self.fixed_threshold
                print(f"   ? ê³ ì • ì„ê³„ê°’: {threshold}")
            
            # í”„ë ˆì„ ìˆ˜ ê³„ì‚°
            frame_count = len(handlist_copy)
            
            floating_frames = pytorch_version.detectfloatingframes(
                handlist_copy, frame_count, defective_frames, lhmodel, rhmodel, ratio, threshold
            )
            floating_time = time.time() - floating_start
            
            print(f"? Floating íŒì • ì™„ë£Œ:")
            print(f"   ? Floating ê°ì§€: {len(floating_frames)}ê°œ")
            print(f"   ??  ì†Œìš” ì‹œê°„: {floating_time:.2f}ì´ˆ")
            
            monitor.finish_step(f"ì´ {len(floating_frames)}ê°œ floating ê°ì§€")
            
        except Exception as e:
            print(f"? PyTorch ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
        
        pytorch_time = time.time() - start_time
        print(f"\n? PyTorch ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"   ??  ì´ ì†Œìš” ì‹œê°„: {pytorch_time:.2f}ì´ˆ")
        print(f"   ? ì „ì²´ ì²˜ë¦¬ëŸ‰: {total_hands/pytorch_time:.1f}ê°œ/ì´ˆ")
        
        return floating_frames, pytorch_time, handlist_copy  # ì—…ë°ì´íŠ¸ëœ handlist_copyë„ ë°˜í™˜
    
    def calculate_auto_threshold(self, handlist: List) -> Dict[str, float]:
        """ìë™ ì„ê³„ê°’ ê³„ì‚°"""
        if self.threshold_method == 'midi_based' and midicomparison:
            return self.calculate_midi_based_threshold(handlist)
        elif self.threshold_method == 'valley':
            return self.calculate_valley_threshold(handlist)
        else:
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return {'Left': self.fixed_threshold, 'Right': self.fixed_threshold}
    
    def calculate_midi_based_threshold(self, handlist: List) -> Dict[str, float]:
        """MIDI ë°ì´í„° ê¸°ë°˜ ì„ê³„ê°’ ê³„ì‚°"""
        try:
            # MIDI ê¸°ë°˜ ì„ê³„ê°’ ê³„ì‚° ë¡œì§ (êµ¬í˜„ í•„ìš”)
            # í˜„ì¬ëŠ” ê¸°ë³¸ê°’ ë°˜í™˜
            return {'Left': self.fixed_threshold, 'Right': self.fixed_threshold}
        except Exception as e:
            print(f"??  MIDI ê¸°ë°˜ ì„ê³„ê°’ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {'Left': self.fixed_threshold, 'Right': self.fixed_threshold}
    
    def calculate_valley_threshold(self, handlist: List) -> Dict[str, float]:
        """Valley detection ê¸°ë°˜ ì„ê³„ê°’ ê³„ì‚°"""
        try:
            # Valley detection ë¡œì§ (êµ¬í˜„ í•„ìš”)
            # í˜„ì¬ëŠ” ê¸°ë³¸ê°’ ë°˜í™˜
            return {'Left': self.fixed_threshold, 'Right': self.fixed_threshold}
        except Exception as e:
            print(f"??  Valley ê¸°ë°˜ ì„ê³„ê°’ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {'Left': self.fixed_threshold, 'Right': self.fixed_threshold}
    
    def analyze_results(self, floating_results: List, processing_time: float, handlist: List) -> Dict[str, Any]:
        """ê²°ê³¼ ë¶„ì„"""
        print("? ê²°ê³¼ ë¶„ì„ ì¤‘...")
        
        # ê¸°ë³¸ í†µê³„
        total_hands = sum(len(hands) for hands in handlist if hands)
        total_frames = len([hands for hands in handlist if hands])
        floating_count = len(floating_results)
        
        # ì† íƒ€ì…ë³„ ë¶„ì„
        left_floating = len([f for f in floating_results if 'Left' in str(f)])
        right_floating = len([f for f in floating_results if 'Right' in str(f)])
        
        # ê¹Šì´ ê°’ ë¶„ì„
        depth_values = []
        for hands in handlist:
            if hands:
                for hand in hands:
                    if hasattr(hand, 'depth') and hand.depth is not None:
                        depth_values.append(hand.depth)
        
        analysis = {
            'total_hands': total_hands,
            'total_frames': total_frames,
            'floating_count': floating_count,
            'left_floating': left_floating,
            'right_floating': right_floating,
            'processing_time': processing_time,
            'hands_per_second': total_hands / processing_time if processing_time > 0 else 0,
            'depth_stats': {
                'mean': np.mean(depth_values) if depth_values else 0,
                'std': np.std(depth_values) if depth_values else 0,
                'min': np.min(depth_values) if depth_values else 0,
                'max': np.max(depth_values) if depth_values else 0
            }
        }
        
        return analysis
    
    def create_result_video(self, data_info: Dict[str, Any], floating_results: List, handlist: List) -> str:
        """ê²°ê³¼ ì‹œê°í™” ë¹„ë””ì˜¤ ìƒì„±"""
        monitor.start_step("ê²°ê³¼ ë¹„ë””ì˜¤ ìƒì„±", 1)
        
        video_path = data_info['video_file']
        output_path = os.path.join(self.script_dir, f"{self.current_video}_pytorch_result.mp4")
        
        print(f"? ê²°ê³¼ ë¹„ë””ì˜¤ ìƒì„±:")
        print(f"   ? ì…ë ¥: {video_path}")
        print(f"   ? ì¶œë ¥: {output_path}")
        
        try:
            # ë¹„ë””ì˜¤ ìº¡ì²˜ ë° ë¼ì´í„° ì„¤ì •
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise ValueError(f"ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            # Quick testì¸ ê²½ìš° í”„ë ˆì„ ì œí•œ
            if self.quick_test:
                total_frames = min(total_frames, self.frame_limit)
            
            # ë¹„ë””ì˜¤ ë¼ì´í„° ì„¤ì •
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            
            print(f"   ? í•´ìƒë„: {width}x{height}")
            print(f"   ? FPS: {fps}")
            print(f"   ??  ì´ í”„ë ˆì„: {total_frames}")
            
            # floating ê²°ê³¼ë¥¼ í”„ë ˆì„ë³„ë¡œ ì •ë¦¬
            floating_by_frame = {}
            for result in floating_results:
                # floating ê²°ê³¼ íŒŒì‹±: [frame, handtype, metric_value, 'floating'/'notfloating']
                frame_num = result[0]  # frame
                hand_type = result[1]  # handtype
                metric_value = result[2]  # metric_value
                floating_status = result[3]  # 'floating' or 'notfloating'
                
                if frame_num not in floating_by_frame:
                    floating_by_frame[frame_num] = {}
                
                floating_by_frame[frame_num][f'{hand_type}_floating'] = (floating_status == 'floating')
                floating_by_frame[frame_num][f'{hand_type}_depth'] = metric_value
            
            # MediaPipe ì´ˆê¸°í™”
            with mp_hands.Hands(
                static_image_mode=False,
                max_num_hands=2,
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5
            ) as hands:
                
                frame_count = 0
                while True:
                    ret, frame = cap.read()
                    if not ret or (self.quick_test and frame_count >= self.frame_limit):
                        break
                    
                    # ì§„í–‰ ìƒí™© í‘œì‹œ
                    if frame_count % 100 == 0:
                        progress = (frame_count / total_frames) * 100
                        print(f"   ? ì§„í–‰: {frame_count}/{total_frames} ({progress:.1f}%)")
                    
                    # ì† ê°ì§€
                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    results = hands.process(rgb_frame)
                    
                    # í˜„ì¬ í”„ë ˆì„ì˜ floating ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
                    current_floating = floating_by_frame.get(frame_count, {})
                    current_depth = {
                        'Left_depth': current_floating.get('Left_depth', 0.0),
                        'Right_depth': current_floating.get('Right_depth', 0.0)
                    }
                    
                    # ì‹œê°í™” ì ìš©
                    if results.multi_hand_landmarks:
                        annotated_frame = draw_hand_landmarks_with_floating(
                            frame, 
                            results.multi_hand_landmarks,
                            results.multi_handedness,
                            current_floating,
                            current_depth,
                            self.cached_threshold
                        )
                    else:
                        annotated_frame = frame.copy()
                    
                    # í—¤ë” ì •ë³´ ì¶”ê°€
                    annotated_frame = draw_header_info(
                        annotated_frame,
                        frame_count + 1,
                        current_floating,
                        current_depth,
                        total_frames
                    )
                    
                    out.write(annotated_frame)
                    frame_count += 1
            
            cap.release()
            out.release()
            
            print(f"? ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ: {output_path}")
            monitor.finish_step(f"ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ: {frame_count}í”„ë ˆì„")
            
            return output_path
            
        except Exception as e:
            print(f"? ë¹„ë””ì˜¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""
    
    def run_detection(self) -> Dict[str, Any]:
        """ì „ì²´ detection ì‹¤í–‰"""
        mode_text = f"({self.frame_limit}í”„ë ˆì„ ì œí•œ)" if self.quick_test else "(ì „ì²´ í”„ë ˆì„)"
        print(f"? {self.target_video} PyTorch Detection ì‹œì‘ {mode_text}")
        print("=" * 60)
        
        # ëª¨ë‹ˆí„°ë§ ì‹œì‘
        monitor.start_monitoring(total_steps=4)
        
        try:
            # 1. ë°ì´í„° ì°¾ê¸° ë° ë¡œë“œ
            monitor.start_step("ë°ì´í„° ë¡œë“œ ë° ì´ˆê¸°í™”", 3)
            
            print("? ë°ì´í„° ê²€ìƒ‰ ì¤‘...")
            data_info = self.find_target_data()
            data_location = data_info.get('data_dir', f"Raw ë¹„ë””ì˜¤: {data_info['video_file']}")
            monitor.update_step_progress(1, f"ë°ì´í„° ìœ„ì¹˜: {data_location}")
            
            print("? ì† ë°ì´í„° ë° ë¹„ë””ì˜¤ ì •ë³´ ë¡œë”© ì¤‘...")
            handlist, existing_floating, ratio = self.load_data(data_info)
            monitor.update_step_progress(2, f"handlist: {len(handlist)}í”„ë ˆì„ ë¡œë”© ì™„ë£Œ")
            
            actual_frame_count = len([hand for hands in handlist if hands for hand in hands])
            total_hands = sum(len(hands) for hands in handlist if hands)
            
            monitor.update_step_progress(3, f"ë¶„ì„ ì¤€ë¹„: {total_hands:,}ê°œ ì† ë°ì´í„°")
            monitor.finish_step(f"ì´ {total_hands:,}ê°œ ì†, {len(handlist)}í”„ë ˆì„ ë¡œë”© ì™„ë£Œ")
            
            # 2. PyTorch Detection ì‹¤í–‰
            print(f"\n? PyTorch Floating Detection ì‹œì‘...")
            floating_results, processing_time, updated_handlist = self.run_pytorch_detection(handlist, ratio)
            
            # 3. ê²°ê³¼ ë¶„ì„
            monitor.start_step("ê²°ê³¼ ë¶„ì„", 1)
            analysis = self.analyze_results(floating_results, processing_time, updated_handlist)
            monitor.finish_step(f"ë¶„ì„ ì™„ë£Œ: {analysis['floating_count']}ê°œ floating ê°ì§€")
            
            # 4. ê¹Šì´ ë°ì´í„° ì €ì¥ (ì„ íƒì )
            if self.save_depth_data:
                monitor.start_step("ê¹Šì´ ë°ì´í„° ì €ì¥", 1)
                depth_data_path = save_depth_data(updated_handlist, self.current_video, self.depth_data_dir)
                analysis['depth_data_path'] = depth_data_path
                monitor.finish_step("ê¹Šì´ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
            
            # 5. ê²°ê³¼ ë¹„ë””ì˜¤ ìƒì„±
            video_path = ""
            if self.generate_video:
                video_path = self.create_result_video(data_info, floating_results, updated_handlist)
                analysis['result_video'] = video_path
            
            analysis['floating_results'] = floating_results
            analysis['video_info'] = data_info
            
            return analysis
            
        finally:
            # ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
            monitor.stop_monitoring()
    
    def check_video_resolutions(self):
        """ëª¨ë“  ëŒ€ìƒ ë¹„ë””ì˜¤ì˜ í•´ìƒë„ë¥¼ ì²´í¬í•©ë‹ˆë‹¤"""
        print(f"\n? ëŒ€ìƒ ë¹„ë””ì˜¤ í•´ìƒë„ ì²´í¬:")
        print("=" * 60)
        
        resolutions = {}
        for video_name in self.target_videos:
            video_file = os.path.join(self.video_capture_dir, f"{video_name}.mp4")
            
            if os.path.exists(video_file):
                cap = cv2.VideoCapture(video_file)
                if cap.isOpened():
                    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                    fps = int(cap.get(cv2.CAP_PROP_FPS))
                    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                    ratio = height / width
                    duration = total_frames / fps if fps > 0 else 0
                    
                    resolutions[video_name] = {
                        'width': width,
                        'height': height,
                        'fps': fps,
                        'total_frames': total_frames,
                        'ratio': ratio,
                        'duration': duration
                    }
                    
                    print(f"? {video_name}:")
                    print(f"   í•´ìƒë„: {width}x{height} (ë¹„ìœ¨: {ratio:.3f})")
                    print(f"   FPS: {fps}, ì´ í”„ë ˆì„: {total_frames:,}ê°œ")
                    print(f"   ê¸¸ì´: {duration:.1f}ì´ˆ ({duration/60:.1f}ë¶„)")
                    
                    cap.release()
                else:
                    print(f"? {video_name}: ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨")
            else:
                print(f"? {video_name}: íŒŒì¼ ì—†ìŒ")
        
        # í•´ìƒë„ í†µê³„
        if resolutions:
            print(f"\n? í•´ìƒë„ í†µê³„:")
            unique_resolutions = set((info['width'], info['height']) for info in resolutions.values())
            print(f"   ê³ ìœ  í•´ìƒë„: {len(unique_resolutions)}ê°€ì§€")
            for width, height in sorted(unique_resolutions):
                count = sum(1 for info in resolutions.values() if info['width'] == width and info['height'] == height)
                print(f"   {width}x{height}: {count}ê°œ ë¹„ë””ì˜¤")
            
            avg_fps = sum(info['fps'] for info in resolutions.values()) / len(resolutions)
            total_duration = sum(info['duration'] for info in resolutions.values())
            print(f"   í‰ê·  FPS: {avg_fps:.1f}")
            print(f"   ì´ ê¸¸ì´: {total_duration:.1f}ì´ˆ ({total_duration/60:.1f}ë¶„)")
        
        return resolutions

    def extract_depth_data_only(self, video_name: str = None) -> str:
        """ê¹Šì´ ë°ì´í„°ë§Œ ì¶”ì¶œí•˜ê³  ì €ì¥í•˜ëŠ” í•¨ìˆ˜ (ë¶„ì„ ì—†ì´)"""
        if video_name:
            self.set_current_video(video_name)
        elif not self.current_video:
            if self.target_videos:
                self.set_current_video(self.target_videos[0])
            else:
                print("âš ï¸ ë¶„ì„í•  ë¹„ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return ""
        
        print(f"ğŸ’¾ {self.current_video} ê¹Šì´ ë°ì´í„° ì¶”ì¶œ ëª¨ë“œ")
        print("=" * 60)
        
        try:
            # 1. ë°ì´í„° ë¡œë“œ
            print("ğŸ“ ë°ì´í„° ë¡œë”© ì¤‘...")
            data_info = self.find_target_data()
            handlist, existing_floating, ratio = self.load_data(data_info)
            
            # 2. PyTorch ê¹Šì´ ê³„ì‚°ë§Œ ì‹¤í–‰
            print("ğŸ”¬ PyTorch ê¹Šì´ ê³„ì‚° ì¤‘...")
            
            # handlist ë³µì‚¬
            import copy
            handlist_copy = copy.deepcopy(handlist)
            
            # ëª¨ë¸ ìƒì„±
            lhmodel, rhmodel = pytorch_version.modelskeleton(handlist_copy)
            
            # ê¹Šì´ ê³„ì‚°
            pytorch_version.depthlist(handlist_copy, lhmodel, rhmodel, ratio)
            
            # 3. ê¹Šì´ ë°ì´í„° ì €ì¥
            print("ğŸ’¾ ê¹Šì´ ë°ì´í„° ì €ì¥ ì¤‘...")
            depth_data_path = save_depth_data(handlist_copy, self.current_video, self.depth_data_dir)
            
            print(f"âœ… ê¹Šì´ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {depth_data_path}")
            return depth_data_path
            
        except Exception as e:
            print(f"âŒ ê¹Šì´ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""
    
    def batch_depth_extraction(self) -> Dict[str, str]:
        """ëª¨ë“  ë¹„ë””ì˜¤ì— ëŒ€í•´ ê¹Šì´ ë°ì´í„°ë§Œ ì¶”ì¶œ"""
        print(f"ğŸ’¾ ì¼ê´„ ê¹Šì´ ë°ì´í„° ì¶”ì¶œ ì‹œì‘ (ì´ {len(self.target_videos)}ê°œ ë¹„ë””ì˜¤)")
        print("ğŸ“‹ ëª©ì : ë¶„ì„ìš© ê¹Šì´ ë°ì´í„° ìˆ˜ì§‘")
        print("=" * 60)
        
        results = {}
        
        for i, video_name in enumerate(self.target_videos, 1):
            print(f"\n[{i}/{len(self.target_videos)}] {video_name} ë°ì´í„° ì¶”ì¶œ ì¤‘...")
            
            try:
                depth_data_path = self.extract_depth_data_only(video_name)
                results[video_name] = {
                    'depth_data_path': depth_data_path,
                    'status': 'success' if depth_data_path else 'failed'
                }
                    
            except Exception as e:
                print(f"âŒ {video_name} ì‹¤íŒ¨: {e}")
                results[video_name] = {
                    'depth_data_path': '',
                    'status': 'failed',
                    'error': str(e)
                }
        
        # ê²°ê³¼ ìš”ì•½
        successful = [v for v in results.values() if v['status'] == 'success']
        failed = [v for v in results.values() if v['status'] == 'failed']
        
        print(f"\n{'='*60}")
        print(f"ğŸ’¾ ì¼ê´„ ê¹Šì´ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ!")
        print(f"   ì„±ê³µ: {len(successful)}/{len(self.target_videos)}ê°œ")
        print(f"   ì‹¤íŒ¨: {len(failed)}ê°œ")
        print(f"   ì €ì¥ ìœ„ì¹˜: {self.depth_data_dir}")
        
        if successful:
            print(f"\nâœ… ì„±ê³µí•œ ë¹„ë””ì˜¤:")
            for video_name, result in results.items():
                if result['status'] == 'success':
                    print(f"   ğŸ’¾ {video_name}: {os.path.basename(result['depth_data_path'])}")
        
        if failed:
            print(f"\nâŒ ì‹¤íŒ¨í•œ ë¹„ë””ì˜¤:")
            for video_name, result in results.items():
                if result['status'] == 'failed':
                    print(f"   âš ï¸ {video_name}: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
        
        return results

    def print_summary(self, results: Dict[str, Any]):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 60)
        mode_text = f" (Quick Test: {self.frame_limit}í”„ë ˆì„)" if self.quick_test else ""
        print(f"? {self.target_video} PyTorch Detection ê²°ê³¼{mode_text}")
        print("=" * 60)
        
        print(f"\n??  ì„±ëŠ¥:")
        print(f"   ì²˜ë¦¬ ì‹œê°„: {results['processing_time']:.2f}ì´ˆ (GPU)")
        print(f"   ì²˜ë¦¬ ì†ë„: {results['hands_per_second']:.1f}ê°œ/ì´ˆ")
        
        print(f"\n? Detection ê²°ê³¼:")
        print(f"   ì´ ì† ë°ì´í„°: {results['total_hands']:,}ê°œ")
        print(f"   ì´ í”„ë ˆì„: {results['total_frames']:,}ê°œ")
        print(f"   Floating ê°ì§€: {results['floating_count']}ê°œ")
        print(f"   ì¢Œì† floating: {results['left_floating']}ê°œ")
        print(f"   ìš°ì† floating: {results['right_floating']}ê°œ")
        
        print(f"\n? ê¹Šì´ í†µê³„:")
        depth_stats = results['depth_stats']
        print(f"   í‰ê·  ê¹Šì´: {depth_stats['mean']:.3f}")
        print(f"   í‘œì¤€í¸ì°¨: {depth_stats['std']:.3f}")
        print(f"   ìµœì†Œê°’: {depth_stats['min']:.3f}")
        print(f"   ìµœëŒ€ê°’: {depth_stats['max']:.3f}")
        
        if results.get('result_video'):
            print(f"\n? ê²°ê³¼ ë¹„ë””ì˜¤: {results['result_video']}")
            print(f"   ??  Hand landmarks ë° floating ìƒíƒœ ì‹œê°í™”")
            print(f"   ? ì„ê³„ê°’: {self.fixed_threshold}")
        
        if results.get('depth_data_path'):
            print(f"\nğŸ’¾ ê¹Šì´ ë°ì´í„°: {results['depth_data_path']}")
            print(f"   ğŸ“Š ë³„ë„ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ í™œìš© ê°€ëŠ¥")

def process_multiple_videos():
    """ì—¬ëŸ¬ ë¹„ë””ì˜¤ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ (í†µí•© ëª¨ë“œ)"""
    detector = TorchFloatingDetector()
    
    if not detector.target_videos:
        print("? ì²˜ë¦¬í•  ë¹„ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ¯ í†µí•© ì²˜ë¦¬ ì‹¤í–‰:")
    print(f"   âœ… Floating Detection (ì„ê³„ê°’: {detector.fixed_threshold})")
    print(f"   ğŸ’¾ ê¹Šì´ ë°ì´í„° ì €ì¥")
    print(f"   ğŸ“¹ ê²°ê³¼ ë¹„ë””ì˜¤ ìƒì„±")
    print(f"   ğŸ“Š ìµœëŒ€ {len(detector.target_videos)}ê°œ ë¹„ë””ì˜¤ ì²˜ë¦¬")
    print("")
    
    # ë¹„ë””ì˜¤ í•´ìƒë„ ì²´í¬
    resolutions = detector.check_video_resolutions()
    
    all_results = []
    
    for i, video_name in enumerate(detector.target_videos, 1):
        print(f"\n{'='*80}")
        print(f"? [{i}/{len(detector.target_videos)}] ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘: {video_name}")
        print(f"   ì„ê³„ê°’: {detector.fixed_threshold}")
        print(f"{'='*80}")
        
        # í˜„ì¬ ë¹„ë””ì˜¤ ì„¤ì •
        detector.set_current_video(video_name)
        
        try:
            # Detection ì‹¤í–‰
            results = detector.run_detection()
            
            if results:
                all_results.append({
                    'video_name': video_name,
                    'results': results
                })
                
                # ê°œë³„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
                print(f"\n? [{i}/{len(detector.target_videos)}] ì™„ë£Œ: {video_name}")
                detector.print_summary(results)
                
        except Exception as e:
            print(f"? [{i}/{len(detector.target_videos)}] ì‹¤íŒ¨: {video_name}")
            print(f"   ì˜¤ë¥˜: {str(e)}")
            continue
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*80}")
    print(f"ğŸ¯ í†µí•© ëª¨ë“œ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"   âœ… ì„±ê³µ: {len(all_results)}/{len(detector.target_videos)}ê°œ")
    print(f"   ğŸ¯ ì„ê³„ê°’: {detector.fixed_threshold}")
    print(f"{'='*80}")
    
    for result in all_results:
        video_name = result['video_name']
        res = result['results']
        print(f"\nğŸ“Š {video_name}:")
        print(f"   â±ï¸  ì²˜ë¦¬ ì‹œê°„: {res.get('processing_time', 0):.2f}ì´ˆ")
        print(f"   ğŸ¯ Floating ê°ì§€: {res.get('floating_count', 0)}ê°œ")
        print(f"   âš¡ ì²˜ë¦¬ ì†ë„: {res.get('hands_per_second', 0):.1f}ê°œ/ì´ˆ")
        
        if res.get('depth_data_path'):
            print(f"   ğŸ’¾ ê¹Šì´ ë°ì´í„°: ì €ì¥ë¨")
        
        if res.get('result_video'):
            print(f"   ğŸ“¹ ê²°ê³¼ ë¹„ë””ì˜¤: ìƒì„±ë¨")

def check_video_resolutions_only():
    """ë¹„ë””ì˜¤ í•´ìƒë„ë§Œ ì²´í¬í•˜ëŠ” í•¨ìˆ˜"""
    detector = TorchFloatingDetector()
    
    if not detector.target_videos:
        print("? ì²˜ë¦¬í•  ë¹„ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("? PyTorch Floating Hand Detection - ë¹„ë””ì˜¤ í•´ìƒë„ ì²´í¬")
    print("=" * 60)
    
    # í•´ìƒë„ ì²´í¬
    resolutions = detector.check_video_resolutions()
    
    return resolutions

def extract_depth_data():
    """ê¹Šì´ ë°ì´í„°ë§Œ ì¶”ì¶œí•˜ì—¬ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ (ì„ê³„ê°’ ì„¤ì •ì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘)"""
    detector = TorchFloatingDetector()
    
    if not detector.target_videos:
        print("? ì²˜ë¦¬í•  ë¹„ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("ğŸ’¾ PyTorch Floating Hand Detection - ê¹Šì´ ë°ì´í„° ì¶”ì¶œ")
    print("=" * 60)
    print("ğŸ“‹ ëª©ì : ê±°ë¦¬ ë°ì´í„° ìˆ˜ì§‘ (ë¶„ì„ì€ ë³„ë„ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ)")
    print("ğŸ’¾ ì¶œë ¥: JSON í˜•íƒœì˜ ê¹Šì´ ë°ì´í„° íŒŒì¼")
    print("=" * 60)
    
    # ê¹Šì´ ë°ì´í„° ì¶”ì¶œ ì‹¤í–‰
    results = detector.batch_depth_extraction()
    
    print(f"\nğŸ’¾ ê¹Šì´ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ!")
    print(f"ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"   1. ë³„ë„ì˜ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ë¡œ ë°ì´í„° ë¶„í¬ë¥¼ ë¶„ì„í•˜ì„¸ìš”")
    print(f"   2. íˆìŠ¤í† ê·¸ë¨ê³¼ í†µê³„ë¥¼ í™•ì¸í•˜ì—¬ ì ì ˆí•œ ì„ê³„ê°’ì„ ì°¾ìœ¼ì„¸ìš”")
    print(f"   3. ì½”ë“œì˜ FIXED_THRESHOLD ê°’ì„ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”")
    
    return results

# ì „ì—­ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
monitor = PerformanceMonitor()

if __name__ == "__main__":
    print("ğŸ”¬ PyTorch Floating Hand Detection")
    print("=" * 60)
    
    # í†µí•© ëª¨ë“œ ìë™ ì‹¤í–‰
    print(f"ğŸ“Š í˜„ì¬ ì„¤ì •: ìµœëŒ€ {TorchFloatingDetector.MAX_VIDEOS}ê°œ ë¹„ë””ì˜¤, ì„ê³„ê°’ {TorchFloatingDetector.FIXED_THRESHOLD}")
    print("\nğŸ¯ í†µí•© ëª¨ë“œ: Floating Detection + ê¹Šì´ê°’ ì €ì¥ + ê²°ê³¼ ë¹„ë””ì˜¤ ìƒì„±")
    print("=" * 60)
    
    try:
        process_multiple_videos()
    except KeyboardInterrupt:
        print("\n\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc() 