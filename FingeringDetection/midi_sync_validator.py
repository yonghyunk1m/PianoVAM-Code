#!/usr/bin/env python3
"""
MIDI-Video Synchronization Validator
MIDIì™€ ë¹„ë””ì˜¤ ë™ê¸°í™” ì •í™•ì„±ì„ ê²€ì¦í•˜ëŠ” ë„êµ¬
"""

import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import mido
from scipy import stats
from scipy.signal import correlate
import seaborn as sns

class MIDISyncValidator:
    """MIDI-ë¹„ë””ì˜¤ ë™ê¸°í™” ê²€ì¦ê¸°"""
    
    def __init__(self, depth_data_dir: str = "depth_data", 
                 midi_dir: str = "/home/jhbae/PianoVAM-Code/FingeringDetection/midiconvert"):
        self.depth_data_dir = Path(depth_data_dir)
        self.midi_dir = Path(midi_dir)
        self.target_fps = 20
        
    def load_sample_data(self, video_name: str) -> Dict:
        """íŠ¹ì • ë¹„ë””ì˜¤ì˜ ê¹Šì´ ë°ì´í„°ì™€ MIDI ë°ì´í„° ë¡œë“œ"""
        # ê¹Šì´ ë°ì´í„° ë¡œë“œ
        depth_file = self.depth_data_dir / f"{video_name}_depth_data.json"
        if not depth_file.exists():
            raise FileNotFoundError(f"ê¹Šì´ ë°ì´í„° ì—†ìŒ: {depth_file}")
        
        with open(depth_file, 'r') as f:
            depth_data = json.load(f)
        
        # ëŒ€ì‘ë˜ëŠ” MIDI íŒŒì¼ ì°¾ê¸°
        midi_files = list(self.midi_dir.glob(f"*{video_name}*"))
        if not midi_files:
            # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
            midi_files = [f for f in self.midi_dir.glob("*.mid") 
                         if video_name in f.stem or f.stem in video_name]
        
        if not midi_files:
            raise FileNotFoundError(f"MIDI íŒŒì¼ ì—†ìŒ: {video_name}")
        
        midi_file = midi_files[0]
        midi_data = self._parse_midi_file(midi_file)
        
        return {
            'depth_data': depth_data,
            'midi_data': midi_data,
            'video_name': video_name
        }
    
    def _parse_midi_file(self, midi_file: Path) -> Dict:
        """MIDI íŒŒì¼ íŒŒì‹±"""
        mid = mido.MidiFile(midi_file)
        
        # ëª¨ë“  ìŒí‘œ ì´ë²¤íŠ¸ ìˆ˜ì§‘
        note_events = []
        
        for track in mid.tracks:
            track_time = 0
            for msg in track:
                track_time += msg.time
                
                if msg.type in ['note_on', 'note_off']:
                    time_seconds = mido.tick2second(track_time, mid.ticks_per_beat, 500000)
                    hand_type = 'Right' if msg.note >= 60 else 'Left'
                    
                    note_events.append({
                        'time': time_seconds,
                        'note': msg.note,
                        'velocity': msg.velocity if msg.type == 'note_on' else 0,
                        'type': msg.type,
                        'hand': hand_type
                    })
        
        return {
            'file': midi_file.name,
            'events': sorted(note_events, key=lambda x: x['time']),
            'duration': max(event['time'] for event in note_events) if note_events else 0
        }
    
    def analyze_timing_correlation(self, data: Dict, time_shift_range: Tuple[float, float] = (-2.0, 2.0), 
                                 step: float = 0.1) -> Dict:
        """ì‹œê°„ ì´ë™ì— ë”°ë¥¸ ìƒê´€ê´€ê³„ ë¶„ì„"""
        print(f"ğŸ” íƒ€ì´ë° ìƒê´€ê´€ê³„ ë¶„ì„: {data['video_name']}")
        
        # í”„ë ˆì„ë³„ ê¹Šì´ ì‹œê³„ì—´ ìƒì„±
        frame_data = data['depth_data']['frame_data']
        video_duration = max(frame['frame'] for frame in frame_data) / self.target_fps
        
        # ì†ë³„ë¡œ ë¶„ì„
        results = {}
        
        for hand_type in ['Left', 'Right']:
            print(f"   ë¶„ì„ ì¤‘: {hand_type}ì†")
            
            # ê¹Šì´ ì‹œê³„ì—´ ìƒì„±
            depth_timeline = self._create_depth_timeline(frame_data, hand_type, video_duration)
            
            # MIDI í™œì„±ë„ ì‹œê³„ì—´ ìƒì„±
            midi_timeline = self._create_midi_timeline(
                data['midi_data']['events'], hand_type, video_duration
            )
            
            if len(depth_timeline) == 0 or len(midi_timeline) == 0:
                continue
            
            # ë‹¤ì–‘í•œ ì‹œê°„ ì´ë™ìœ¼ë¡œ ìƒê´€ê´€ê³„ ê³„ì‚°
            time_shifts = np.arange(time_shift_range[0], time_shift_range[1] + step, step)
            correlations = []
            
            for shift in time_shifts:
                shifted_midi = self._shift_timeline(midi_timeline, shift, video_duration)
                
                # ê°™ì€ ê¸¸ì´ë¡œ ë§ì¶”ê¸°
                min_len = min(len(depth_timeline), len(shifted_midi))
                if min_len > 10:  # ìµœì†Œ ê¸¸ì´ í™•ì¸
                    depth_seg = depth_timeline[:min_len]
                    midi_seg = shifted_midi[:min_len]
                    
                    # ê¹Šì´ ë³€í™”ìœ¨ê³¼ MIDI í™œì„±ë„ ìƒê´€ê´€ê³„
                    depth_changes = np.diff(depth_seg)
                    midi_changes = np.diff(midi_seg)
                    
                    if len(depth_changes) > 5:
                        corr = np.corrcoef(depth_changes, midi_changes)[0, 1]
                        correlations.append(corr if not np.isnan(corr) else 0)
                    else:
                        correlations.append(0)
                else:
                    correlations.append(0)
            
            # ìµœì  ì‹œê°„ ì´ë™ ì°¾ê¸°
            correlations = np.array(correlations)
            best_idx = np.argmax(np.abs(correlations))
            best_shift = time_shifts[best_idx]
            best_corr = correlations[best_idx]
            
            results[hand_type] = {
                'time_shifts': time_shifts.tolist(),
                'correlations': correlations.tolist(),
                'best_shift': best_shift,
                'best_correlation': best_corr,
                'depth_timeline': depth_timeline[:100],  # ìƒ˜í”Œë§Œ ì €ì¥
                'midi_timeline': midi_timeline[:100]
            }
            
            print(f"      ìµœì  ì‹œê°„ ì´ë™: {best_shift:.1f}ì´ˆ")
            print(f"      ìµœëŒ€ ìƒê´€ê´€ê³„: {best_corr:.3f}")
        
        return results
    
    def _create_depth_timeline(self, frame_data: List[Dict], hand_type: str, duration: float) -> np.ndarray:
        """í”„ë ˆì„ ë°ì´í„°ì—ì„œ ê¹Šì´ ì‹œê³„ì—´ ìƒì„±"""
        # ì‹œê°„ í•´ìƒë„ (ì´ˆ ë‹¨ìœ„)
        time_resolution = 0.05  # 20fps
        time_points = int(duration / time_resolution) + 1
        
        depth_timeline = np.full(time_points, np.nan)
        
        for frame in frame_data:
            frame_time = frame['frame'] / self.target_fps
            time_idx = int(frame_time / time_resolution)
            
            if time_idx < time_points:
                # í•´ë‹¹ ì†ì˜ ê¹Šì´ ì°¾ê¸°
                for hand in frame.get('hands', []):
                    if hand['type'] == hand_type:
                        depth_timeline[time_idx] = hand['depth']
                        break
        
        # NaN ê°’ ë³´ê°„
        mask = ~np.isnan(depth_timeline)
        if np.sum(mask) > 1:
            from scipy.interpolate import interp1d
            valid_indices = np.where(mask)[0]
            valid_values = depth_timeline[mask]
            
            if len(valid_indices) > 1:
                interp_func = interp1d(valid_indices, valid_values, 
                                     kind='linear', fill_value='extrapolate')
                depth_timeline = interp_func(np.arange(time_points))
        
        return depth_timeline
    
    def _create_midi_timeline(self, midi_events: List[Dict], hand_type: str, duration: float) -> np.ndarray:
        """MIDI ì´ë²¤íŠ¸ì—ì„œ í™œì„±ë„ ì‹œê³„ì—´ ìƒì„±"""
        time_resolution = 0.05
        time_points = int(duration / time_resolution) + 1
        
        midi_timeline = np.zeros(time_points)
        active_notes = set()
        
        # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ëœ ì´ë²¤íŠ¸ ì²˜ë¦¬
        hand_events = [e for e in midi_events if e['hand'] == hand_type]
        
        for event in hand_events:
            time_idx = int(event['time'] / time_resolution)
            
            if event['type'] == 'note_on' and event['velocity'] > 0:
                active_notes.add(event['note'])
            elif event['type'] == 'note_off' or (event['type'] == 'note_on' and event['velocity'] == 0):
                active_notes.discard(event['note'])
            
            # í™œì„± ìŒí‘œ ê°œìˆ˜ë¥¼ ì‹œê³„ì—´ì— ê¸°ë¡
            if time_idx < time_points:
                midi_timeline[time_idx:] = len(active_notes)
        
        return midi_timeline
    
    def _shift_timeline(self, timeline: np.ndarray, shift_seconds: float, duration: float) -> np.ndarray:
        """ì‹œê³„ì—´ì„ ì‹œê°„ ì´ë™"""
        time_resolution = 0.05
        shift_samples = int(shift_seconds / time_resolution)
        
        if shift_samples == 0:
            return timeline.copy()
        elif shift_samples > 0:
            # ì–‘ìˆ˜: ë¯¸ë˜ë¡œ ì´ë™ (ì•ìª½ì„ 0ìœ¼ë¡œ ì±„ì›€)
            shifted = np.concatenate([np.zeros(shift_samples), timeline])
        else:
            # ìŒìˆ˜: ê³¼ê±°ë¡œ ì´ë™ (ë’¤ìª½ì„ 0ìœ¼ë¡œ ì±„ì›€)
            shifted = np.concatenate([timeline[-shift_samples:], np.zeros(-shift_samples)])
        
        # ì›ë˜ ê¸¸ì´ë¡œ ìë¥´ê¸°
        return shifted[:len(timeline)]
    
    def visualize_sync_analysis(self, results: Dict, video_name: str, save_path: str = "sync_analysis.png"):
        """ë™ê¸°í™” ë¶„ì„ ê²°ê³¼ ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        for idx, hand_type in enumerate(['Left', 'Right']):
            if hand_type not in results:
                continue
                
            hand_results = results[hand_type]
            
            # ìƒê´€ê´€ê³„ vs ì‹œê°„ ì´ë™
            ax1 = axes[idx, 0]
            ax1.plot(hand_results['time_shifts'], hand_results['correlations'], 'b-', linewidth=2)
            ax1.axvline(hand_results['best_shift'], color='red', linestyle='--', 
                       label=f"Best: {hand_results['best_shift']:.1f}s")
            ax1.axhline(0, color='gray', linestyle='-', alpha=0.3)
            ax1.set_xlabel('Time Shift (seconds)')
            ax1.set_ylabel('Correlation')
            ax1.set_title(f'{hand_type} Hand: Correlation vs Time Shift')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # ì›ë³¸ ì‹œê³„ì—´ ë¹„êµ
            ax2 = axes[idx, 1]
            time_axis = np.arange(len(hand_results['depth_timeline'])) * 0.05
            
            ax2_twin = ax2.twinx()
            
            line1 = ax2.plot(time_axis, hand_results['depth_timeline'], 'b-', 
                           label='Depth', linewidth=2)
            line2 = ax2_twin.plot(time_axis, hand_results['midi_timeline'], 'r-', 
                                label='MIDI Activity', linewidth=2)
            
            ax2.set_xlabel('Time (seconds)')
            ax2.set_ylabel('Depth Value', color='blue')
            ax2_twin.set_ylabel('Active Notes', color='red')
            ax2.set_title(f'{hand_type} Hand: Timeline Comparison')
            
            # ë²”ë¡€ í†µí•©
            lines = line1 + line2
            labels = [l.get_label() for l in lines]
            ax2.legend(lines, labels, loc='upper right')
            
            ax2.grid(True, alpha=0.3)
        
        plt.suptitle(f'MIDI-Video Synchronization Analysis: {video_name}', fontsize=16)
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š ì‹œê°í™” ì €ì¥: {save_path}")
    
    def generate_sync_report(self, video_name: str) -> Dict:
        """íŠ¹ì • ë¹„ë””ì˜¤ì˜ ë™ê¸°í™” ë³´ê³ ì„œ ìƒì„±"""
        try:
            data = self.load_sample_data(video_name)
            results = self.analyze_timing_correlation(data)
            
            # ì‹œê°í™”
            self.visualize_sync_analysis(results, video_name, f"sync_analysis_{video_name}.png")
            
            # ì¢…í•© í‰ê°€
            sync_quality = self._evaluate_sync_quality(results)
            
            report = {
                'video_name': video_name,
                'sync_analysis': results,
                'quality_assessment': sync_quality,
                'recommendations': self._generate_sync_recommendations(sync_quality)
            }
            
            return report
            
        except Exception as e:
            print(f"âŒ ë™ê¸°í™” ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _evaluate_sync_quality(self, results: Dict) -> Dict:
        """ë™ê¸°í™” í’ˆì§ˆ í‰ê°€"""
        quality = {}
        
        for hand_type, hand_results in results.items():
            best_corr = abs(hand_results['best_correlation'])
            best_shift = abs(hand_results['best_shift'])
            
            # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
            if best_corr > 0.7 and best_shift < 0.2:
                grade = "Excellent"
            elif best_corr > 0.5 and best_shift < 0.5:
                grade = "Good"
            elif best_corr > 0.3 and best_shift < 1.0:
                grade = "Fair"
            else:
                grade = "Poor"
            
            quality[hand_type] = {
                'grade': grade,
                'correlation': best_corr,
                'time_offset': hand_results['best_shift'],
                'needs_correction': best_shift > 0.1 or best_corr < 0.5
            }
        
        return quality
    
    def _generate_sync_recommendations(self, quality: Dict) -> List[str]:
        """ë™ê¸°í™” ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        for hand_type, qual in quality.items():
            if qual['needs_correction']:
                if abs(qual['time_offset']) > 0.5:
                    recommendations.append(
                        f"{hand_type}ì†: í° ì‹œê°„ ì˜¤ì°¨ ({qual['time_offset']:.1f}ì´ˆ) - ë…¹í™” ì‹œì‘ì  ì¬ì¡°ì • í•„ìš”"
                    )
                elif qual['correlation'] < 0.3:
                    recommendations.append(
                        f"{hand_type}ì†: ë‚®ì€ ìƒê´€ê´€ê³„ ({qual['correlation']:.2f}) - ì† êµ¬ë¶„ ì•Œê³ ë¦¬ì¦˜ ì¬ê²€í†  í•„ìš”"
                    )
                else:
                    recommendations.append(
                        f"{hand_type}ì†: ë¯¸ì„¸ ì¡°ì • í•„ìš” (ì˜¤ì°¨: {qual['time_offset']:.1f}ì´ˆ)"
                    )
        
        if not recommendations:
            recommendations.append("ë™ê¸°í™” ìƒíƒœ ì–‘í˜¸ - ì¶”ê°€ ì¡°ì • ë¶ˆí•„ìš”")
        
        return recommendations

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    validator = MIDISyncValidator()
    
    # ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ë¡œ í…ŒìŠ¤íŠ¸
    try:
        # ê°€ëŠ¥í•œ ë¹„ë””ì˜¤ ì´ë¦„ë“¤ ì‹œë„
        test_videos = ["2024-02-15_20-38-23", "2024-03-04_04-14-36", "2024-09-05_21-13-49"]
        
        for video_name in test_videos:
            try:
                print(f"\nğŸ” ë™ê¸°í™” ê²€ì¦: {video_name}")
                report = validator.generate_sync_report(video_name)
                
                if report:
                    print(f"\nğŸ“Š {video_name} ë™ê¸°í™” í’ˆì§ˆ:")
                    for hand_type, quality in report['quality_assessment'].items():
                        print(f"   {hand_type}ì†: {quality['grade']} (ìƒê´€ê´€ê³„: {quality['correlation']:.3f}, ì˜¤ì°¨: {quality['time_offset']:.1f}ì´ˆ)")
                    
                    print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
                    for rec in report['recommendations']:
                        print(f"   - {rec}")
                    
                    break  # ì„±ê³µí•˜ë©´ ì¤‘ë‹¨
                    
            except FileNotFoundError:
                continue
        
    except Exception as e:
        print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main() 