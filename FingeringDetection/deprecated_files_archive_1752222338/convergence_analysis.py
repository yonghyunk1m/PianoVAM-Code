#!/usr/bin/env python3
"""
ìˆ˜ë ´ ì°¨ì´ ì›ì¸ ë¶„ì„ ë° í•´ê²° ë°©ì•ˆ
"""

import torch
import torch.optim as optim
import numpy as np
from scipy.optimize import fsolve
import matplotlib.pyplot as plt

def analyze_convergence_issue():
    """ìˆ˜ë ´ ì°¨ì´ ì›ì¸ ë¶„ì„"""
    print("ğŸš¨ ìˆ˜ë ´ ì°¨ì´ ì›ì¸ ë¶„ì„")
    print("=" * 50)
    
    # ë™ì¼í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    w = [0.3, 0.2]
    i = [0.1, 0.4] 
    r = [-0.2, 0.1]
    
    class MockLandmark:
        def __init__(self, x, y):
            self.x = x
            self.y = y
    
    lhmodel = [MockLandmark(0, 0) for _ in range(21)]
    lhmodel[0] = MockLandmark(0.0, 0.0)
    lhmodel[5] = MockLandmark(0.2, 0.3)
    lhmodel[13] = MockLandmark(-0.1, 0.2)
    ratio = 1.0
    
    def landmarkdistance(landmark_a, landmark_b, ratio):
        return ((landmark_a.x - landmark_b.x) ** 2 + 
                (landmark_a.y * ratio - landmark_b.y * ratio) ** 2) ** 0.5 * 2
    
    # ëª©í‘œ ê±°ë¦¬ ê³„ì‚°
    target_WI = landmarkdistance(lhmodel[0], lhmodel[5], ratio)
    target_IR = landmarkdistance(lhmodel[5], lhmodel[13], ratio)
    target_RW = landmarkdistance(lhmodel[13], lhmodel[0], ratio)
    
    print(f"ëª©í‘œ ê±°ë¦¬:")
    print(f"  WI: {target_WI:.6f}")
    print(f"  IR: {target_IR:.6f}")
    print(f"  RW: {target_RW:.6f}")
    
    # SciPy í•´
    def system(vars):
        t, u, v = vars
        eq1 = (t*w[0] - u*i[0])**2 + (t*w[1] - u*i[1])**2 + (t - u)**2 - target_WI**2
        eq2 = (u*i[0] - v*r[0])**2 + (u*i[1] - v*r[1])**2 + (u - v)**2 - target_IR**2  
        eq3 = (v*r[0] - t*w[0])**2 + (v*r[1] - t*w[1])**2 + (v - t)**2 - target_RW**2
        return [eq1, eq2, eq3]
    
    scipy_solution = fsolve(system, [1.0, 1.0, 1.0])
    scipy_depth = scipy_solution.mean()
    
    print(f"\nSciPy í•´:")
    print(f"  t, u, v: {scipy_solution}")
    print(f"  ê¹Šì´: {scipy_depth:.6f}")
    
    # ë°©ì •ì‹ ì”ì°¨ í™•ì¸
    residual = system(scipy_solution)
    print(f"  ë°©ì •ì‹ ì”ì°¨: {residual}")
    print(f"  ì”ì°¨ í¬ê¸°: {np.linalg.norm(residual):.2e}")
    
    return scipy_solution, target_WI, target_IR, target_RW, w, i, r

def improved_pytorch_approach(scipy_solution, target_WI, target_IR, target_RW, w, i, r):
    """ê°œì„ ëœ PyTorch ì ‘ê·¼ë²•"""
    print(f"\nğŸ”§ ê°œì„ ëœ PyTorch ì ‘ê·¼ë²•")
    print("=" * 50)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    target_distances = torch.tensor([target_WI, target_IR, target_RW], device=device)
    w_tensor = torch.tensor(w, device=device)
    i_tensor = torch.tensor(i, device=device)  
    r_tensor = torch.tensor(r, device=device)
    
    # ê°œì„  ì‚¬í•­ë“¤ì„ í…ŒìŠ¤íŠ¸
    configs = [
        {"name": "ê¸°ë³¸ ì„¤ì •", "lr": 0.01, "max_iter": 100, "tol": 1e-6},
        {"name": "ë‚®ì€ í•™ìŠµë¥ ", "lr": 0.001, "max_iter": 200, "tol": 1e-8},
        {"name": "ë§¤ìš° ë‚®ì€ í•™ìŠµë¥ ", "lr": 0.0001, "max_iter": 500, "tol": 1e-10},
        {"name": "LBFGS ì˜µí‹°ë§ˆì´ì €", "optimizer": "LBFGS", "lr": 1.0, "max_iter": 100, "tol": 1e-10}
    ]
    
    results = []
    
    for config in configs:
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸: {config['name']}")
        
        # SciPy í•´ì— ê°€ê¹Œìš´ ì´ˆê¸°ê°’ ì‚¬ìš©
        tuv = torch.tensor(scipy_solution, device=device, requires_grad=True, dtype=torch.float32)
        
        if config.get("optimizer") == "LBFGS":
            optimizer = optim.LBFGS([tuv], lr=config["lr"])
        else:
            optimizer = optim.Adam([tuv], lr=config["lr"])
        
        loss_history = []
        
        for iteration in range(config["max_iter"]):
            if config.get("optimizer") == "LBFGS":
                def closure():
                    optimizer.zero_grad()
                    loss = compute_loss(tuv, w_tensor, i_tensor, r_tensor, target_distances)
                    loss.backward()
                    return loss
                optimizer.step(closure)
                loss = compute_loss(tuv, w_tensor, i_tensor, r_tensor, target_distances)
            else:
                optimizer.zero_grad()
                loss = compute_loss(tuv, w_tensor, i_tensor, r_tensor, target_distances)
                loss.backward()
                optimizer.step()
            
            loss_history.append(loss.item())
            
            if loss.item() < config["tol"]:
                print(f"  ìˆ˜ë ´ ì™„ë£Œ (ë°˜ë³µ {iteration})")
                break
        
        final_tuv = tuv.detach().cpu().numpy()
        final_depth = final_tuv.mean()
        final_loss = loss_history[-1] if loss_history else float('inf')
        
        depth_diff = abs(final_depth - scipy_solution.mean())
        solution_diff = np.linalg.norm(final_tuv - scipy_solution)
        
        print(f"  ìµœì¢… í•´: {final_tuv}")
        print(f"  ìµœì¢… ê¹Šì´: {final_depth:.6f}")
        print(f"  ìµœì¢… Loss: {final_loss:.2e}")
        print(f"  SciPyì™€ ê¹Šì´ ì°¨ì´: {depth_diff:.2e}")
        print(f"  SciPyì™€ í•´ ì°¨ì´: {solution_diff:.2e}")
        
        results.append({
            "config": config["name"],
            "final_tuv": final_tuv,
            "final_depth": final_depth,
            "final_loss": final_loss,
            "depth_diff": depth_diff,
            "solution_diff": solution_diff,
            "loss_history": loss_history
        })
    
    return results

def compute_loss(tuv, w_tensor, i_tensor, r_tensor, target_distances):
    """Loss í•¨ìˆ˜ ê³„ì‚°"""
    t, u, v = tuv[0], tuv[1], tuv[2]
    
    # 3D ì¢Œí‘œ ê³„ì‚°
    w_3d = torch.stack([t * w_tensor[0], t * w_tensor[1], t])
    i_3d = torch.stack([u * i_tensor[0], u * i_tensor[1], u])
    r_3d = torch.stack([v * r_tensor[0], v * r_tensor[1], v])
    
    # ê±°ë¦¬ ê³„ì‚°
    dist_wi = torch.norm(w_3d - i_3d)
    dist_ir = torch.norm(i_3d - r_3d)
    dist_rw = torch.norm(r_3d - w_3d)
    
    # Loss function
    loss = ((dist_wi - target_distances[0])**2 + 
           (dist_ir - target_distances[1])**2 + 
           (dist_rw - target_distances[2])**2)
    
    return loss

def plot_comparison_results(results):
    """ê²°ê³¼ ë¹„êµ ì‹œê°í™”"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Loss íˆìŠ¤í† ë¦¬
    for result in results:
        if len(result["loss_history"]) > 0:
            ax1.plot(result["loss_history"], label=result["config"])
    
    ax1.set_xlabel('Iteration')
    ax1.set_ylabel('Loss')
    ax1.set_title('Convergence Comparison')
    ax1.set_yscale('log')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # ì •í™•ë„ ë¹„êµ
    configs = [r["config"] for r in results]
    depth_diffs = [r["depth_diff"] for r in results]
    solution_diffs = [r["solution_diff"] for r in results]
    
    x = np.arange(len(configs))
    width = 0.35
    
    ax2.bar(x - width/2, depth_diffs, width, label='Depth Difference', alpha=0.7)
    ax2.bar(x + width/2, solution_diffs, width, label='Solution Difference', alpha=0.7)
    
    ax2.set_xlabel('Configuration')
    ax2.set_ylabel('Difference from SciPy')
    ax2.set_title('Accuracy Comparison')
    ax2.set_xticks(x)
    ax2.set_xticklabels(configs, rotation=45)
    ax2.set_yscale('log')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('convergence_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    print("ğŸ“ˆ ë¶„ì„ ê²°ê³¼ê°€ 'convergence_analysis.png'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def recommend_solution():
    """ì¶”ì²œ í•´ê²°ì±…"""
    print(f"\nğŸ’¡ ì¶”ì²œ í•´ê²°ì±…")
    print("=" * 50)
    
    recommendations = [
        "1. ğŸ¯ **ë” ì—„ê²©í•œ ìˆ˜ë ´ ê¸°ì¤€ ì‚¬ìš©**",
        "   - toleranceë¥¼ 1e-10 ì´í•˜ë¡œ ì„¤ì •",
        "   - max_iterationsë¥¼ 500 ì´ìƒìœ¼ë¡œ ì¦ê°€",
        "",
        "2. ğŸ”§ **LBFGS ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©**",
        "   - 2ì°¨ ìµœì í™” ë°©ë²•ìœ¼ë¡œ ë” ì •í™•í•œ ìˆ˜ë ´",
        "   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì€ ëŠ˜ì§€ë§Œ ì •í™•ë„ í–¥ìƒ",
        "",
        "3. ğŸ² **ì´ˆê¸°ê°’ ê°œì„ **",
        "   - SciPy ê²°ê³¼ì— ê°€ê¹Œìš´ ì´ˆê¸°ê°’ ì‚¬ìš©",
        "   - ë˜ëŠ” ì—¬ëŸ¬ ì´ˆê¸°ê°’ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ í›„ ìµœì  ì„ íƒ",
        "",
        "4. âœ… **ê²°ê³¼ ê²€ì¦ ì¶”ê°€**",
        "   - ìµœì¢… Lossê°€ í—ˆìš© ê¸°ì¤€ ì´í•˜ì¸ì§€ í™•ì¸",
        "   - SciPy ê²°ê³¼ì™€ ë¹„êµí•˜ì—¬ ì°¨ì´ê°€ í¬ë©´ ê²½ê³ ",
        "",
        "5. ğŸ”„ **í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼**",
        "   - ë°°ì¹˜ ì²˜ë¦¬ëŠ” PyTorchë¡œ ë¹ ë¥´ê²Œ",
        "   - ì¤‘ìš”í•œ ê³„ì‚°ì€ SciPyë¡œ ê²€ì¦"
    ]
    
    for rec in recommendations:
        print(rec)

if __name__ == "__main__":
    # ë¬¸ì œ ë¶„ì„
    scipy_solution, target_WI, target_IR, target_RW, w, i, r = analyze_convergence_issue()
    
    # ê°œì„ ëœ PyTorch í…ŒìŠ¤íŠ¸
    results = improved_pytorch_approach(scipy_solution, target_WI, target_IR, target_RW, w, i, r)
    
    # ê²°ê³¼ ì‹œê°í™”
    plot_comparison_results(results)
    
    # í•´ê²°ì±… ì œì‹œ
    recommend_solution()
    
    # ìµœê³  ì„±ëŠ¥ ê²°ê³¼ ì¶œë ¥
    best_result = min(results, key=lambda x: x["depth_diff"])
    print(f"\nğŸ† ìµœê³  ì„±ëŠ¥:")
    print(f"  ì„¤ì •: {best_result['config']}")
    print(f"  ê¹Šì´ ì°¨ì´: {best_result['depth_diff']:.2e}")
    print(f"  í•´ ì°¨ì´: {best_result['solution_diff']:.2e}")
    
    if best_result['depth_diff'] < 1e-6:
        print("  âœ… í—ˆìš© ê°€ëŠ¥í•œ ì •í™•ë„!")
    else:
        print("  âš ï¸ ì—¬ì „íˆ ê°œì„  í•„ìš”!") 