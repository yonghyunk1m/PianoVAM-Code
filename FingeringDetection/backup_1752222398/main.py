import cv2
import numpy as np
import os
from psutil import Process
# STEP 1: Import the necessary modules.
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
from PIL import Image
from floatinghands import *
from midicomparison import *
import pickle
from tqdm.auto import tqdm
from stqdm import stqdm
import time as timemodule
# Note: file order is main>evaluate>midicomparison>floatinghands

# STEP 2: Create an HandLandmarker object with GPU support.
# Get the directory where this script is located
script_dir = os.path.dirname(os.path.abspath(__file__))

def check_gpu_support():
    """GPU ì§€ì› ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        # OpenCV GPU ì§€ì› í™•ì¸
        has_opencv_gpu = cv2.cuda.getCudaEnabledDeviceCount() > 0
        
        # TensorFlow GPU ì§€ì› í™•ì¸ (ì„ íƒì )
        try:
            import tensorflow as tf
            has_tf_gpu = len(tf.config.list_physical_devices('GPU')) > 0
        except ImportError:
            has_tf_gpu = False
        
        print(f"ğŸ” GPU ì§€ì› ìƒíƒœ:")
        print(f"   OpenCV CUDA: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if has_opencv_gpu else 'âŒ ì‚¬ìš© ë¶ˆê°€'}")
        print(f"   TensorFlow GPU: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if has_tf_gpu else 'âŒ ì‚¬ìš© ë¶ˆê°€'}")
        
        if not has_opencv_gpu and not has_tf_gpu:
            print("ğŸ’¡ GPU ê°€ì†ì„ ìœ„í•´ CUDA ë“œë¼ì´ë²„ì™€ OpenCV-CUDA ë˜ëŠ” TensorFlow-GPUë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")
            
        return has_opencv_gpu or has_tf_gpu
            
    except Exception as e:
        print(f"âš ï¸ GPU ì§€ì› í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def create_handlandmarker_with_gpu():
    """GPU ì§€ì›ì„ í¬í•¨í•œ HandLandmarker ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    hand_landmarker_path = os.path.join(script_dir, "hand_landmarker.task")
    
    # GPU delegate ì„¤ì • ì‹œë„
    base_options = python.BaseOptions(model_asset_path=hand_landmarker_path)
    
    try:
        # GPU delegate ì‹œë„
        if hasattr(python.BaseOptions, 'Delegate') and hasattr(python.BaseOptions.Delegate, 'GPU'):
            base_options = python.BaseOptions(
                model_asset_path=hand_landmarker_path,
                delegate=python.BaseOptions.Delegate.GPU
            )
            print("ğŸš€ GPU delegate ì„¤ì • ì™„ë£Œ - GPU ê°€ì† í™œì„±í™”")
        else:
            print("âš ï¸ GPU delegate ë¯¸ì§€ì› - CPU ì‚¬ìš©")
    except Exception as e:
        print(f"âš ï¸ GPU delegate ì„¤ì • ì‹¤íŒ¨, CPU ì‚¬ìš©: {e}")
    
    options = vision.HandLandmarkerOptions(
        base_options=base_options,
        running_mode=mp.tasks.vision.RunningMode.VIDEO,
        num_hands=2,
        min_hand_detection_confidence=min_hand_detection_confidence,
        min_hand_presence_confidence=min_hand_presence_confidence,
        min_tracking_confidence=min_tracking_confidence,
    )
    
    return vision.HandLandmarker.create_from_options(options)

# Create paths relative to script location
min_hand_detection_confidence = 0.85
min_hand_presence_confidence = 0.8
min_tracking_confidence = 0.5
VisionRunningMode = mp.tasks.vision.RunningMode

# GPU ì§€ì› í™•ì¸
print("ğŸš€ MediaPipe HandLandmarker ì´ˆê¸°í™”...")
check_gpu_support()

# HandLandmarker ìƒì„± (GPU ì§€ì› í¬í•¨)
detector = create_handlandmarker_with_gpu()
print("âœ… MediaPipe HandLandmarker ì´ˆê¸°í™” ì™„ë£Œ")

filepath = os.path.join(script_dir, 'videocapture') #Video capture directory relative to script

def datagenerate(videoname):
    start = timemodule.time()
    video = cv2.VideoCapture(
        os.path.join(filepath,
                     videoname,
        )
    )
    width = video.get(cv2.CAP_PROP_FRAME_WIDTH)
    height = video.get(cv2.CAP_PROP_FRAME_HEIGHT)

    ratio=height/width
    dirname = os.path.join(
        filepath,
        videoname[:-4]+'_'+f"{min_hand_detection_confidence*100}{min_hand_presence_confidence*100}{min_tracking_confidence*100}",
    )

    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_rate = video.get(cv2.CAP_PROP_FPS)
    with open(
        os.path.join(script_dir, "keyboardcoordinateinfo.pkl"),
        "rb",
    ) as f:
        keyboardcoordinateinfo = pickle.load(f)
    keyboard = generatekeyboard(
        lu=keyboardcoordinateinfo[videoname[:-4]][0],
        ru=keyboardcoordinateinfo[videoname[:-4]][1],
        ld=keyboardcoordinateinfo[videoname[:-4]][2],
        rd=keyboardcoordinateinfo[videoname[:-4]][3],
        blackratio=keyboardcoordinateinfo[videoname[:-4]][4],
        ldistortion=keyboardcoordinateinfo[videoname[:-4]][5],
        rdistortion=keyboardcoordinateinfo[videoname[:-4]][6],
        cdistortion=keyboardcoordinateinfo[videoname[:-4]][7],
    )
    if not os.path.exists(dirname):
        os.makedirs(dirname)

    for file in os.scandir(dirname):
        os.remove(file.path)

    count = 0
    if not video.isOpened():
        print("Could not Open :", filepath)
        exit(0)
    pbar = tqdm(total=frame_count)
    
    while video.isOpened():
        for _ in stqdm(range(frame_count), desc="Generating Frame images from video..."):
            if count < frame_count:
                ret, image = video.read()
                cv2.imwrite(r"{}".format(dirname + "/frame%d.jpg" % count), image)
                count += 1
                pbar.update(1)
            del ret, image
        if count >= frame_count:
            break
    pbar.close()

    video.release()

    # STEP 3: Load the input image.
    file_list = os.listdir(dirname)
    for files in file_list:
        if files[-4:] != ".jpg":
            file_list.remove(files)
    file_count = len(file_list)

    nohandframelist = []
    handlist = []
    handtypelist = []
    tempimglist = []

    pbar2 = tqdm(total=file_count)
    frame=0
    for _ in stqdm(range(file_count), desc="Generating hand information from framewise images..."):
        with Image.open(dirname + "/frame%d.jpg" % frame) as img:
            img_np = np.array(img)
            image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_np)
            time = frame * int(1000 / frame_rate)
            detection_result = detector.detect_for_video(image, timestamp_ms=time)
            # tempimglist.append([image, detection_result])                          # ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•´ ì¢€ ëŠë¦¬ë”ë¼ë„ ë’¤ì—ì„œ imageëŠ” ë‹¤ì‹œ ë¡œë“œí•´ì¤€ë‹¤.
            tempimglist.append(detection_result)
            if len(detection_result.handedness) == 0:
                nohandframelist.append(frame)

            for j in range(len(detection_result.handedness)):
                for finger_landmark in detection_result.hand_landmarks[j]:
                    finger_landmark.x=finger_landmark.x*2-1 #[0,1]->[-1,1]
                    finger_landmark.y=finger_landmark.y*2-1 #[0,1]->[-1,1]

            handsinfo = [
                handclass(
                    handtype=detection_result.handedness[j][0].category_name,
                    handlandmark=detection_result.hand_landmarks[j],
                    handframe=frame,
                )
                for j in range(len(detection_result.handedness))
            ]                                                               # handclass : ( handtype = "Left" or "Right" / hand_landmarks = (Normalized_landmark(x,y,z,visibiliry,presence)) handframe = frame : int )
            handlist.append(handsinfo)
            handtypelist.append([hand.handtype for hand in handsinfo])
            if frame == max(0, file_count - 100):
                keyboard_image = draw_keyboard_on_image(image.numpy_view(), keyboard)
                cv2.imwrite(
                    dirname + "/" + "keyboard.jpg", cv2.cvtColor(keyboard_image, cv2.COLOR_BGR2RGB)
                )
            pbar2.update(1)
            frame += 1

    lhmodel,rhmodel=modelskeleton(handlist)
    depthlist(handlist,lhmodel,rhmodel,ratio)

    faultyframe = faultyframes(handlist)
    pbar2.close()
    print("Calculating faulty frames...")
    floatingframes = detectfloatingframes(handlist, frame_count, faultyframe, lhmodel, rhmodel, ratio=ratio)
    with open(
        dirname
        + "/floatingframes_"
        + videoname[:-4]
        + "_"
        + f"{min_hand_detection_confidence*100}{min_hand_presence_confidence*100}{min_tracking_confidence*100}"
        + ".pkl",
        "wb",
    ) as f:
        pickle.dump(floatingframes, f)

    if os.path.exists(        
        dirname
        + "/handlist_"
        + videoname[:-4]
        + "_"
        + f"{min_hand_detection_confidence*100}{min_hand_presence_confidence*100}{min_tracking_confidence*100}"
        + ".pkl",
    ):
        os.remove(
            dirname
            + "/handlist_"
            + videoname[:-4]
            + "_"
            + f"{min_hand_detection_confidence*100}{min_hand_presence_confidence*100}{min_tracking_confidence*100}"
            + ".pkl",
        )
    with open(
        dirname
        + "/handlist_"
        + videoname[:-4]
        + "_"
        + f"{min_hand_detection_confidence*100}{min_hand_presence_confidence*100}{min_tracking_confidence*100}"
        + ".pkl",
        "wb",
    ) as f:
        pickle.dump(handlist, f)

    print("Data generated and saved")
    print(
        f"faulty frames:{len(faultyframe)}, nohand frames:{len(nohandframelist)}"
    )
    pbar3 = tqdm(total=file_count)
    frame=0
    for _ in stqdm(range(file_count), desc="Generating new images with hand information..."):
        #image, detection_result = tempimglist[frame]
        with Image.open(dirname + "/frame%d.jpg" % frame) as img:
            img_np = np.array(img)
            image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_np)
            detection_result = tempimglist[frame]
            
            new_file_name = f"frame_annotated{frame}.jpg"
            if frame in nohandframelist:
                new_file_name = new_file_name[:-4] + "_NoDetect" + ".jpg"
            annotated_image = draw_landmarks_and_floatedness_on_image(
                image.numpy_view(), detection_result, frame, floatingframes
            )
            cv2.imwrite(
                dirname + "/" + new_file_name,
                cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB),
            )
        pbar3.update(1)
        frame += 1
    pbar3.close()
    
    print("Image generated and saved")
    datatime = timemodule.time()
    print(f"Data generation time: {datatime-start: .5f} sec")




##############################
#datagenerate() ##############
##############################



def handdetokenizer(originaltokenlist, tokenhandinfo):
    noinfocounter = 0
    outputtoken = []
    for i in range(len(originaltokenlist)):
        if tokenhandinfo[i][-2] == "Right":
            for j in range(len(originaltokenlist[i])):
                if "Program" in originaltokenlist[i][j]:
                    originaltokenlist[i][j] = "Program_0"
        elif tokenhandinfo[i][-2] == "Left":
            for j in range(len(originaltokenlist[i])):
                if "Program" in originaltokenlist[i][j]:
                    originaltokenlist[i][j] = "Program_1"
        elif tokenhandinfo[i][-2] == "noinfo":
            for j in range(len(originaltokenlist[i])):
                if "Program" in originaltokenlist[i][j]:
                    originaltokenlist[i][j] = "Program_2"
                    noinfocounter += 1
    for token in originaltokenlist:
        for attribute in token:
            outputtoken.append(attribute)
    print(f"noinfocount: {noinfocounter}")
    return outputtoken
